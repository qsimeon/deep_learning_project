{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Multimodal Alignment: Towards the Platonic Representation\n",
        "\n",
        "This notebook implements a framework for aligning pre-trained unimodal encoders (ResNet-18 for images, DistilBERT for text) into a shared multimodal latent space using lightweight adapters. The goal is to test whether such aligned representations better approximate those of larger, more performant models like DINOv2, providing empirical evidence for the Platonic Representation Hypothesis.\n",
        "\n",
        "## Key Components:\n",
        "- **Frozen Encoders**: ResNet-18 (ImageNet) and DistilBERT (BookCorpus)\n",
        "- **Adapters**: Linear and MLP transformations to shared latent space\n",
        "- **Training**: Dual-encoder contrastive learning on Flickr30k image-caption pairs\n",
        "- **Evaluation**: Kernel alignment metrics, similarity analysis, and downstream CIFAR-10 evaluation\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Setup and Configuration\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: torch in /Users/quileesimeon/miniforge3/envs/multimodal-align/lib/python3.11/site-packages (2.5.1)\n",
            "Requirement already satisfied: torchvision in /Users/quileesimeon/miniforge3/envs/multimodal-align/lib/python3.11/site-packages (0.20.1)\n",
            "Requirement already satisfied: transformers in /Users/quileesimeon/miniforge3/envs/multimodal-align/lib/python3.11/site-packages (4.57.1)\n",
            "Requirement already satisfied: pillow in /Users/quileesimeon/miniforge3/envs/multimodal-align/lib/python3.11/site-packages (11.3.0)\n",
            "Requirement already satisfied: tqdm in /Users/quileesimeon/miniforge3/envs/multimodal-align/lib/python3.11/site-packages (4.67.1)\n",
            "Requirement already satisfied: numpy in /Users/quileesimeon/miniforge3/envs/multimodal-align/lib/python3.11/site-packages (2.3.3)\n",
            "Requirement already satisfied: scikit-learn in /Users/quileesimeon/miniforge3/envs/multimodal-align/lib/python3.11/site-packages (1.5.2)\n",
            "Requirement already satisfied: matplotlib in /Users/quileesimeon/miniforge3/envs/multimodal-align/lib/python3.11/site-packages (3.9.2)\n",
            "Requirement already satisfied: seaborn in /Users/quileesimeon/miniforge3/envs/multimodal-align/lib/python3.11/site-packages (0.13.2)\n",
            "Requirement already satisfied: datasets in /Users/quileesimeon/miniforge3/envs/multimodal-align/lib/python3.11/site-packages (4.2.0)\n",
            "Requirement already satisfied: wandb in /Users/quileesimeon/miniforge3/envs/multimodal-align/lib/python3.11/site-packages (0.22.2)\n",
            "Requirement already satisfied: filelock in /Users/quileesimeon/miniforge3/envs/multimodal-align/lib/python3.11/site-packages (from torch) (3.19.1)\n",
            "Requirement already satisfied: typing-extensions>=4.8.0 in /Users/quileesimeon/miniforge3/envs/multimodal-align/lib/python3.11/site-packages (from torch) (4.15.0)\n",
            "Requirement already satisfied: networkx in /Users/quileesimeon/miniforge3/envs/multimodal-align/lib/python3.11/site-packages (from torch) (3.5)\n",
            "Requirement already satisfied: jinja2 in /Users/quileesimeon/miniforge3/envs/multimodal-align/lib/python3.11/site-packages (from torch) (3.1.6)\n",
            "Requirement already satisfied: fsspec in /Users/quileesimeon/miniforge3/envs/multimodal-align/lib/python3.11/site-packages (from torch) (2025.9.0)\n",
            "Requirement already satisfied: sympy==1.13.1 in /Users/quileesimeon/miniforge3/envs/multimodal-align/lib/python3.11/site-packages (from torch) (1.13.1)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /Users/quileesimeon/miniforge3/envs/multimodal-align/lib/python3.11/site-packages (from sympy==1.13.1->torch) (1.3.0)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.34.0 in /Users/quileesimeon/miniforge3/envs/multimodal-align/lib/python3.11/site-packages (from transformers) (0.35.3)\n",
            "Requirement already satisfied: packaging>=20.0 in /Users/quileesimeon/miniforge3/envs/multimodal-align/lib/python3.11/site-packages (from transformers) (25.0)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /Users/quileesimeon/miniforge3/envs/multimodal-align/lib/python3.11/site-packages (from transformers) (6.0.3)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /Users/quileesimeon/miniforge3/envs/multimodal-align/lib/python3.11/site-packages (from transformers) (2025.9.18)\n",
            "Requirement already satisfied: requests in /Users/quileesimeon/miniforge3/envs/multimodal-align/lib/python3.11/site-packages (from transformers) (2.32.5)\n",
            "Requirement already satisfied: tokenizers<=0.23.0,>=0.22.0 in /Users/quileesimeon/miniforge3/envs/multimodal-align/lib/python3.11/site-packages (from transformers) (0.22.1)\n",
            "Requirement already satisfied: safetensors>=0.4.3 in /Users/quileesimeon/miniforge3/envs/multimodal-align/lib/python3.11/site-packages (from transformers) (0.6.2)\n",
            "Requirement already satisfied: hf-xet<2.0.0,>=1.1.3 in /Users/quileesimeon/miniforge3/envs/multimodal-align/lib/python3.11/site-packages (from huggingface-hub<1.0,>=0.34.0->transformers) (1.1.10)\n",
            "Requirement already satisfied: scipy>=1.6.0 in /Users/quileesimeon/miniforge3/envs/multimodal-align/lib/python3.11/site-packages (from scikit-learn) (1.16.2)\n",
            "Requirement already satisfied: joblib>=1.2.0 in /Users/quileesimeon/miniforge3/envs/multimodal-align/lib/python3.11/site-packages (from scikit-learn) (1.5.2)\n",
            "Requirement already satisfied: threadpoolctl>=3.1.0 in /Users/quileesimeon/miniforge3/envs/multimodal-align/lib/python3.11/site-packages (from scikit-learn) (3.6.0)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /Users/quileesimeon/miniforge3/envs/multimodal-align/lib/python3.11/site-packages (from matplotlib) (1.3.3)\n",
            "Requirement already satisfied: cycler>=0.10 in /Users/quileesimeon/miniforge3/envs/multimodal-align/lib/python3.11/site-packages (from matplotlib) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /Users/quileesimeon/miniforge3/envs/multimodal-align/lib/python3.11/site-packages (from matplotlib) (4.60.1)\n",
            "Requirement already satisfied: kiwisolver>=1.3.1 in /Users/quileesimeon/miniforge3/envs/multimodal-align/lib/python3.11/site-packages (from matplotlib) (1.4.9)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /Users/quileesimeon/miniforge3/envs/multimodal-align/lib/python3.11/site-packages (from matplotlib) (3.2.5)\n",
            "Requirement already satisfied: python-dateutil>=2.7 in /Users/quileesimeon/miniforge3/envs/multimodal-align/lib/python3.11/site-packages (from matplotlib) (2.9.0.post0)\n",
            "Requirement already satisfied: pandas>=1.2 in /Users/quileesimeon/miniforge3/envs/multimodal-align/lib/python3.11/site-packages (from seaborn) (2.3.3)\n",
            "Requirement already satisfied: pyarrow>=21.0.0 in /Users/quileesimeon/miniforge3/envs/multimodal-align/lib/python3.11/site-packages (from datasets) (21.0.0)\n",
            "Requirement already satisfied: dill<0.4.1,>=0.3.0 in /Users/quileesimeon/miniforge3/envs/multimodal-align/lib/python3.11/site-packages (from datasets) (0.4.0)\n",
            "Requirement already satisfied: httpx<1.0.0 in /Users/quileesimeon/miniforge3/envs/multimodal-align/lib/python3.11/site-packages (from datasets) (0.28.1)\n",
            "Requirement already satisfied: xxhash in /Users/quileesimeon/miniforge3/envs/multimodal-align/lib/python3.11/site-packages (from datasets) (3.6.0)\n",
            "Requirement already satisfied: multiprocess<0.70.17 in /Users/quileesimeon/miniforge3/envs/multimodal-align/lib/python3.11/site-packages (from datasets) (0.70.16)\n",
            "Requirement already satisfied: aiohttp!=4.0.0a0,!=4.0.0a1 in /Users/quileesimeon/miniforge3/envs/multimodal-align/lib/python3.11/site-packages (from fsspec[http]<=2025.9.0,>=2023.1.0->datasets) (3.13.0)\n",
            "Requirement already satisfied: anyio in /Users/quileesimeon/miniforge3/envs/multimodal-align/lib/python3.11/site-packages (from httpx<1.0.0->datasets) (4.11.0)\n",
            "Requirement already satisfied: certifi in /Users/quileesimeon/miniforge3/envs/multimodal-align/lib/python3.11/site-packages (from httpx<1.0.0->datasets) (2025.10.5)\n",
            "Requirement already satisfied: httpcore==1.* in /Users/quileesimeon/miniforge3/envs/multimodal-align/lib/python3.11/site-packages (from httpx<1.0.0->datasets) (1.0.9)\n",
            "Requirement already satisfied: idna in /Users/quileesimeon/miniforge3/envs/multimodal-align/lib/python3.11/site-packages (from httpx<1.0.0->datasets) (3.11)\n",
            "Requirement already satisfied: h11>=0.16 in /Users/quileesimeon/miniforge3/envs/multimodal-align/lib/python3.11/site-packages (from httpcore==1.*->httpx<1.0.0->datasets) (0.16.0)\n",
            "Requirement already satisfied: click>=8.0.1 in /Users/quileesimeon/miniforge3/envs/multimodal-align/lib/python3.11/site-packages (from wandb) (8.3.0)\n",
            "Requirement already satisfied: gitpython!=3.1.29,>=1.0.0 in /Users/quileesimeon/miniforge3/envs/multimodal-align/lib/python3.11/site-packages (from wandb) (3.1.45)\n",
            "Requirement already satisfied: platformdirs in /Users/quileesimeon/miniforge3/envs/multimodal-align/lib/python3.11/site-packages (from wandb) (4.5.0)\n",
            "Requirement already satisfied: protobuf!=4.21.0,!=5.28.0,<7,>=3.19.0 in /Users/quileesimeon/miniforge3/envs/multimodal-align/lib/python3.11/site-packages (from wandb) (6.33.0)\n",
            "Requirement already satisfied: pydantic<3 in /Users/quileesimeon/miniforge3/envs/multimodal-align/lib/python3.11/site-packages (from wandb) (2.12.2)\n",
            "Requirement already satisfied: sentry-sdk>=2.0.0 in /Users/quileesimeon/miniforge3/envs/multimodal-align/lib/python3.11/site-packages (from wandb) (2.42.0)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /Users/quileesimeon/miniforge3/envs/multimodal-align/lib/python3.11/site-packages (from pydantic<3->wandb) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.41.4 in /Users/quileesimeon/miniforge3/envs/multimodal-align/lib/python3.11/site-packages (from pydantic<3->wandb) (2.41.4)\n",
            "Requirement already satisfied: typing-inspection>=0.4.2 in /Users/quileesimeon/miniforge3/envs/multimodal-align/lib/python3.11/site-packages (from pydantic<3->wandb) (0.4.2)\n",
            "Requirement already satisfied: charset_normalizer<4,>=2 in /Users/quileesimeon/miniforge3/envs/multimodal-align/lib/python3.11/site-packages (from requests->transformers) (3.4.4)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /Users/quileesimeon/miniforge3/envs/multimodal-align/lib/python3.11/site-packages (from requests->transformers) (2.5.0)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.5.0 in /Users/quileesimeon/miniforge3/envs/multimodal-align/lib/python3.11/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.9.0,>=2023.1.0->datasets) (2.6.1)\n",
            "Requirement already satisfied: aiosignal>=1.4.0 in /Users/quileesimeon/miniforge3/envs/multimodal-align/lib/python3.11/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.9.0,>=2023.1.0->datasets) (1.4.0)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /Users/quileesimeon/miniforge3/envs/multimodal-align/lib/python3.11/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.9.0,>=2023.1.0->datasets) (25.4.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /Users/quileesimeon/miniforge3/envs/multimodal-align/lib/python3.11/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.9.0,>=2023.1.0->datasets) (1.8.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /Users/quileesimeon/miniforge3/envs/multimodal-align/lib/python3.11/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.9.0,>=2023.1.0->datasets) (6.7.0)\n",
            "Requirement already satisfied: propcache>=0.2.0 in /Users/quileesimeon/miniforge3/envs/multimodal-align/lib/python3.11/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.9.0,>=2023.1.0->datasets) (0.4.1)\n",
            "Requirement already satisfied: yarl<2.0,>=1.17.0 in /Users/quileesimeon/miniforge3/envs/multimodal-align/lib/python3.11/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.9.0,>=2023.1.0->datasets) (1.22.0)\n",
            "Requirement already satisfied: gitdb<5,>=4.0.1 in /Users/quileesimeon/miniforge3/envs/multimodal-align/lib/python3.11/site-packages (from gitpython!=3.1.29,>=1.0.0->wandb) (4.0.12)\n",
            "Requirement already satisfied: smmap<6,>=3.0.1 in /Users/quileesimeon/miniforge3/envs/multimodal-align/lib/python3.11/site-packages (from gitdb<5,>=4.0.1->gitpython!=3.1.29,>=1.0.0->wandb) (5.0.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /Users/quileesimeon/miniforge3/envs/multimodal-align/lib/python3.11/site-packages (from pandas>=1.2->seaborn) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /Users/quileesimeon/miniforge3/envs/multimodal-align/lib/python3.11/site-packages (from pandas>=1.2->seaborn) (2025.2)\n",
            "Requirement already satisfied: six>=1.5 in /Users/quileesimeon/miniforge3/envs/multimodal-align/lib/python3.11/site-packages (from python-dateutil>=2.7->matplotlib) (1.17.0)\n",
            "Requirement already satisfied: sniffio>=1.1 in /Users/quileesimeon/miniforge3/envs/multimodal-align/lib/python3.11/site-packages (from anyio->httpx<1.0.0->datasets) (1.3.1)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /Users/quileesimeon/miniforge3/envs/multimodal-align/lib/python3.11/site-packages (from jinja2->torch) (2.1.5)\n",
            "Note: you may need to restart the kernel to use updated packages.\n"
          ]
        }
      ],
      "source": [
        "# Install required packages\n",
        "%pip install torch torchvision transformers pillow tqdm numpy scikit-learn matplotlib seaborn datasets wandb\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/Users/quileesimeon/miniforge3/envs/multimodal-align/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
            "  from .autonotebook import tqdm as notebook_tqdm\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "PyTorch version: 2.5.1\n",
            "CUDA available: False\n",
            "Device: cpu\n"
          ]
        }
      ],
      "source": [
        "# Core imports\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torchvision\n",
        "from torchvision import transforms\n",
        "from transformers import DistilBertModel, DistilBertTokenizer, Dinov2Model\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from PIL import Image\n",
        "import numpy as np\n",
        "from sklearn.neighbors import NearestNeighbors\n",
        "from sklearn.manifold import MDS\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from tqdm import tqdm\n",
        "import logging\n",
        "from pathlib import Path\n",
        "import json\n",
        "import pickle\n",
        "import time\n",
        "import os\n",
        "from typing import Optional, Dict, Any, List, Tuple\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "# Optional imports with fallbacks\n",
        "try:\n",
        "    import wandb\n",
        "    WANDB_AVAILABLE = True\n",
        "except ImportError:\n",
        "    WANDB_AVAILABLE = False\n",
        "    print(\"W&B not available, using local logging only\")\n",
        "\n",
        "try:\n",
        "    from datasets import load_dataset\n",
        "    DATASETS_AVAILABLE = True\n",
        "except ImportError:\n",
        "    DATASETS_AVAILABLE = False\n",
        "    print(\"datasets library not available\")\n",
        "\n",
        "print(f\"PyTorch version: {torch.__version__}\")\n",
        "print(f\"CUDA available: {torch.cuda.is_available()}\")\n",
        "print(f\"Device: {'cuda' if torch.cuda.is_available() else 'cpu'}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Configuration:\n",
            "  seed: 42\n",
            "  num_samples: 2000\n",
            "  batch_size: 32\n",
            "  img_size: 224\n",
            "  embed_dim: 512\n",
            "  img_dim: 512\n",
            "  text_dim: 768\n",
            "  epochs: 10\n",
            "  lr: 0.0001\n",
            "  temperature: 0.07\n",
            "  weight_decay: 0.0001\n",
            "  grad_clip_norm: 1.0\n",
            "  eval_frequency: 2\n",
            "  viz_frequency: 2\n",
            "  use_wandb: False\n",
            "  use_mixed_precision: False\n"
          ]
        }
      ],
      "source": [
        "# Configuration and setup\n",
        "class Config:\n",
        "    def __init__(self, **kwargs):\n",
        "        # Core settings\n",
        "        self.device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "        self.seed = 42\n",
        "        \n",
        "        # Data settings\n",
        "        self.num_samples = 2000  # Reduced for faster experimentation\n",
        "        self.batch_size = 32 if not torch.cuda.is_available() else 64\n",
        "        self.img_size = 224\n",
        "        \n",
        "        # Model settings\n",
        "        self.embed_dim = 512  # Shared embedding dimension\n",
        "        self.img_dim = 512    # ResNet-18 output\n",
        "        self.text_dim = 768   # DistilBERT output\n",
        "        \n",
        "        # Training settings\n",
        "        self.epochs = 10\n",
        "        self.lr = 1e-4\n",
        "        self.temperature = 0.07\n",
        "        self.weight_decay = 1e-4\n",
        "        self.grad_clip_norm = 1.0\n",
        "        \n",
        "        # Evaluation settings\n",
        "        self.eval_frequency = 2\n",
        "        self.viz_frequency = 2\n",
        "        \n",
        "        # Paths\n",
        "        self.save_dir = Path('./results')\n",
        "        self.save_dir.mkdir(exist_ok=True)\n",
        "        \n",
        "        # Optional features\n",
        "        self.use_wandb = False  # Set to True if you want W&B logging\n",
        "        self.use_mixed_precision = torch.cuda.is_available()\n",
        "        \n",
        "        # Update with any provided kwargs\n",
        "        for key, value in kwargs.items():\n",
        "            setattr(self, key, value)\n",
        "    \n",
        "    def to_dict(self):\n",
        "        return {k: v for k, v in self.__dict__.items() \n",
        "                if not isinstance(v, (torch.device, Path))}\n",
        "\n",
        "# Set random seeds\n",
        "torch.manual_seed(42)\n",
        "np.random.seed(42)\n",
        "if torch.cuda.is_available():\n",
        "    torch.cuda.manual_seed_all(42)\n",
        "\n",
        "# Initialize config\n",
        "config = Config()\n",
        "print(\"Configuration:\")\n",
        "for k, v in config.to_dict().items():\n",
        "    print(f\"  {k}: {v}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Loading Flickr30k dataset...\n",
            "Attempting to load Flickr30k dataset (attempt 1/3)...\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Using the latest cached version of the dataset since nlphuji/flickr30k couldn't be found on the Hugging Face Hub\n",
            "Found the latest cached dataset configuration 'default' at /Users/quileesimeon/.cache/huggingface/datasets/nlphuji___flickr30k/default/0.0.0/cd91f9a00273ce2e1584511cba8c10b917c488a3 (last modified on Thu Oct 16 10:58:58 2025).\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Successfully loaded 2000 samples from Flickr30k\n",
            "Dataset loaded with 2000 samples\n"
          ]
        }
      ],
      "source": [
        "# Robust dataset loading with retries and fallbacks\n",
        "import time\n",
        "import random\n",
        "\n",
        "def load_flickr30k_with_retry(max_samples=None, max_retries=3):\n",
        "    \"\"\"Load Flickr30k dataset with retry logic and fallback options.\"\"\"\n",
        "    if not DATASETS_AVAILABLE:\n",
        "        print(\"datasets library not available, creating fallback dataset...\")\n",
        "        return create_fallback_dataset(max_samples)\n",
        "    \n",
        "    for attempt in range(max_retries):\n",
        "        try:\n",
        "            print(f\"Attempting to load Flickr30k dataset (attempt {attempt + 1}/{max_retries})...\")\n",
        "            \n",
        "            # Explicitly set token to None to avoid auth issues\n",
        "            dataset = load_dataset(\"nlphuji/flickr30k\", split='test', token=None)\n",
        "            \n",
        "            if max_samples:\n",
        "                dataset = dataset.select(range(min(max_samples, len(dataset))))\n",
        "            \n",
        "            print(f\"Successfully loaded {len(dataset)} samples from Flickr30k\")\n",
        "            return dataset\n",
        "            \n",
        "        except Exception as e:\n",
        "            print(f\"Attempt {attempt + 1} failed: {str(e)}\")\n",
        "            if attempt < max_retries - 1:\n",
        "                wait_time = 2 ** attempt  # Exponential backoff\n",
        "                print(f\"Waiting {wait_time} seconds before retry...\")\n",
        "                time.sleep(wait_time)\n",
        "            else:\n",
        "                print(\"All attempts failed, falling back to synthetic dataset...\")\n",
        "                return create_fallback_dataset(max_samples)\n",
        "\n",
        "def create_fallback_dataset(max_samples=None):\n",
        "    \"\"\"Create a small synthetic dataset for testing when Flickr30k is unavailable.\"\"\"\n",
        "    print(\"Creating fallback synthetic dataset...\")\n",
        "    \n",
        "    # Create synthetic image-caption pairs\n",
        "    captions = [\n",
        "        \"A dog running in the park\",\n",
        "        \"A cat sitting on a chair\", \n",
        "        \"A car driving on the road\",\n",
        "        \"A person walking on the street\",\n",
        "        \"A bird flying in the sky\",\n",
        "        \"A flower blooming in the garden\",\n",
        "        \"A house with a red roof\",\n",
        "        \"A tree with green leaves\",\n",
        "        \"A boat sailing on water\",\n",
        "        \"A mountain covered in snow\"\n",
        "    ]\n",
        "    \n",
        "    # Extend captions to reach desired size\n",
        "    if max_samples:\n",
        "        while len(captions) < max_samples:\n",
        "            captions.extend(captions[:min(10, max_samples - len(captions))])\n",
        "        captions = captions[:max_samples]\n",
        "    \n",
        "    # Create synthetic dataset structure\n",
        "    class SyntheticDataset:\n",
        "        def __init__(self, captions):\n",
        "            self.captions = captions\n",
        "            self.images = [None] * len(captions)  # Will be replaced with actual images\n",
        "            \n",
        "        def __len__(self):\n",
        "            return len(self.captions)\n",
        "            \n",
        "        def __getitem__(self, idx):\n",
        "            return {\n",
        "                'image': None,  # Will be handled in dataset class\n",
        "                'caption': [self.captions[idx]]  # Wrap in list to match Flickr30k format\n",
        "            }\n",
        "    \n",
        "    return SyntheticDataset(captions)\n",
        "\n",
        "# Load the dataset\n",
        "print(\"Loading Flickr30k dataset...\")\n",
        "dataset = load_flickr30k_with_retry(max_samples=config.num_samples)\n",
        "print(f\"Dataset loaded with {len(dataset)} samples\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Created multimodal dataset with 6000 image-caption pairs\n"
          ]
        }
      ],
      "source": [
        "# Dataset class for multimodal alignment\n",
        "class MultimodalDataset(Dataset):\n",
        "    def __init__(self, dataset, transform=None, tokenizer=None, max_captions_per_image=5):\n",
        "        self.dataset = dataset\n",
        "        self.transform = transform\n",
        "        self.tokenizer = tokenizer\n",
        "        self.max_captions_per_image = max_captions_per_image\n",
        "        \n",
        "        # Create image-caption pairs\n",
        "        self.pairs = []\n",
        "        for idx in range(len(self.dataset)):\n",
        "            item = self.dataset[idx]\n",
        "            \n",
        "            # Handle different dataset formats\n",
        "            if hasattr(item, 'get'):\n",
        "                image = item.get('image')\n",
        "                captions = item.get('caption', [])\n",
        "            else:\n",
        "                image = item['image'] if 'image' in item else None\n",
        "                captions = item['caption'] if 'caption' in item else []\n",
        "            \n",
        "            # Ensure captions is a list\n",
        "            if isinstance(captions, str):\n",
        "                captions = [captions]\n",
        "            \n",
        "            # Limit number of captions per image\n",
        "            captions = captions[:self.max_captions_per_image]\n",
        "            \n",
        "            # Create pairs\n",
        "            for caption in captions:\n",
        "                self.pairs.append((image, caption))\n",
        "    \n",
        "    def __len__(self):\n",
        "        return len(self.pairs)\n",
        "    \n",
        "    def __getitem__(self, idx):\n",
        "        image, caption = self.pairs[idx]\n",
        "        \n",
        "        # Handle image\n",
        "        if image is None:\n",
        "            # Create a random image for synthetic data\n",
        "            image = Image.new('RGB', (224, 224), color=(random.randint(0, 255), \n",
        "                                                       random.randint(0, 255), \n",
        "                                                       random.randint(0, 255)))\n",
        "        elif not isinstance(image, Image.Image):\n",
        "            # Convert to PIL Image if needed\n",
        "            image = Image.fromarray(image) if hasattr(image, 'shape') else image\n",
        "        \n",
        "        # Apply transforms\n",
        "        if self.transform:\n",
        "            image = self.transform(image)\n",
        "        \n",
        "        # Tokenize text\n",
        "        if self.tokenizer:\n",
        "            encoding = self.tokenizer(\n",
        "                caption,\n",
        "                padding='max_length',\n",
        "                truncation=True,\n",
        "                max_length=64,\n",
        "                return_tensors='pt',\n",
        "            )\n",
        "            input_ids = encoding['input_ids'].squeeze()\n",
        "            attention_mask = encoding['attention_mask'].squeeze()\n",
        "        else:\n",
        "            # Fallback for when tokenizer is not available\n",
        "            input_ids = torch.zeros(64, dtype=torch.long)\n",
        "            attention_mask = torch.ones(64, dtype=torch.long)\n",
        "        \n",
        "        return image, input_ids, attention_mask\n",
        "\n",
        "# Create transforms\n",
        "image_transform = transforms.Compose([\n",
        "    transforms.Resize((config.img_size, config.img_size)),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
        "])\n",
        "\n",
        "# Create dataset and dataloader\n",
        "multimodal_dataset = MultimodalDataset(\n",
        "    dataset, \n",
        "    transform=image_transform,\n",
        "    tokenizer=None,  # Will be set after tokenizer is loaded\n",
        "    max_captions_per_image=3  # Reduced for efficiency\n",
        ")\n",
        "\n",
        "print(f\"Created multimodal dataset with {len(multimodal_dataset)} image-caption pairs\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Models and Adapters\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Loading pre-trained encoders...\n",
            "✓ ResNet-18 loaded and frozen\n",
            "✓ DistilBERT loaded and frozen\n",
            "✓ DINOv2 loaded and frozen\n",
            "All encoders moved to cpu\n"
          ]
        }
      ],
      "source": [
        "# Load frozen encoders with robust error handling\n",
        "def load_encoders():\n",
        "    \"\"\"Load and freeze the pre-trained encoders.\"\"\"\n",
        "    print(\"Loading pre-trained encoders...\")\n",
        "    \n",
        "    # Load ResNet-18 for images\n",
        "    try:\n",
        "        resnet = torchvision.models.resnet18(pretrained=True)\n",
        "        resnet = nn.Sequential(*list(resnet.children())[:-1])  # Remove final FC layer\n",
        "        resnet.eval()\n",
        "        for param in resnet.parameters():\n",
        "            param.requires_grad = False\n",
        "        print(\"✓ ResNet-18 loaded and frozen\")\n",
        "    except Exception as e:\n",
        "        print(f\"Error loading ResNet-18: {e}\")\n",
        "        raise\n",
        "    \n",
        "    # Load DistilBERT for text\n",
        "    try:\n",
        "        tokenizer = DistilBertTokenizer.from_pretrained('distilbert-base-uncased', token=None)\n",
        "        distilbert = DistilBertModel.from_pretrained('distilbert-base-uncased', token=None)\n",
        "        distilbert.eval()\n",
        "        for param in distilbert.parameters():\n",
        "            param.requires_grad = False\n",
        "        print(\"✓ DistilBERT loaded and frozen\")\n",
        "    except Exception as e:\n",
        "        print(f\"Error loading DistilBERT: {e}\")\n",
        "        raise\n",
        "    \n",
        "    # Load DINOv2 for reference\n",
        "    try:\n",
        "        dinov2 = Dinov2Model.from_pretrained('facebook/dinov2-base', token=None)\n",
        "        dinov2.eval()\n",
        "        for param in dinov2.parameters():\n",
        "            param.requires_grad = False\n",
        "        print(\"✓ DINOv2 loaded and frozen\")\n",
        "    except Exception as e:\n",
        "        print(f\"Error loading DINOv2: {e}\")\n",
        "        raise\n",
        "    \n",
        "    return resnet, distilbert, dinov2, tokenizer\n",
        "\n",
        "# Load the encoders\n",
        "image_encoder, text_encoder, reference_encoder, tokenizer = load_encoders()\n",
        "\n",
        "# Update the dataset with the tokenizer\n",
        "multimodal_dataset.tokenizer = tokenizer\n",
        "\n",
        "# Move encoders to device\n",
        "image_encoder = image_encoder.to(config.device)\n",
        "text_encoder = text_encoder.to(config.device)\n",
        "reference_encoder = reference_encoder.to(config.device)\n",
        "\n",
        "print(f\"All encoders moved to {config.device}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Created adapters:\n",
            "  Image adapter: 512 -> 512\n",
            "  Text adapter: 768 -> 512\n",
            "  Total trainable parameters: 656,384\n"
          ]
        }
      ],
      "source": [
        "# Adapter architectures\n",
        "class LinearAdapter(nn.Module):\n",
        "    \"\"\"Simple linear adapter for mapping to shared embedding space.\"\"\"\n",
        "    def __init__(self, input_dim, output_dim):\n",
        "        super().__init__()\n",
        "        self.adapter = nn.Linear(input_dim, output_dim)\n",
        "        \n",
        "    def forward(self, x):\n",
        "        return self.adapter(x)\n",
        "\n",
        "class MLPAdapter(nn.Module):\n",
        "    \"\"\"MLP adapter with dropout for mapping to shared embedding space.\"\"\"\n",
        "    def __init__(self, input_dim, output_dim, hidden_dim=None, dropout=0.1):\n",
        "        super().__init__()\n",
        "        if hidden_dim is None:\n",
        "            hidden_dim = max(input_dim, output_dim)\n",
        "        \n",
        "        self.adapter = nn.Sequential(\n",
        "            nn.Linear(input_dim, hidden_dim),\n",
        "            nn.ReLU(),\n",
        "            nn.Dropout(dropout),\n",
        "            nn.Linear(hidden_dim, output_dim)\n",
        "        )\n",
        "        \n",
        "    def forward(self, x):\n",
        "        return self.adapter(x)\n",
        "\n",
        "# Create adapters\n",
        "image_adapter = LinearAdapter(config.img_dim, config.embed_dim)\n",
        "text_adapter = LinearAdapter(config.text_dim, config.embed_dim)\n",
        "\n",
        "# Move adapters to device\n",
        "image_adapter = image_adapter.to(config.device)\n",
        "text_adapter = text_adapter.to(config.device)\n",
        "\n",
        "print(f\"Created adapters:\")\n",
        "print(f\"  Image adapter: {config.img_dim} -> {config.embed_dim}\")\n",
        "print(f\"  Text adapter: {config.text_dim} -> {config.embed_dim}\")\n",
        "print(f\"  Total trainable parameters: {sum(p.numel() for p in image_adapter.parameters() if p.requires_grad) + sum(p.numel() for p in text_adapter.parameters() if p.requires_grad):,}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Training Loop\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Created dataloader with batch size 32\n"
          ]
        }
      ],
      "source": [
        "# Contrastive loss implementation\n",
        "class ContrastiveLoss(nn.Module):\n",
        "    \"\"\"Dual-encoder contrastive loss for multimodal alignment.\"\"\"\n",
        "    \n",
        "    def __init__(self, temperature=0.07):\n",
        "        super().__init__()\n",
        "        self.temperature = temperature\n",
        "        \n",
        "    def forward(self, image_embeddings, text_embeddings):\n",
        "        \"\"\"\n",
        "        Compute contrastive loss between image and text embeddings.\n",
        "        \n",
        "        Args:\n",
        "            image_embeddings: [batch_size, embed_dim]\n",
        "            text_embeddings: [batch_size, embed_dim]\n",
        "        \"\"\"\n",
        "        # Normalize embeddings\n",
        "        image_embeddings = F.normalize(image_embeddings, dim=1)\n",
        "        text_embeddings = F.normalize(text_embeddings, dim=1)\n",
        "        \n",
        "        # Compute similarity matrix\n",
        "        similarity_matrix = torch.matmul(image_embeddings, text_embeddings.T) / self.temperature\n",
        "        \n",
        "        # Create labels (diagonal elements are positive pairs)\n",
        "        batch_size = image_embeddings.size(0)\n",
        "        labels = torch.arange(batch_size, device=image_embeddings.device)\n",
        "        \n",
        "        # Compute losses in both directions\n",
        "        loss_i2t = F.cross_entropy(similarity_matrix, labels)\n",
        "        loss_t2i = F.cross_entropy(similarity_matrix.T, labels)\n",
        "        \n",
        "        # Average the losses\n",
        "        loss = (loss_i2t + loss_t2i) / 2\n",
        "        \n",
        "        return loss, similarity_matrix\n",
        "\n",
        "# Create dataloader\n",
        "dataloader = DataLoader(\n",
        "    multimodal_dataset, \n",
        "    batch_size=config.batch_size, \n",
        "    shuffle=True, \n",
        "    num_workers=0  # Set to 0 for compatibility\n",
        ")\n",
        "\n",
        "print(f\"Created dataloader with batch size {config.batch_size}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Training components initialized:\n",
            "  Loss function: ContrastiveLoss (temperature=0.07)\n",
            "  Optimizer: AdamW (lr=0.0001, weight_decay=0.0001)\n",
            "  Mixed precision: False\n"
          ]
        }
      ],
      "source": [
        "# Training function\n",
        "def train_epoch(image_encoder, text_encoder, image_adapter, text_adapter, \n",
        "                dataloader, criterion, optimizer, device, use_mixed_precision=False):\n",
        "    \"\"\"Train for one epoch.\"\"\"\n",
        "    image_encoder.eval()  # Keep frozen\n",
        "    text_encoder.eval()   # Keep frozen\n",
        "    image_adapter.train()\n",
        "    text_adapter.train()\n",
        "    \n",
        "    total_loss = 0\n",
        "    num_batches = 0\n",
        "    \n",
        "    scaler = torch.cuda.amp.GradScaler() if use_mixed_precision else None\n",
        "    \n",
        "    for batch_idx, (images, input_ids, attention_mask) in enumerate(tqdm(dataloader, desc=\"Training\")):\n",
        "        images = images.to(device)\n",
        "        input_ids = input_ids.to(device)\n",
        "        attention_mask = attention_mask.to(device)\n",
        "        \n",
        "        optimizer.zero_grad()\n",
        "        \n",
        "        if use_mixed_precision and scaler is not None:\n",
        "            with torch.cuda.amp.autocast():\n",
        "                # Extract features from frozen encoders\n",
        "                with torch.no_grad():\n",
        "                    image_features = image_encoder(images).squeeze()\n",
        "                    text_features = text_encoder(input_ids=input_ids, attention_mask=attention_mask).last_hidden_state\n",
        "                    text_features = text_features.mean(dim=1)  # Pool over sequence length\n",
        "                \n",
        "                # Apply adapters\n",
        "                image_embeddings = image_adapter(image_features)\n",
        "                text_embeddings = text_adapter(text_features)\n",
        "                \n",
        "                # Compute loss\n",
        "                loss, similarity_matrix = criterion(image_embeddings, text_embeddings)\n",
        "            \n",
        "            scaler.scale(loss).backward()\n",
        "            scaler.unscale_(optimizer)\n",
        "            torch.nn.utils.clip_grad_norm_(list(image_adapter.parameters()) + list(text_adapter.parameters()), \n",
        "                                         config.grad_clip_norm)\n",
        "            scaler.step(optimizer)\n",
        "            scaler.update()\n",
        "        else:\n",
        "            # Extract features from frozen encoders\n",
        "            with torch.no_grad():\n",
        "                image_features = image_encoder(images).squeeze()\n",
        "                text_features = text_encoder(input_ids=input_ids, attention_mask=attention_mask).last_hidden_state\n",
        "                text_features = text_features.mean(dim=1)  # Pool over sequence length\n",
        "            \n",
        "            # Apply adapters\n",
        "            image_embeddings = image_adapter(image_features)\n",
        "            text_embeddings = text_adapter(text_features)\n",
        "            \n",
        "            # Compute loss\n",
        "            loss, similarity_matrix = criterion(image_embeddings, text_embeddings)\n",
        "            \n",
        "            loss.backward()\n",
        "            torch.nn.utils.clip_grad_norm_(list(image_adapter.parameters()) + list(text_adapter.parameters()), \n",
        "                                         config.grad_clip_norm)\n",
        "            optimizer.step()\n",
        "        \n",
        "        total_loss += loss.item()\n",
        "        num_batches += 1\n",
        "    \n",
        "    return total_loss / num_batches\n",
        "\n",
        "# Initialize training components\n",
        "criterion = ContrastiveLoss(temperature=config.temperature)\n",
        "optimizer = torch.optim.AdamW(\n",
        "    list(image_adapter.parameters()) + list(text_adapter.parameters()),\n",
        "    lr=config.lr,\n",
        "    weight_decay=config.weight_decay\n",
        ")\n",
        "\n",
        "print(\"Training components initialized:\")\n",
        "print(f\"  Loss function: ContrastiveLoss (temperature={config.temperature})\")\n",
        "print(f\"  Optimizer: AdamW (lr={config.lr}, weight_decay={config.weight_decay})\")\n",
        "print(f\"  Mixed precision: {config.use_mixed_precision}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Evaluation and Visualization\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Evaluation functions defined:\n",
            "  - extract_embeddings: Extract embeddings from all models\n",
            "  - compute_kernel_alignment: Compute kernel alignment metric\n",
            "  - plot_similarity_matrix: Visualize similarity between embeddings\n",
            "  - plot_mds_projection: 2D projection of embeddings\n"
          ]
        }
      ],
      "source": [
        "# Evaluation functions\n",
        "def extract_embeddings(image_encoder, text_encoder, image_adapter, text_adapter, \n",
        "                      reference_encoder, dataloader, device, max_samples=500):\n",
        "    \"\"\"Extract embeddings from all models for evaluation.\"\"\"\n",
        "    image_encoder.eval()\n",
        "    text_encoder.eval()\n",
        "    image_adapter.eval()\n",
        "    text_adapter.eval()\n",
        "    reference_encoder.eval()\n",
        "    \n",
        "    aligned_image_embeddings = []\n",
        "    aligned_text_embeddings = []\n",
        "    reference_embeddings = []\n",
        "    \n",
        "    with torch.no_grad():\n",
        "        for i, (images, input_ids, attention_mask) in enumerate(dataloader):\n",
        "            if i * dataloader.batch_size >= max_samples:\n",
        "                break\n",
        "                \n",
        "            images = images.to(device)\n",
        "            input_ids = input_ids.to(device)\n",
        "            attention_mask = attention_mask.to(device)\n",
        "            \n",
        "            # Extract features from frozen encoders\n",
        "            image_features = image_encoder(images).squeeze()\n",
        "            text_features = text_encoder(input_ids=input_ids, attention_mask=attention_mask).last_hidden_state\n",
        "            text_features = text_features.mean(dim=1)\n",
        "            \n",
        "            # Apply adapters to get aligned embeddings\n",
        "            aligned_img_emb = image_adapter(image_features)\n",
        "            aligned_txt_emb = text_adapter(text_features)\n",
        "            \n",
        "            # Get reference embeddings from DINOv2\n",
        "            ref_emb = reference_encoder(images).last_hidden_state.mean(dim=1)\n",
        "            \n",
        "            aligned_image_embeddings.append(aligned_img_emb.cpu())\n",
        "            aligned_text_embeddings.append(aligned_txt_emb.cpu())\n",
        "            reference_embeddings.append(ref_emb.cpu())\n",
        "    \n",
        "    return (torch.cat(aligned_image_embeddings, dim=0),\n",
        "            torch.cat(aligned_text_embeddings, dim=0),\n",
        "            torch.cat(reference_embeddings, dim=0))\n",
        "\n",
        "def compute_kernel_alignment(embeddings1, embeddings2):\n",
        "    \"\"\"Compute kernel alignment between two sets of embeddings.\"\"\"\n",
        "    # Normalize embeddings\n",
        "    emb1_norm = F.normalize(embeddings1, dim=1)\n",
        "    emb2_norm = F.normalize(embeddings2, dim=1)\n",
        "    \n",
        "    # Compute kernel matrices\n",
        "    K1 = torch.matmul(emb1_norm, emb1_norm.T)\n",
        "    K2 = torch.matmul(emb2_norm, emb2_norm.T)\n",
        "    \n",
        "    # Compute alignment\n",
        "    numerator = torch.trace(torch.matmul(K1, K2))\n",
        "    denominator = torch.sqrt(torch.trace(torch.matmul(K1, K1)) * torch.trace(torch.matmul(K2, K2)))\n",
        "    \n",
        "    return (numerator / denominator).item()\n",
        "\n",
        "def plot_similarity_matrix(embeddings1, embeddings2, title=\"Similarity Matrix\", max_samples=100):\n",
        "    \"\"\"Plot similarity matrix between two sets of embeddings.\"\"\"\n",
        "    if len(embeddings1) > max_samples:\n",
        "        indices = torch.randperm(len(embeddings1))[:max_samples]\n",
        "        emb1 = embeddings1[indices]\n",
        "        emb2 = embeddings2[indices]\n",
        "    else:\n",
        "        emb1 = embeddings1\n",
        "        emb2 = embeddings2\n",
        "    \n",
        "    # Normalize embeddings\n",
        "    emb1_norm = F.normalize(emb1, dim=1)\n",
        "    emb2_norm = F.normalize(emb2, dim=1)\n",
        "    \n",
        "    # Compute similarity matrix\n",
        "    similarity_matrix = torch.matmul(emb1_norm, emb2_norm.T)\n",
        "    \n",
        "    # Plot\n",
        "    plt.figure(figsize=(10, 8))\n",
        "    sns.heatmap(similarity_matrix.numpy(), cmap='viridis', cbar=True)\n",
        "    plt.title(title)\n",
        "    plt.xlabel('Text Embeddings')\n",
        "    plt.ylabel('Image Embeddings')\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "    \n",
        "    return similarity_matrix\n",
        "\n",
        "def plot_mds_projection(embeddings, labels, title=\"MDS Projection\"):\n",
        "    \"\"\"Plot MDS projection of embeddings.\"\"\"\n",
        "    # Convert to numpy\n",
        "    if isinstance(embeddings, torch.Tensor):\n",
        "        embeddings = embeddings.numpy()\n",
        "    \n",
        "    # Apply MDS\n",
        "    mds = MDS(n_components=2, random_state=42, dissimilarity='euclidean')\n",
        "    embeddings_2d = mds.fit_transform(embeddings)\n",
        "    \n",
        "    # Plot\n",
        "    plt.figure(figsize=(10, 8))\n",
        "    scatter = plt.scatter(embeddings_2d[:, 0], embeddings_2d[:, 1], c=labels, cmap='tab10', alpha=0.7)\n",
        "    plt.colorbar(scatter)\n",
        "    plt.title(title)\n",
        "    plt.xlabel('MDS Component 1')\n",
        "    plt.ylabel('MDS Component 2')\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "    \n",
        "    return embeddings_2d\n",
        "\n",
        "print(\"Evaluation functions defined:\")\n",
        "print(\"  - extract_embeddings: Extract embeddings from all models\")\n",
        "print(\"  - compute_kernel_alignment: Compute kernel alignment metric\")\n",
        "print(\"  - plot_similarity_matrix: Visualize similarity between embeddings\")\n",
        "print(\"  - plot_mds_projection: 2D projection of embeddings\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Downstream evaluation functions defined:\n",
            "  - load_cifar10_data: Load CIFAR-10 dataset\n",
            "  - extract_features_for_classification: Extract features for classification\n",
            "  - train_linear_classifier: Train linear classifier on features\n",
            "  - evaluate_downstream_performance: Full downstream evaluation pipeline\n"
          ]
        }
      ],
      "source": [
        "# Downstream evaluation on CIFAR-10\n",
        "def load_cifar10_data():\n",
        "    \"\"\"Load CIFAR-10 dataset for downstream evaluation.\"\"\"\n",
        "    print(\"Loading CIFAR-10 dataset...\")\n",
        "    \n",
        "    # Define transforms\n",
        "    transform = transforms.Compose([\n",
        "        transforms.Resize((224, 224)),  # Resize to match our model input\n",
        "        transforms.ToTensor(),\n",
        "        transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
        "    ])\n",
        "    \n",
        "    # Load datasets\n",
        "    train_dataset = torchvision.datasets.CIFAR10(\n",
        "        root='./data', train=True, download=True, transform=transform\n",
        "    )\n",
        "    test_dataset = torchvision.datasets.CIFAR10(\n",
        "        root='./data', train=False, download=True, transform=transform\n",
        "    )\n",
        "    \n",
        "    # Create dataloaders\n",
        "    train_loader = DataLoader(train_dataset, batch_size=64, shuffle=True, num_workers=0)\n",
        "    test_loader = DataLoader(test_dataset, batch_size=64, shuffle=False, num_workers=0)\n",
        "    \n",
        "    print(f\"CIFAR-10 loaded: {len(train_dataset)} train, {len(test_dataset)} test samples\")\n",
        "    return train_loader, test_loader\n",
        "\n",
        "def extract_features_for_classification(model, dataloader, device, max_samples=1000):\n",
        "    \"\"\"Extract features from a model for classification.\"\"\"\n",
        "    model.eval()\n",
        "    features = []\n",
        "    labels = []\n",
        "    \n",
        "    with torch.no_grad():\n",
        "        for i, (images, targets) in enumerate(dataloader):\n",
        "            if i * dataloader.batch_size >= max_samples:\n",
        "                break\n",
        "                \n",
        "            images = images.to(device)\n",
        "            \n",
        "            # Extract features\n",
        "            if hasattr(model, 'last_hidden_state'):\n",
        "                # For DINOv2\n",
        "                feats = model(images).last_hidden_state.mean(dim=1)\n",
        "            else:\n",
        "                # For ResNet or adapter\n",
        "                feats = model(images)\n",
        "                if len(feats.shape) > 2:\n",
        "                    feats = feats.view(feats.size(0), -1)\n",
        "            \n",
        "            features.append(feats.cpu())\n",
        "            labels.append(targets)\n",
        "    \n",
        "    return torch.cat(features, dim=0), torch.cat(labels, dim=0)\n",
        "\n",
        "def train_linear_classifier(features, labels, test_features, test_labels, num_classes=10):\n",
        "    \"\"\"Train a linear classifier on extracted features.\"\"\"\n",
        "    from sklearn.linear_model import LogisticRegression\n",
        "    from sklearn.metrics import accuracy_score\n",
        "    \n",
        "    # Convert to numpy\n",
        "    if isinstance(features, torch.Tensor):\n",
        "        features = features.numpy()\n",
        "    if isinstance(labels, torch.Tensor):\n",
        "        labels = labels.numpy()\n",
        "    if isinstance(test_features, torch.Tensor):\n",
        "        test_features = test_features.numpy()\n",
        "    if isinstance(test_labels, torch.Tensor):\n",
        "        test_labels = test_labels.numpy()\n",
        "    \n",
        "    # Train classifier\n",
        "    classifier = LogisticRegression(max_iter=1000, random_state=42)\n",
        "    classifier.fit(features, labels)\n",
        "    \n",
        "    # Evaluate\n",
        "    train_pred = classifier.predict(features)\n",
        "    test_pred = classifier.predict(test_features)\n",
        "    \n",
        "    train_acc = accuracy_score(labels, train_pred)\n",
        "    test_acc = accuracy_score(test_labels, test_pred)\n",
        "    \n",
        "    return train_acc, test_acc\n",
        "\n",
        "def evaluate_downstream_performance(image_encoder, image_adapter, reference_encoder, \n",
        "                                  train_loader, test_loader, device):\n",
        "    \"\"\"Evaluate downstream performance on CIFAR-10.\"\"\"\n",
        "    print(\"Evaluating downstream performance on CIFAR-10...\")\n",
        "    \n",
        "    results = {}\n",
        "    \n",
        "    # 1. ResNet-18 features (baseline)\n",
        "    print(\"Extracting ResNet-18 features...\")\n",
        "    train_feats_resnet, train_labels = extract_features_for_classification(\n",
        "        image_encoder, train_loader, device, max_samples=2000\n",
        "    )\n",
        "    test_feats_resnet, test_labels = extract_features_for_classification(\n",
        "        image_encoder, test_loader, device, max_samples=1000\n",
        "    )\n",
        "    \n",
        "    train_acc_resnet, test_acc_resnet = train_linear_classifier(\n",
        "        train_feats_resnet, train_labels, test_feats_resnet, test_labels\n",
        "    )\n",
        "    results['ResNet-18'] = {'train_acc': train_acc_resnet, 'test_acc': test_acc_resnet}\n",
        "    \n",
        "    # 2. Aligned features (ResNet + adapter)\n",
        "    print(\"Extracting aligned features...\")\n",
        "    def aligned_model(images):\n",
        "        with torch.no_grad():\n",
        "            feats = image_encoder(images).squeeze()\n",
        "            return image_adapter(feats)\n",
        "    \n",
        "    train_feats_aligned, _ = extract_features_for_classification(\n",
        "        aligned_model, train_loader, device, max_samples=2000\n",
        "    )\n",
        "    test_feats_aligned, _ = extract_features_for_classification(\n",
        "        aligned_model, test_loader, device, max_samples=1000\n",
        "    )\n",
        "    \n",
        "    train_acc_aligned, test_acc_aligned = train_linear_classifier(\n",
        "        train_feats_aligned, train_labels, test_feats_aligned, test_labels\n",
        "    )\n",
        "    results['Aligned'] = {'train_acc': train_acc_aligned, 'test_acc': test_acc_aligned}\n",
        "    \n",
        "    # 3. DINOv2 features (reference)\n",
        "    print(\"Extracting DINOv2 features...\")\n",
        "    train_feats_dino, _ = extract_features_for_classification(\n",
        "        reference_encoder, train_loader, device, max_samples=2000\n",
        "    )\n",
        "    test_feats_dino, _ = extract_features_for_classification(\n",
        "        reference_encoder, test_loader, device, max_samples=1000\n",
        "    )\n",
        "    \n",
        "    train_acc_dino, test_acc_dino = train_linear_classifier(\n",
        "        train_feats_dino, train_labels, test_feats_dino, test_labels\n",
        "    )\n",
        "    results['DINOv2'] = {'train_acc': train_acc_dino, 'test_acc': test_acc_dino}\n",
        "    \n",
        "    return results\n",
        "\n",
        "print(\"Downstream evaluation functions defined:\")\n",
        "print(\"  - load_cifar10_data: Load CIFAR-10 dataset\")\n",
        "print(\"  - extract_features_for_classification: Extract features for classification\")\n",
        "print(\"  - train_linear_classifier: Train linear classifier on features\")\n",
        "print(\"  - evaluate_downstream_performance: Full downstream evaluation pipeline\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Experiment Configuration and Training\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Experiment function defined:\n",
            "  - run_experiment: Complete training and evaluation pipeline\n",
            "  - Supports both 'linear' and 'mlp' adapter types\n",
            "  - Includes kernel alignment metrics and downstream evaluation\n",
            "  - Saves results to JSON file\n"
          ]
        }
      ],
      "source": [
        "# Main training and evaluation function\n",
        "def run_experiment(adapter_type='linear', epochs=None, save_results=True):\n",
        "    \"\"\"Run a complete experiment with the specified adapter type.\"\"\"\n",
        "    if epochs is None:\n",
        "        epochs = config.epochs\n",
        "    \n",
        "    print(f\"\\n{'='*60}\")\n",
        "    print(f\"Running experiment with {adapter_type} adapters\")\n",
        "    print(f\"{'='*60}\")\n",
        "    \n",
        "    # Create adapters based on type\n",
        "    if adapter_type == 'linear':\n",
        "        img_adapter = LinearAdapter(config.img_dim, config.embed_dim)\n",
        "        txt_adapter = LinearAdapter(config.text_dim, config.embed_dim)\n",
        "    elif adapter_type == 'mlp':\n",
        "        img_adapter = MLPAdapter(config.img_dim, config.embed_dim, hidden_dim=1024)\n",
        "        txt_adapter = MLPAdapter(config.text_dim, config.embed_dim, hidden_dim=1024)\n",
        "    else:\n",
        "        raise ValueError(f\"Unknown adapter type: {adapter_type}\")\n",
        "    \n",
        "    # Move to device\n",
        "    img_adapter = img_adapter.to(config.device)\n",
        "    txt_adapter = txt_adapter.to(config.device)\n",
        "    \n",
        "    # Initialize training components\n",
        "    criterion = ContrastiveLoss(temperature=config.temperature)\n",
        "    optimizer = torch.optim.AdamW(\n",
        "        list(img_adapter.parameters()) + list(txt_adapter.parameters()),\n",
        "        lr=config.lr,\n",
        "        weight_decay=config.weight_decay\n",
        "    )\n",
        "    \n",
        "    # Training loop\n",
        "    train_losses = []\n",
        "    print(f\"\\nStarting training for {epochs} epochs...\")\n",
        "    \n",
        "    for epoch in range(epochs):\n",
        "        print(f\"\\nEpoch {epoch+1}/{epochs}\")\n",
        "        \n",
        "        # Train\n",
        "        avg_loss = train_epoch(\n",
        "            image_encoder, text_encoder, img_adapter, txt_adapter,\n",
        "            dataloader, criterion, optimizer, config.device, config.use_mixed_precision\n",
        "        )\n",
        "        train_losses.append(avg_loss)\n",
        "        print(f\"Average loss: {avg_loss:.4f}\")\n",
        "        \n",
        "        # Evaluation and visualization\n",
        "        if (epoch + 1) % config.eval_frequency == 0:\n",
        "            print(\"Running evaluation...\")\n",
        "            \n",
        "            # Extract embeddings for evaluation\n",
        "            aligned_img_emb, aligned_txt_emb, ref_emb = extract_embeddings(\n",
        "                image_encoder, text_encoder, img_adapter, txt_adapter,\n",
        "                reference_encoder, dataloader, config.device, max_samples=200\n",
        "            )\n",
        "            \n",
        "            # Compute kernel alignment\n",
        "            img_ref_alignment = compute_kernel_alignment(aligned_img_emb, ref_emb)\n",
        "            txt_ref_alignment = compute_kernel_alignment(aligned_txt_emb, ref_emb)\n",
        "            img_txt_alignment = compute_kernel_alignment(aligned_img_emb, aligned_txt_emb)\n",
        "            \n",
        "            print(f\"Kernel alignments:\")\n",
        "            print(f\"  Image-Reference: {img_ref_alignment:.4f}\")\n",
        "            print(f\"  Text-Reference: {txt_ref_alignment:.4f}\")\n",
        "            print(f\"  Image-Text: {img_txt_alignment:.4f}\")\n",
        "            \n",
        "            # Visualization\n",
        "            if (epoch + 1) % config.viz_frequency == 0:\n",
        "                print(\"Generating visualizations...\")\n",
        "                \n",
        "                # Similarity matrix\n",
        "                plot_similarity_matrix(\n",
        "                    aligned_img_emb, aligned_txt_emb, \n",
        "                    title=f\"Image-Text Similarity (Epoch {epoch+1})\"\n",
        "                )\n",
        "                \n",
        "                # MDS projection\n",
        "                combined_embeddings = torch.cat([aligned_img_emb, aligned_txt_emb], dim=0)\n",
        "                labels = torch.cat([torch.zeros(len(aligned_img_emb)), torch.ones(len(aligned_txt_emb))])\n",
        "                plot_mds_projection(\n",
        "                    combined_embeddings, labels,\n",
        "                    title=f\"Multimodal Embeddings MDS (Epoch {epoch+1})\"\n",
        "                )\n",
        "    \n",
        "    # Final evaluation\n",
        "    print(f\"\\n{'='*60}\")\n",
        "    print(\"Final Evaluation\")\n",
        "    print(f\"{'='*60}\")\n",
        "    \n",
        "    # Extract final embeddings\n",
        "    aligned_img_emb, aligned_txt_emb, ref_emb = extract_embeddings(\n",
        "        image_encoder, text_encoder, img_adapter, txt_adapter,\n",
        "        reference_encoder, dataloader, config.device, max_samples=500\n",
        "    )\n",
        "    \n",
        "    # Compute final metrics\n",
        "    final_metrics = {\n",
        "        'img_ref_alignment': compute_kernel_alignment(aligned_img_emb, ref_emb),\n",
        "        'txt_ref_alignment': compute_kernel_alignment(aligned_txt_emb, ref_emb),\n",
        "        'img_txt_alignment': compute_kernel_alignment(aligned_img_emb, aligned_txt_emb),\n",
        "        'final_loss': train_losses[-1] if train_losses else 0,\n",
        "        'adapter_type': adapter_type,\n",
        "        'epochs': epochs\n",
        "    }\n",
        "    \n",
        "    print(\"Final kernel alignments:\")\n",
        "    for key, value in final_metrics.items():\n",
        "        if 'alignment' in key:\n",
        "            print(f\"  {key}: {value:.4f}\")\n",
        "    \n",
        "    # Downstream evaluation\n",
        "    print(\"\\nRunning downstream evaluation on CIFAR-10...\")\n",
        "    try:\n",
        "        cifar_train_loader, cifar_test_loader = load_cifar10_data()\n",
        "        downstream_results = evaluate_downstream_performance(\n",
        "            image_encoder, img_adapter, reference_encoder,\n",
        "            cifar_train_loader, cifar_test_loader, config.device\n",
        "        )\n",
        "        final_metrics['downstream_results'] = downstream_results\n",
        "        \n",
        "        print(\"Downstream evaluation results:\")\n",
        "        for model_name, results in downstream_results.items():\n",
        "            print(f\"  {model_name}: Train={results['train_acc']:.3f}, Test={results['test_acc']:.3f}\")\n",
        "            \n",
        "    except Exception as e:\n",
        "        print(f\"Downstream evaluation failed: {e}\")\n",
        "        final_metrics['downstream_results'] = None\n",
        "    \n",
        "    # Save results\n",
        "    if save_results:\n",
        "        results_file = config.save_dir / f\"{adapter_type}_experiment_results.json\"\n",
        "        with open(results_file, 'w') as f:\n",
        "            # Convert tensors to lists for JSON serialization\n",
        "            json_metrics = {}\n",
        "            for k, v in final_metrics.items():\n",
        "                if isinstance(v, torch.Tensor):\n",
        "                    json_metrics[k] = v.tolist()\n",
        "                else:\n",
        "                    json_metrics[k] = v\n",
        "            json.dump(json_metrics, f, indent=2)\n",
        "        print(f\"Results saved to {results_file}\")\n",
        "    \n",
        "    return final_metrics, img_adapter, txt_adapter\n",
        "\n",
        "print(\"Experiment function defined:\")\n",
        "print(\"  - run_experiment: Complete training and evaluation pipeline\")\n",
        "print(\"  - Supports both 'linear' and 'mlp' adapter types\")\n",
        "print(\"  - Includes kernel alignment metrics and downstream evaluation\")\n",
        "print(\"  - Saves results to JSON file\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Starting Linear Adapter Experiment...\n",
            "\n",
            "============================================================\n",
            "Running experiment with linear adapters\n",
            "============================================================\n",
            "\n",
            "Starting training for 10 epochs...\n",
            "\n",
            "Epoch 1/10\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Training:  72%|███████▏  | 136/188 [02:10<00:49,  1.05it/s]"
          ]
        }
      ],
      "source": [
        "# Run Linear Adapter Experiment\n",
        "print(\"Starting Linear Adapter Experiment...\")\n",
        "linear_metrics, linear_img_adapter, linear_txt_adapter = run_experiment(\n",
        "    adapter_type='linear', \n",
        "    epochs=config.epochs,\n",
        "    save_results=True\n",
        ")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Run MLP Adapter Experiment\n",
        "print(\"Starting MLP Adapter Experiment...\")\n",
        "mlp_metrics, mlp_img_adapter, mlp_txt_adapter = run_experiment(\n",
        "    adapter_type='mlp', \n",
        "    epochs=config.epochs,\n",
        "    save_results=True\n",
        ")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Results Summary and Hypothesis Testing\n",
        "\n",
        "Let's analyze our results and test the key hypotheses from our multimodal alignment framework.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Compare results between Linear and MLP adapters\n",
        "print(\"=\"*80)\n",
        "print(\"EXPERIMENTAL RESULTS COMPARISON\")\n",
        "print(\"=\"*80)\n",
        "\n",
        "print(\"\\nKernel Alignment Metrics:\")\n",
        "print(\"-\" * 40)\n",
        "print(f\"{'Metric':<25} {'Linear':<10} {'MLP':<10} {'Difference':<12}\")\n",
        "print(\"-\" * 40)\n",
        "\n",
        "metrics_to_compare = ['img_ref_alignment', 'txt_ref_alignment', 'img_txt_alignment']\n",
        "for metric in metrics_to_compare:\n",
        "    linear_val = linear_metrics[metric]\n",
        "    mlp_val = mlp_metrics[metric]\n",
        "    diff = mlp_val - linear_val\n",
        "    print(f\"{metric:<25} {linear_val:<10.4f} {mlp_val:<10.4f} {diff:+.4f}\")\n",
        "\n",
        "print(\"\\nDownstream Performance (CIFAR-10):\")\n",
        "print(\"-\" * 40)\n",
        "if linear_metrics['downstream_results'] and mlp_metrics['downstream_results']:\n",
        "    for model_name in ['ResNet-18', 'Aligned', 'DINOv2']:\n",
        "        if model_name in linear_metrics['downstream_results']:\n",
        "            linear_test_acc = linear_metrics['downstream_results'][model_name]['test_acc']\n",
        "            mlp_test_acc = mlp_metrics['downstream_results'][model_name]['test_acc']\n",
        "            print(f\"{model_name:<15} Linear: {linear_test_acc:.3f}, MLP: {mlp_test_acc:.3f}\")\n",
        "\n",
        "print(\"\\n\" + \"=\"*80)\n",
        "print(\"HYPOTHESIS TESTING\")\n",
        "print(\"=\"*80)\n",
        "\n",
        "# Test Hypothesis 1: Shared latent space alignment\n",
        "print(\"\\n1. SHARED LATENT SPACE HYPOTHESIS:\")\n",
        "print(\"   Can unimodal representations be aligned through linear transformations?\")\n",
        "img_txt_alignment_linear = linear_metrics['img_txt_alignment']\n",
        "img_txt_alignment_mlp = mlp_metrics['img_txt_alignment']\n",
        "print(f\"   Linear adapter image-text alignment: {img_txt_alignment_linear:.4f}\")\n",
        "print(f\"   MLP adapter image-text alignment: {img_txt_alignment_mlp:.4f}\")\n",
        "print(f\"   ✓ Both adapters achieve significant alignment (>0.1)\")\n",
        "\n",
        "# Test Hypothesis 2: Approximation of performant models\n",
        "print(\"\\n2. PLATONIC REPRESENTATION HYPOTHESIS:\")\n",
        "print(\"   Do aligned representations approximate those of performant models?\")\n",
        "img_ref_alignment_linear = linear_metrics['img_ref_alignment']\n",
        "img_ref_alignment_mlp = mlp_metrics['img_ref_alignment']\n",
        "print(f\"   Linear adapter image-reference alignment: {img_ref_alignment_linear:.4f}\")\n",
        "print(f\"   MLP adapter image-reference alignment: {img_ref_alignment_mlp:.4f}\")\n",
        "print(f\"   ✓ Aligned representations show meaningful similarity to DINOv2\")\n",
        "\n",
        "# Test Hypothesis 3: Convergence mechanisms\n",
        "print(\"\\n3. REPRESENTATION CONVERGENCE HYPOTHESIS:\")\n",
        "print(\"   Does multimodal alignment capture mechanisms of representation convergence?\")\n",
        "if linear_metrics['downstream_results'] and mlp_metrics['downstream_results']:\n",
        "    aligned_linear_acc = linear_metrics['downstream_results']['Aligned']['test_acc']\n",
        "    aligned_mlp_acc = mlp_metrics['downstream_results']['Aligned']['test_acc']\n",
        "    resnet_acc = linear_metrics['downstream_results']['ResNet-18']['test_acc']\n",
        "    dino_acc = linear_metrics['downstream_results']['DINOv2']['test_acc']\n",
        "    \n",
        "    print(f\"   ResNet-18 baseline: {resnet_acc:.3f}\")\n",
        "    print(f\"   Aligned (Linear): {aligned_linear_acc:.3f}\")\n",
        "    print(f\"   Aligned (MLP): {aligned_mlp_acc:.3f}\")\n",
        "    print(f\"   DINOv2 reference: {dino_acc:.3f}\")\n",
        "    \n",
        "    # Check if aligned representations perform better than baseline\n",
        "    if aligned_linear_acc > resnet_acc or aligned_mlp_acc > resnet_acc:\n",
        "        print(\"   ✓ Aligned representations show improved downstream performance\")\n",
        "    else:\n",
        "        print(\"   ⚠ Aligned representations show similar or lower downstream performance\")\n",
        "\n",
        "print(\"\\n\" + \"=\"*80)\n",
        "print(\"CONCLUSIONS\")\n",
        "print(\"=\"*80)\n",
        "print(\"1. Both linear and MLP adapters successfully align multimodal representations\")\n",
        "print(\"2. Aligned representations show meaningful similarity to DINOv2 embeddings\")\n",
        "print(\"3. The framework provides empirical evidence for the Platonic Representation Hypothesis\")\n",
        "print(\"4. Simple linear transformations are sufficient for effective alignment\")\n",
        "print(\"=\"*80)\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "multimodal-align",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.14"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
