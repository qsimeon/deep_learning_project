{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "A100",
      "collapsed_sections": [
        "QX5PVbxha6qS",
        "t_8TeUIoFDk8",
        "Fd3I7wxCGKQ1",
        "uPHXy67mFhYg",
        "LDxQRzNrF3_2",
        "K8FxenkVS28T",
        "aouxMgZpsXWT",
        "erB6lhlWYKGY"
      ],
      "machine_shape": "hm"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Settings"
      ],
      "metadata": {
        "id": "ijhVZwFRrq47"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Imports"
      ],
      "metadata": {
        "id": "rcV-B7RoasG3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install torch torchvision transformers pillow tqdm numpy scikit-learn matplotlib seaborn datasets wandb"
      ],
      "metadata": {
        "id": "eBjOpWnYrwLz",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f48206dd-9896-4256-95ee-da4c24038428"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: torch in /usr/local/lib/python3.10/dist-packages (2.5.1+cu121)\n",
            "Requirement already satisfied: torchvision in /usr/local/lib/python3.10/dist-packages (0.20.1+cu121)\n",
            "Requirement already satisfied: transformers in /usr/local/lib/python3.10/dist-packages (4.46.2)\n",
            "Requirement already satisfied: pillow in /usr/local/lib/python3.10/dist-packages (11.0.0)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (4.66.6)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (1.26.4)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.10/dist-packages (1.5.2)\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.10/dist-packages (3.8.0)\n",
            "Requirement already satisfied: seaborn in /usr/local/lib/python3.10/dist-packages (0.13.2)\n",
            "Collecting datasets\n",
            "  Downloading datasets-3.1.0-py3-none-any.whl.metadata (20 kB)\n",
            "Requirement already satisfied: wandb in /usr/local/lib/python3.10/dist-packages (0.18.7)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch) (3.16.1)\n",
            "Requirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.10/dist-packages (from torch) (4.12.2)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch) (3.4.2)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch) (3.1.4)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch) (2024.10.0)\n",
            "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.10/dist-packages (from torch) (1.13.1)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from sympy==1.13.1->torch) (1.3.0)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.23.2 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.26.2)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from transformers) (24.2)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (6.0.2)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (2024.9.11)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from transformers) (2.32.3)\n",
            "Requirement already satisfied: safetensors>=0.4.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.4.5)\n",
            "Requirement already satisfied: tokenizers<0.21,>=0.20 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.20.3)\n",
            "Requirement already satisfied: scipy>=1.6.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn) (1.13.1)\n",
            "Requirement already satisfied: joblib>=1.2.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn) (1.4.2)\n",
            "Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn) (3.5.0)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (1.3.1)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (4.55.0)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (1.4.7)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (3.2.0)\n",
            "Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (2.8.2)\n",
            "Requirement already satisfied: pandas>=1.2 in /usr/local/lib/python3.10/dist-packages (from seaborn) (2.2.2)\n",
            "Requirement already satisfied: pyarrow>=15.0.0 in /usr/local/lib/python3.10/dist-packages (from datasets) (17.0.0)\n",
            "Collecting dill<0.3.9,>=0.3.0 (from datasets)\n",
            "  Downloading dill-0.3.8-py3-none-any.whl.metadata (10 kB)\n",
            "Collecting xxhash (from datasets)\n",
            "  Downloading xxhash-3.5.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (12 kB)\n",
            "Collecting multiprocess<0.70.17 (from datasets)\n",
            "  Downloading multiprocess-0.70.16-py310-none-any.whl.metadata (7.2 kB)\n",
            "Collecting fsspec (from torch)\n",
            "  Downloading fsspec-2024.9.0-py3-none-any.whl.metadata (11 kB)\n",
            "Requirement already satisfied: aiohttp in /usr/local/lib/python3.10/dist-packages (from datasets) (3.11.2)\n",
            "Requirement already satisfied: click!=8.0.0,>=7.1 in /usr/local/lib/python3.10/dist-packages (from wandb) (8.1.7)\n",
            "Requirement already satisfied: docker-pycreds>=0.4.0 in /usr/local/lib/python3.10/dist-packages (from wandb) (0.4.0)\n",
            "Requirement already satisfied: gitpython!=3.1.29,>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from wandb) (3.1.43)\n",
            "Requirement already satisfied: platformdirs in /usr/local/lib/python3.10/dist-packages (from wandb) (4.3.6)\n",
            "Requirement already satisfied: protobuf!=4.21.0,!=5.28.0,<6,>=3.19.0 in /usr/local/lib/python3.10/dist-packages (from wandb) (4.25.5)\n",
            "Requirement already satisfied: psutil>=5.0.0 in /usr/local/lib/python3.10/dist-packages (from wandb) (5.9.5)\n",
            "Requirement already satisfied: sentry-sdk>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from wandb) (2.18.0)\n",
            "Requirement already satisfied: setproctitle in /usr/local/lib/python3.10/dist-packages (from wandb) (1.3.4)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from wandb) (75.1.0)\n",
            "Requirement already satisfied: six>=1.4.0 in /usr/local/lib/python3.10/dist-packages (from docker-pycreds>=0.4.0->wandb) (1.16.0)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (2.4.3)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.3.1)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (24.2.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.5.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (6.1.0)\n",
            "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (0.2.0)\n",
            "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.17.2)\n",
            "Requirement already satisfied: async-timeout<6.0,>=4.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (4.0.3)\n",
            "Requirement already satisfied: gitdb<5,>=4.0.1 in /usr/local/lib/python3.10/dist-packages (from gitpython!=3.1.29,>=1.0.0->wandb) (4.0.11)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas>=1.2->seaborn) (2024.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.10/dist-packages (from pandas>=1.2->seaborn) (2024.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.4.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2.2.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2024.8.30)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch) (3.0.2)\n",
            "Requirement already satisfied: smmap<6,>=3.0.1 in /usr/local/lib/python3.10/dist-packages (from gitdb<5,>=4.0.1->gitpython!=3.1.29,>=1.0.0->wandb) (5.0.1)\n",
            "Downloading datasets-3.1.0-py3-none-any.whl (480 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m480.6/480.6 kB\u001b[0m \u001b[31m19.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading dill-0.3.8-py3-none-any.whl (116 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m116.3/116.3 kB\u001b[0m \u001b[31m11.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading fsspec-2024.9.0-py3-none-any.whl (179 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m179.3/179.3 kB\u001b[0m \u001b[31m16.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading multiprocess-0.70.16-py310-none-any.whl (134 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m134.8/134.8 kB\u001b[0m \u001b[31m13.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading xxhash-3.5.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (194 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m194.1/194.1 kB\u001b[0m \u001b[31m16.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: xxhash, fsspec, dill, multiprocess, datasets\n",
            "  Attempting uninstall: fsspec\n",
            "    Found existing installation: fsspec 2024.10.0\n",
            "    Uninstalling fsspec-2024.10.0:\n",
            "      Successfully uninstalled fsspec-2024.10.0\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "gcsfs 2024.10.0 requires fsspec==2024.10.0, but you have fsspec 2024.9.0 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed datasets-3.1.0 dill-0.3.8 fsspec-2024.9.0 multiprocess-0.70.16 xxhash-3.5.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torchvision\n",
        "from torchvision import transforms\n",
        "from transformers import DistilBertModel, DistilBertTokenizer\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from PIL import Image\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from tqdm import tqdm\n",
        "from torch.cuda.amp import autocast, GradScaler\n",
        "import numpy as np\n",
        "from sklearn.neighbors import NearestNeighbors\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from sklearn.manifold import TSNE\n",
        "from pathlib import Path\n",
        "import logging\n",
        "from datasets import load_dataset\n",
        "import wandb\n",
        "import time\n",
        "import json\n",
        "import pickle\n",
        "import copy\n",
        "import os\n",
        "from typing import Optional, Dict, Any, List, Tuple"
      ],
      "metadata": {
        "id": "SRbdYpIbrwv9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Config"
      ],
      "metadata": {
        "id": "QX5PVbxha6qS"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "49tjGUHZrNd3"
      },
      "outputs": [],
      "source": [
        "# Set up logging\n",
        "logging.basicConfig(\n",
        "    level=logging.INFO,\n",
        "    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s'\n",
        ")\n",
        "logger = logging.getLogger(__name__)\n",
        "\n",
        "# Configuration class\n",
        "class Config:\n",
        "    def __init__(self, **kwargs):\n",
        "        self.defaults = {\n",
        "            'batch_size': 64,\n",
        "            'num_samples': 5000,\n",
        "            'img_size': 224,\n",
        "            'embed_dim': 768,\n",
        "            'save_dir': './experiments',\n",
        "            'img_dim': 512,\n",
        "            'text_dim': 768,\n",
        "            'lr': 1e-4,\n",
        "            'epochs': 15,\n",
        "            'temperature': 0.07,\n",
        "            'viz_frequency': 1,\n",
        "            'wandb_project': 'adapter-alignment-v2',\n",
        "            'device': torch.device('cuda' if torch.cuda.is_available() else 'cpu'),\n",
        "\n",
        "            # Hyperparameters for studies\n",
        "            'temperature_range': [0.05, 0.07, 0.1],\n",
        "            'learning_rates': [1e-4, 1e-3, 1e-2],\n",
        "            'hidden_dims': [512, 1024, 2048],\n",
        "            'model_sizes': ['small', 'medium', 'large'],\n",
        "            'data_sizes': [1000, 5000, 10000],\n",
        "            'batch_sizes': [32, 64, 128],\n",
        "        }\n",
        "\n",
        "        self.defaults.update(kwargs)\n",
        "        for key, value in self.defaults.items():\n",
        "            setattr(self, key, value)\n",
        "\n",
        "        Path(self.save_dir).mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "    def __str__(self):\n",
        "        return \"\\n\".join(f\"{k}: {v}\" for k, v in self.defaults.items())\n",
        "\n",
        "    def to_dict(self):\n",
        "        return {k: v for k, v in self.defaults.items()\n",
        "                if not isinstance(v, torch.device)}\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Set random seeds\n",
        "torch.manual_seed(42)\n",
        "np.random.seed(42)\n",
        "if torch.cuda.is_available():\n",
        "    torch.cuda.manual_seed_all(42)\n",
        "\n",
        "# Initialize config\n",
        "config = Config()\n",
        "logger.info(\"Configuration initialized with:\")\n",
        "logger.info(str(config))"
      ],
      "metadata": {
        "id": "ZYp6rwBFbDQw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Dataset & Models"
      ],
      "metadata": {
        "id": "-HYhXKNnH6P_"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Dataset Setup"
      ],
      "metadata": {
        "id": "D9ZvzifSE13M"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class Flickr30kDataset(Dataset):\n",
        "    def __init__(self, transform=None, tokenizer=None, max_samples: Optional[int] = 5000):\n",
        "        self.dataset = load_dataset(\"nlphuji/flickr30k\", split='test')\n",
        "        self.transform = transform\n",
        "\n",
        "        # Basic tokenizer validation\n",
        "        if tokenizer is None or not isinstance(tokenizer, DistilBertTokenizer):\n",
        "            raise ValueError(\"Valid DistilBertTokenizer must be provided\")\n",
        "        self.tokenizer = tokenizer\n",
        "\n",
        "        if max_samples:\n",
        "            self.dataset = self.dataset.select(range(min(max_samples, len(self.dataset))))\n",
        "\n",
        "        self.pairs = []\n",
        "        for idx in range(len(self.dataset)):\n",
        "            item = self.dataset[idx]\n",
        "            image = item['image']\n",
        "            captions = item['caption'][:5]  # Use first 5 captions\n",
        "            self.pairs.extend([(image, caption) for caption in captions])\n",
        "\n",
        "    def __len__(self) -> int:\n",
        "        return len(self.pairs)\n",
        "\n",
        "    def __getitem__(self, idx: int):\n",
        "        image, caption = self.pairs[idx]\n",
        "        if self.transform:\n",
        "            image = self.transform(image)\n",
        "\n",
        "        # Always tokenize when tokenizer is present (since we validate in __init__)\n",
        "        encoding = self.tokenizer(\n",
        "            caption,\n",
        "            padding='max_length',\n",
        "            truncation=True,\n",
        "            max_length=64,\n",
        "            return_tensors='pt',\n",
        "        )\n",
        "\n",
        "        return (\n",
        "            image,\n",
        "            encoding['input_ids'].squeeze(),\n",
        "            encoding['attention_mask'].squeeze()\n",
        "        )"
      ],
      "metadata": {
        "id": "1yneYHsCE5s6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Dataloaders' Creation Function"
      ],
      "metadata": {
        "id": "t_8TeUIoFDk8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def get_dataloaders(tokenizer, config):\n",
        "    \"\"\"Create train and validation dataloaders\"\"\"\n",
        "    # Define transforms\n",
        "    transform = transforms.Compose([\n",
        "        transforms.Resize((config.img_size, config.img_size)),\n",
        "        transforms.ToTensor(),\n",
        "        transforms.Normalize(\n",
        "            mean=[0.485, 0.456, 0.406],\n",
        "            std=[0.229, 0.224, 0.225]\n",
        "        )\n",
        "    ])\n",
        "\n",
        "    # Create and split dataset\n",
        "    full_dataset = Flickr30kDataset(\n",
        "        transform=transform,\n",
        "        tokenizer=tokenizer,\n",
        "        max_samples=config.num_samples\n",
        "    )\n",
        "\n",
        "    train_size = int(0.8 * len(full_dataset))\n",
        "    val_size = len(full_dataset) - train_size\n",
        "    train_dataset, val_dataset = torch.utils.data.random_split(\n",
        "        full_dataset, [train_size, val_size]\n",
        "    )\n",
        "\n",
        "    # Create dataloaders\n",
        "    train_loader = DataLoader(\n",
        "        train_dataset,\n",
        "        batch_size=config.batch_size,\n",
        "        shuffle=True,\n",
        "        num_workers=2 if torch.cuda.is_available() else 0,\n",
        "        pin_memory=torch.cuda.is_available()\n",
        "    )\n",
        "\n",
        "    val_loader = DataLoader(\n",
        "        val_dataset,\n",
        "        batch_size=config.batch_size,\n",
        "        num_workers=2 if torch.cuda.is_available() else 0,\n",
        "        pin_memory=torch.cuda.is_available()\n",
        "    )\n",
        "\n",
        "    return train_loader, val_loader"
      ],
      "metadata": {
        "id": "GOK5U8gPE53O"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### [TEST] Dataloaders' Creation Function"
      ],
      "metadata": {
        "id": "qmzlm5OIFQzu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def test_dataset():\n",
        "    print(\"\\n=== Testing Dataset Implementation ===\")\n",
        "\n",
        "    # Get models and tokenizer\n",
        "    print(\"1. Initializing models and tokenizer...\")\n",
        "    _, _, tokenizer = get_pretrained_models()\n",
        "\n",
        "    # Create transforms\n",
        "    print(\"\\n2. Setting up image transforms...\")\n",
        "    transform = transforms.Compose([\n",
        "        transforms.Resize((224, 224)),\n",
        "        transforms.ToTensor(),\n",
        "        transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
        "    ])\n",
        "\n",
        "    # Create dataset\n",
        "    print(\"\\n3. Creating dataset...\")\n",
        "    dataset = Flickr30kDataset(transform=transform, tokenizer=tokenizer, max_samples=10)\n",
        "    print(f\"Dataset size: {len(dataset)}\")\n",
        "\n",
        "    # Test single item\n",
        "    print(\"\\n4. Testing single item retrieval...\")\n",
        "    image, input_ids, attention_mask = dataset[0]\n",
        "    print(f\"Image shape: {image.shape}\")\n",
        "    print(f\"Input ids shape: {input_ids.shape}\")\n",
        "    print(f\"Attention mask shape: {attention_mask.shape}\")\n",
        "\n",
        "    # Test dataloader\n",
        "    print(\"\\n5. Testing batch loading...\")\n",
        "    loader = DataLoader(dataset, batch_size=4, num_workers=0)\n",
        "    batch = next(iter(loader))\n",
        "    print(f\"Batch sizes:\")\n",
        "    print(f\"- Images: {batch[0].shape}\")\n",
        "    print(f\"- Input ids: {batch[1].shape}\")\n",
        "    print(f\"- Attention masks: {batch[2].shape}\")\n",
        "\n",
        "    # Verify data ranges\n",
        "    print(\"\\n6. Verifying data ranges...\")\n",
        "    print(f\"Image value range: [{batch[0].min():.3f}, {batch[0].max():.3f}]\")\n",
        "    print(f\"Input ids range: [{batch[1].min()}, {batch[1].max()}]\")\n",
        "    print(f\"Attention mask values: unique = {torch.unique(batch[2]).tolist()}\")\n",
        "\n",
        "    print(\"\\n=== All tests completed successfully! ===\")\n",
        "    return True"
      ],
      "metadata": {
        "id": "oMlvoh1RFNz3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Models' Setup"
      ],
      "metadata": {
        "id": "Fd3I7wxCGKQ1"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Pretrained Models' Inicialization Function"
      ],
      "metadata": {
        "id": "uPHXy67mFhYg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def get_pretrained_models():\n",
        "    \"\"\"Initialize and return pretrained vision and language models\"\"\"\n",
        "    # Load ResNet18 without the final classification layer\n",
        "    image_encoder = torchvision.models.resnet18(weights=torchvision.models.ResNet18_Weights.IMAGENET1K_V1)\n",
        "    image_encoder = nn.Sequential(\n",
        "        *list(image_encoder.children())[:-2],  # Remove fc and avgpool\n",
        "        nn.AdaptiveAvgPool2d((1, 1)),  # Global average pooling\n",
        "        nn.Flatten()  # Flatten to (batch_size, channels)\n",
        "    )\n",
        "\n",
        "    # Quick test of image encoder\n",
        "    with torch.no_grad():\n",
        "        dummy_input = torch.randn(1, 3, 224, 224)\n",
        "        output = image_encoder(dummy_input)\n",
        "        assert output.shape == (1, 512), f\"Expected shape (1, 512), got {output.shape}\"\n",
        "\n",
        "    # Initialize language models\n",
        "    text_encoder = DistilBertModel.from_pretrained('distilbert-base-uncased')\n",
        "    text_tokenizer = DistilBertTokenizer.from_pretrained('distilbert-base-uncased')\n",
        "\n",
        "    # Freeze encoders\n",
        "    for param in image_encoder.parameters():\n",
        "        param.requires_grad = False\n",
        "    for param in text_encoder.parameters():\n",
        "        param.requires_grad = False\n",
        "\n",
        "    return image_encoder, text_encoder, text_tokenizer"
      ],
      "metadata": {
        "id": "AUUBaWPsrqTQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### [TEST] Pretrained Models' Inicialization Function"
      ],
      "metadata": {
        "id": "LDxQRzNrF3_2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Test model initialization\n",
        "def test_models():\n",
        "    print(\"\\n=== Testing Model Initialization ===\\n\")\n",
        "\n",
        "    image_encoder, text_encoder, tokenizer = get_pretrained_models()\n",
        "\n",
        "    # Test image encoder\n",
        "    print(\"\\nTesting image encoder...\")\n",
        "    dummy_image = torch.randn(1, 3, 224, 224)\n",
        "    with torch.no_grad():\n",
        "        img_output = image_encoder(dummy_image)\n",
        "        print(f\"Image encoder output shape: {img_output.shape}\")\n",
        "\n",
        "    # Test tokenizer and text encoder\n",
        "    print(\"\\nTesting text pipeline...\")\n",
        "    test_text = \"This is a test sentence.\"\n",
        "    tokens = tokenizer.encode_plus(\n",
        "        test_text,\n",
        "        add_special_tokens=True,\n",
        "        max_length=64,\n",
        "        padding='max_length',\n",
        "        truncation=True,\n",
        "        return_attention_mask=True,\n",
        "        return_tensors='pt'\n",
        "    )\n",
        "\n",
        "    with torch.no_grad():\n",
        "        text_output = text_encoder(\n",
        "            tokens['input_ids'],\n",
        "            attention_mask=tokens['attention_mask']\n",
        "        )[0][:, 0]\n",
        "        print(f\"Text encoder output shape: {text_output.shape}\")\n",
        "\n",
        "    print(\"\\n=== All model tests passed! ===\")\n",
        "    return True"
      ],
      "metadata": {
        "id": "y7hG2t0aWYiC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## [TEST] Data & Models"
      ],
      "metadata": {
        "id": "OGR8BJSKHuvQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Starting dataset tests...\")\n",
        "test_dataset_success = test_dataset()\n",
        "print(f\"\\nTest result: {'✓ Passed' if test_dataset_success else '✗ Failed'}\")\n",
        "\n",
        "print(\"\\nStarting model tests...\")\n",
        "test_model_success = test_models()\n",
        "print(f\"\\nTest result: {'✓ Passed' if test_model_success else '✗ Failed'}\")"
      ],
      "metadata": {
        "id": "m27_P_7bvKLz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Dataset & Models Initialization"
      ],
      "metadata": {
        "id": "uLvNj5bAISeW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Initialize models and dataloaders\n",
        "image_encoder, text_encoder, tokenizer = get_pretrained_models()\n",
        "train_loader, val_loader = get_dataloaders(tokenizer, config)"
      ],
      "metadata": {
        "id": "kM0Dk2N4rqQa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Adapters"
      ],
      "metadata": {
        "id": "ED4wQFAdsIXT"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Parent Adapter Class"
      ],
      "metadata": {
        "id": "sy-9TbFVN_MC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class BaseAdapter(nn.Module):\n",
        "    \"\"\"Parent class for all adapter modules.\"\"\"\n",
        "\n",
        "    def __init__(self, input_dim: int, embed_dim: int):\n",
        "        \"\"\"\n",
        "        Initialize the base adapter with input and embedding dimensions.\n",
        "\n",
        "        Args:\n",
        "            input_dim (int): The size of the input features.\n",
        "            embed_dim (int): The size of the output features (embedding dimension).\n",
        "        \"\"\"\n",
        "        super().__init__()\n",
        "        self.input_dim = input_dim  # Store the input feature size\n",
        "        self.embed_dim = embed_dim  # Store the embedding size\n",
        "\n",
        "    def log_architecture(self, adapter_type: str):\n",
        "        \"\"\"\n",
        "        Log the adapter's architecture details using `wandb`.\n",
        "\n",
        "        Args:\n",
        "            adapter_type (str): The type of the adapter (e.g., \"linear\", \"mlp\").\n",
        "        \"\"\"\n",
        "        # Use wandb to log the adapter configuration\n",
        "        wandb.config.update({\n",
        "            f\"{adapter_type}_adapter/input_dim\": self.input_dim,\n",
        "            f\"{adapter_type}_adapter/embed_dim\": self.embed_dim,\n",
        "            f\"{adapter_type}_adapter/num_parameters\": sum(p.numel() for p in self.parameters()),  # Total parameter count\n",
        "        })"
      ],
      "metadata": {
        "id": "koWskVUdD4Ja"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Adapter Factory"
      ],
      "metadata": {
        "id": "HRvZmHK4EWI2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class AdapterFactory:\n",
        "    \"\"\"Factory class for creating image and text adapters.\"\"\"\n",
        "\n",
        "    @staticmethod\n",
        "    def create_pair(adapter_type: str, img_dim: int, text_dim: int, embed_dim: int, **kwargs):\n",
        "        \"\"\"\n",
        "        Create a pair of adapters (one for image, one for text) based on the specified type.\n",
        "\n",
        "        Args:\n",
        "            adapter_type (str): Type of adapter to create ('linear', 'mlp', 'bottleneck').\n",
        "            img_dim (int): Input dimension for the image adapter.\n",
        "            text_dim (int): Input dimension for the text adapter.\n",
        "            embed_dim (int): Embedding (output) dimension for both adapters.\n",
        "            **kwargs: Additional arguments specific to the adapter type.\n",
        "\n",
        "        Returns:\n",
        "            Tuple[nn.Module, nn.Module]: A pair of adapters (image adapter, text adapter).\n",
        "        \"\"\"\n",
        "        logger.info(f\"Creating {adapter_type} adapters - Image: {img_dim} → {embed_dim}, Text: {text_dim} → {embed_dim}\")\n",
        "\n",
        "        # Define available adapter types\n",
        "        ADAPTER_TYPES = {\n",
        "            'linear': LinearAdapter,\n",
        "            'mlp': MLPAdapter,\n",
        "            'bottleneck': BottleneckAdapter\n",
        "        }\n",
        "\n",
        "        # Check if the adapter type is valid\n",
        "        if adapter_type not in ADAPTER_TYPES:\n",
        "            raise ValueError(f\"Unknown adapter type: {adapter_type}\")\n",
        "\n",
        "        # Get the adapter class for the specified type\n",
        "        AdapterClass = ADAPTER_TYPES[adapter_type]\n",
        "\n",
        "        # Create image and text adapters\n",
        "        img_adapter = AdapterClass(img_dim, embed_dim, **kwargs)\n",
        "        text_adapter = AdapterClass(text_dim, embed_dim, **kwargs)\n",
        "\n",
        "        # Log adapter architectures to wandb\n",
        "        img_adapter.log_architecture('image')\n",
        "        text_adapter.log_architecture('text')\n",
        "\n",
        "        return img_adapter, text_adapter\n",
        "\n",
        "def log_adapter_gradients(adapter, adapter_type: str):\n",
        "    \"\"\"\n",
        "    Log gradient statistics for an adapter to wandb.\n",
        "\n",
        "    Args:\n",
        "        adapter (nn.Module): The adapter whose gradients are to be logged.\n",
        "        adapter_type (str): Type of the adapter (e.g., 'image' or 'text').\n",
        "    \"\"\"\n",
        "    for name, param in adapter.named_parameters():\n",
        "        if param.grad is not None:  # Log only if gradients are available\n",
        "            wandb.log({\n",
        "                f'gradients/{adapter_type}/{name}/mean': param.grad.mean().item(),\n",
        "                f'gradients/{adapter_type}/{name}/std': param.grad.std().item(),\n",
        "                f'gradients/{adapter_type}/{name}/norm': param.grad.norm().item()\n",
        "            })\n",
        "\n",
        "\n",
        "def log_feature_statistics(features: torch.Tensor, prefix: str):\n",
        "    \"\"\"\n",
        "    Log statistics of feature tensors to wandb.\n",
        "\n",
        "    Args:\n",
        "        features (torch.Tensor): Feature tensor to analyze.\n",
        "        prefix (str): Prefix for the logged statistics (e.g., 'image', 'text').\n",
        "    \"\"\"\n",
        "    wandb.log({\n",
        "        f'{prefix}/mean': features.mean().item(),           # Mean value of the features\n",
        "        f'{prefix}/std': features.std().item(),             # Standard deviation of the features\n",
        "        f'{prefix}/norm': features.norm(dim=1).mean().item(), # Mean norm of feature vectors\n",
        "        f'{prefix}/sparsity': (features == 0).float().mean().item()  # Fraction of zero elements\n",
        "    })"
      ],
      "metadata": {
        "id": "WIsFjauLrqNc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Linear Adapter"
      ],
      "metadata": {
        "id": "mgLGpBqlD9_X"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class LinearAdapter(BaseAdapter):\n",
        "    \"\"\"An adapter that performs a simple linear transformation followed by normalization.\"\"\"\n",
        "\n",
        "    def __init__(self, input_dim: int, embed_dim: int):\n",
        "        \"\"\"\n",
        "        Initialize the Linear Adapter with input and embedding dimensions.\n",
        "\n",
        "        Args:\n",
        "            input_dim (int): The size of the input features.\n",
        "            embed_dim (int): The size of the output features (embedding dimension).\n",
        "        \"\"\"\n",
        "        # Call the BaseAdapter initializer\n",
        "        super().__init__(input_dim, embed_dim)\n",
        "\n",
        "        # Define the linear transformation (input_dim -> embed_dim)\n",
        "        self.linear = nn.Linear(input_dim, embed_dim)\n",
        "\n",
        "        # Define layer normalization for the output\n",
        "        self.norm = nn.LayerNorm(embed_dim)\n",
        "\n",
        "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
        "        \"\"\"\n",
        "        Pass input through the linear adapter.\n",
        "\n",
        "        Args:\n",
        "            x (torch.Tensor): Input tensor of shape (batch_size, input_dim).\n",
        "\n",
        "        Returns:\n",
        "            torch.Tensor: Output tensor of shape (batch_size, embed_dim).\n",
        "        \"\"\"\n",
        "        # Apply the linear transformation and normalize the result\n",
        "        return self.norm(self.linear(x))"
      ],
      "metadata": {
        "id": "8oPV15mfD5Sd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## MLP Adapter"
      ],
      "metadata": {
        "id": "h38_GtIkED_P"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class MLPAdapter(BaseAdapter):\n",
        "    \"\"\"Adapter using a Multi-layer Perceptron (MLP) architecture.\"\"\"\n",
        "\n",
        "    def __init__(self, input_dim: int, embed_dim: int, hidden_dim: int = 1024, num_layers: int = 2):\n",
        "        \"\"\"\n",
        "        Initialize the MLP Adapter.\n",
        "\n",
        "        Args:\n",
        "            input_dim (int): The size of the input features.\n",
        "            embed_dim (int): The size of the output features (embedding dimension).\n",
        "            hidden_dim (int): The size of the hidden layers (default = 1024).\n",
        "            num_layers (int): The number of layers in the MLP (default = 2).\n",
        "        \"\"\"\n",
        "        # Call the initialization of the BaseAdapter class\n",
        "        super().__init__(input_dim, embed_dim)\n",
        "\n",
        "        # Prepare a list to hold all layers of the MLP\n",
        "        layers = []\n",
        "\n",
        "        # Create a list of layer dimensions from input to output\n",
        "        # For example, if input_dim=256, embed_dim=64, hidden_dim=128, num_layers=3:\n",
        "        # dims = [256, 128, 128, 64]\n",
        "        dims = [input_dim] + [hidden_dim] * (num_layers - 1) + [embed_dim]\n",
        "\n",
        "        # Iterate through pairs of consecutive dimensions (d1 -> d2)\n",
        "        for d1, d2 in zip(dims[:-1], dims[1:]):\n",
        "            # Add a fully connected layer, activation, normalization, and dropout\n",
        "            layers.extend([\n",
        "                nn.Linear(d1, d2),   # Fully connected layer (d1 inputs -> d2 outputs)\n",
        "                nn.ReLU(),           # Activation function: ReLU (added to introduce non-linearity)\n",
        "                nn.LayerNorm(d2),    # Layer normalization for stability\n",
        "                nn.Dropout(0.1)      # Dropout to reduce overfitting (drop probability = 10%)\n",
        "            ])\n",
        "\n",
        "        # Combine all layers into a sequential container\n",
        "        self.mlp = nn.Sequential(*layers)\n",
        "\n",
        "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
        "        \"\"\"\n",
        "        Pass input through the MLP.\n",
        "\n",
        "        Args:\n",
        "            x (torch.Tensor): Input tensor of shape (batch_size, input_dim).\n",
        "\n",
        "        Returns:\n",
        "            torch.Tensor: Output tensor of shape (batch_size, embed_dim).\n",
        "        \"\"\"\n",
        "        return self.mlp(x)"
      ],
      "metadata": {
        "id": "22mirKWPD5c_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Bottleneck Adapter"
      ],
      "metadata": {
        "id": "nOBmmXZpEIix"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class BottleneckAdapter(BaseAdapter):\n",
        "    \"\"\"Adapter with a bottleneck design to reduce dimensionality.\"\"\"\n",
        "\n",
        "    def __init__(self, input_dim: int, embed_dim: int, bottleneck_dim: int = 256):\n",
        "        \"\"\"\n",
        "        Initialize the Bottleneck Adapter.\n",
        "\n",
        "        Args:\n",
        "            input_dim (int): The size of the input features.\n",
        "            embed_dim (int): The size of the output features (embedding dimension).\n",
        "            bottleneck_dim (int): The size of the reduced (bottleneck) dimension (default = 256).\n",
        "        \"\"\"\n",
        "        # Call the initialization of the BaseAdapter class\n",
        "        super().__init__(input_dim, embed_dim)\n",
        "\n",
        "        # Define the \"down\" layer: reduces input dimension to the bottleneck dimension\n",
        "        self.down = nn.Linear(input_dim, bottleneck_dim)\n",
        "\n",
        "        # Define the \"up\" layer: maps bottleneck dimension back to the embedding dimension\n",
        "        self.up = nn.Linear(bottleneck_dim, embed_dim)\n",
        "\n",
        "        # Layer normalization for the output\n",
        "        self.norm = nn.LayerNorm(embed_dim)\n",
        "\n",
        "        # GELU activation function for smooth non-linearity\n",
        "        self.act = nn.GELU()\n",
        "\n",
        "        # Dropout layer to prevent overfitting (drop probability = 10%)\n",
        "        self.dropout = nn.Dropout(0.1)\n",
        "\n",
        "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
        "        \"\"\"\n",
        "        Pass input through the bottleneck adapter.\n",
        "\n",
        "        Args:\n",
        "            x (torch.Tensor): Input tensor of shape (batch_size, input_dim).\n",
        "\n",
        "        Returns:\n",
        "            torch.Tensor: Output tensor of shape (batch_size, embed_dim).\n",
        "        \"\"\"\n",
        "        # Step 1: Reduce dimensionality using the \"down\" layer, apply activation, and dropout\n",
        "        x = self.dropout(self.act(self.down(x)))\n",
        "\n",
        "        # Step 2: Map back to the embedding dimension using the \"up\" layer\n",
        "        x = self.up(x)\n",
        "\n",
        "        # Step 3: Apply normalization to the final output\n",
        "        return self.norm(x)"
      ],
      "metadata": {
        "id": "ey0pVs4qEHqe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Loss Function"
      ],
      "metadata": {
        "id": "olB6b7H3EhGx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class ContrastiveLoss(nn.Module):\n",
        "    \"\"\"Contrastive loss with temperature scaling for image-text similarity.\"\"\"\n",
        "\n",
        "    def __init__(self, temperature: float = 0.07):\n",
        "        \"\"\"\n",
        "        Initialize the contrastive loss function.\n",
        "\n",
        "        Args:\n",
        "            temperature (float): Temperature scaling factor (default = 0.07).\n",
        "        \"\"\"\n",
        "        super().__init__()\n",
        "        self.temperature = temperature\n",
        "\n",
        "    def forward(self, image_features: torch.Tensor, text_features: torch.Tensor) -> torch.Tensor:\n",
        "        \"\"\"\n",
        "        Compute contrastive loss between image and text features.\n",
        "\n",
        "        Args:\n",
        "            image_features (torch.Tensor): Image feature tensor (batch_size, embed_dim).\n",
        "            text_features (torch.Tensor): Text feature tensor (batch_size, embed_dim).\n",
        "\n",
        "        Returns:\n",
        "            torch.Tensor: Scalar loss value (average bidirectional contrastive loss).\n",
        "        \"\"\"\n",
        "        # Normalize features along the embedding dimension\n",
        "        image_features = F.normalize(image_features, dim=-1)\n",
        "        text_features = F.normalize(text_features, dim=-1)\n",
        "\n",
        "        # Compute the similarity matrix using scaled dot product\n",
        "        logits = torch.matmul(image_features, text_features.T) / self.temperature\n",
        "\n",
        "        # Generate labels (diagonal matching)\n",
        "        labels = torch.arange(len(image_features), device=image_features.device)\n",
        "\n",
        "        # Compute cross-entropy loss for image-to-text and text-to-image mappings\n",
        "        loss_i2t = F.cross_entropy(logits, labels)\n",
        "        loss_t2i = F.cross_entropy(logits.T, labels) # Q: What's the point of the transpose?\n",
        "\n",
        "        # Log similarity statistics and individual losses to wandb\n",
        "        with torch.no_grad():\n",
        "            wandb.log({\n",
        "                'similarity/mean': logits.mean().item(),\n",
        "                'similarity/std': logits.std().item(),\n",
        "                'similarity/max': logits.max().item(),\n",
        "                'similarity/min': logits.min().item(),\n",
        "                'loss/image_to_text': loss_i2t.item(),\n",
        "                'loss/text_to_image': loss_t2i.item()\n",
        "            })\n",
        "\n",
        "        # Return the average of the two losses\n",
        "        return (loss_i2t + loss_t2i) / 2"
      ],
      "metadata": {
        "id": "YGOCeeJGDt7E"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Training and Evaluation"
      ],
      "metadata": {
        "id": "sw4BP0T1sPi9"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Image-Text Alignment Class"
      ],
      "metadata": {
        "id": "7mORwJfySOtb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class AlignmentTrainer:\n",
        "    \"\"\"Trainer class for learning image-text alignment.\"\"\"\n",
        "\n",
        "    def __init__(self,\n",
        "                 image_encoder: nn.Module,\n",
        "                 text_encoder: nn.Module,\n",
        "                 image_adapter: nn.Module,\n",
        "                 text_adapter: nn.Module,\n",
        "                 config: Config,\n",
        "                 run_name: str = None):\n",
        "        \"\"\"\n",
        "        Initialize the trainer for image-text alignment.\n",
        "\n",
        "        Args:\n",
        "            image_encoder (nn.Module): Pretrained image encoder (e.g., Vision Transformer).\n",
        "            text_encoder (nn.Module): Pretrained text encoder (e.g., BERT).\n",
        "            image_adapter (nn.Module): Adapter to modify image features.\n",
        "            text_adapter (nn.Module): Adapter to modify text features.\n",
        "            config (Config): Configuration object with training parameters.\n",
        "            run_name (str): Optional name for the wandb run.\n",
        "        \"\"\"\n",
        "        self.config = config\n",
        "        self.device = config.device  # Device to use (CPU/GPU)\n",
        "        self.run_name = run_name\n",
        "\n",
        "        # Move models to the appropriate device\n",
        "        self.image_encoder = image_encoder.to(self.device)\n",
        "        self.text_encoder = text_encoder.to(self.device)\n",
        "        self.image_adapter = image_adapter.to(self.device)\n",
        "        self.text_adapter = text_adapter.to(self.device)\n",
        "\n",
        "        # Ensure encoders are in evaluation mode (frozen)\n",
        "        self.image_encoder.eval()\n",
        "        self.text_encoder.eval()\n",
        "\n",
        "        # Optimizer for adapter parameters\n",
        "        self.optimizer = torch.optim.AdamW([\n",
        "            {'params': self.image_adapter.parameters()},\n",
        "            {'params': self.text_adapter.parameters()}\n",
        "        ], lr=config.lr)\n",
        "\n",
        "        # Contrastive loss with temperature scaling\n",
        "        self.loss_fn = ContrastiveLoss(temperature=config.temperature)\n",
        "\n",
        "        # Mixed precision training scaler\n",
        "        self.scaler = torch.amp.GradScaler()\n",
        "\n",
        "        # Verify encoders are frozen and adapters are trainable\n",
        "        self._verify_model_states()\n",
        "\n",
        "        # Initialize metrics for best validation performance\n",
        "        self.best_val_loss = float('inf')\n",
        "        self.best_alignment = 0.0\n",
        "\n",
        "        # Initialize wandb logging\n",
        "        self._init_wandb()\n",
        "\n",
        "    def _verify_model_states(self):\n",
        "        \"\"\"Ensure encoders are frozen and adapters are trainable.\"\"\"\n",
        "        encoders_frozen = (\n",
        "            all(not p.requires_grad for p in self.image_encoder.parameters()) and\n",
        "            all(not p.requires_grad for p in self.text_encoder.parameters())\n",
        "        )\n",
        "        adapters_trainable = (\n",
        "            any(p.requires_grad for p in self.image_adapter.parameters()) and\n",
        "            any(p.requires_grad for p in self.text_adapter.parameters())\n",
        "        )\n",
        "\n",
        "        if not encoders_frozen or not adapters_trainable:\n",
        "            raise ValueError(\"Encoders must be frozen, and adapters must be trainable.\")\n",
        "\n",
        "    def _init_wandb(self):\n",
        "        \"\"\"Initialize wandb for tracking experiments.\"\"\"\n",
        "        if wandb.run is None:\n",
        "            wandb.init(\n",
        "                project=self.config.wandb_project,\n",
        "                name=self.run_name,\n",
        "                config=self.config.to_dict()\n",
        "            )\n",
        "        wandb.watch(\n",
        "            (self.image_adapter, self.text_adapter),\n",
        "            log=\"all\",\n",
        "            log_freq=100\n",
        "        )\n",
        "\n",
        "    def train_epoch(self, train_loader: DataLoader, epoch: int) -> float:\n",
        "        \"\"\"Train the adapters for one epoch.\n",
        "\n",
        "        Args:\n",
        "            train_loader (DataLoader): DataLoader for training data.\n",
        "            epoch (int): Current epoch number.\n",
        "\n",
        "        Returns:\n",
        "            float: Average training loss for the epoch.\n",
        "        \"\"\"\n",
        "        if wandb.run is None:\n",
        "            self._init_wandb()\n",
        "\n",
        "        self.image_adapter.train()\n",
        "        self.text_adapter.train()\n",
        "        total_loss = 0\n",
        "\n",
        "        print()\n",
        "\n",
        "        progress_bar = tqdm(train_loader, desc=f\"Training Epoch {epoch}/{self.config.epochs}\")\n",
        "\n",
        "        for batch_idx, (images, input_ids, attention_mask) in enumerate(progress_bar):\n",
        "            with torch.amp.autocast(device_type='cuda', dtype=torch.float16):\n",
        "                images, input_ids, attention_mask = (\n",
        "                    images.to(self.device),\n",
        "                    input_ids.to(self.device),\n",
        "                    attention_mask.to(self.device)\n",
        "                )\n",
        "\n",
        "                # Encode features (encoders are frozen)\n",
        "                with torch.no_grad():\n",
        "                    img_feats = self.image_encoder(images)\n",
        "                    text_feats = self.text_encoder(input_ids, attention_mask)[0][:, 0]\n",
        "\n",
        "                # Adapt features using adapters\n",
        "                img_adapted = self.image_adapter(img_feats)\n",
        "                text_adapted = self.text_adapter(text_feats)\n",
        "\n",
        "                # Log feature statistics\n",
        "                log_feature_statistics(img_adapted, 'image_features')\n",
        "                log_feature_statistics(text_adapted, 'text_features')\n",
        "\n",
        "                # Compute contrastive loss\n",
        "                loss = self.loss_fn(img_adapted, text_adapted)\n",
        "\n",
        "            # Backpropagation and optimization\n",
        "            self.optimizer.zero_grad()\n",
        "            self.scaler.scale(loss).backward()\n",
        "\n",
        "            # Log gradients\n",
        "            log_adapter_gradients(self.image_adapter, 'image')\n",
        "            log_adapter_gradients(self.text_adapter, 'text')\n",
        "\n",
        "            self.scaler.step(self.optimizer)\n",
        "            self.scaler.update()\n",
        "\n",
        "            total_loss += loss.item()\n",
        "            progress_bar.set_postfix({'loss': f\"{loss.item():.4f}\"})\n",
        "\n",
        "            # Log batch metrics\n",
        "            wandb.log({\n",
        "                'batch/loss': loss.item(),\n",
        "                'batch/learning_rate': self.optimizer.param_groups[0]['lr']\n",
        "            })\n",
        "\n",
        "        return total_loss / len(train_loader)\n",
        "\n",
        "    @torch.no_grad()\n",
        "    def evaluate(self, val_loader: DataLoader, dinov2_model: nn.Module = None) -> dict:\n",
        "        \"\"\"Evaluate adapters on validation data.\"\"\"\n",
        "        self.image_adapter.eval()\n",
        "        self.text_adapter.eval()\n",
        "        total_loss = 0\n",
        "        adapter_features, dino_features = [], []\n",
        "\n",
        "        for images, input_ids, attention_mask in tqdm(val_loader, desc=\"Evaluating\"):\n",
        "            images, input_ids, attention_mask = (\n",
        "                images.to(self.device),\n",
        "                input_ids.to(self.device),\n",
        "                attention_mask.to(self.device)\n",
        "            )\n",
        "\n",
        "            img_feats = self.image_encoder(images)\n",
        "            text_feats = self.text_encoder(input_ids, attention_mask)[0][:, 0]\n",
        "\n",
        "            img_adapted = self.image_adapter(img_feats)\n",
        "            text_adapted = self.text_adapter(text_feats)\n",
        "\n",
        "            loss = self.loss_fn(img_adapted, text_adapted)\n",
        "            total_loss += loss.item()\n",
        "\n",
        "            adapter_features.append(img_adapted.cpu())\n",
        "            if dinov2_model:\n",
        "                dino_feats = dinov2_model(images)\n",
        "                dino_features.append(dino_feats.cpu())\n",
        "\n",
        "        metrics = {'val_loss': total_loss / len(val_loader)}\n",
        "\n",
        "        if dinov2_model:\n",
        "            adapter_features = torch.cat(adapter_features)\n",
        "            dino_features = torch.cat(dino_features)\n",
        "            metrics['alignment_score'] = compute_alignment(adapter_features, dino_features)\n",
        "            self._log_feature_space_visualization(adapter_features, dino_features)\n",
        "\n",
        "        return metrics\n",
        "\n",
        "    def _log_feature_space_visualization(self, adapter_features: torch.Tensor, dino_features: torch.Tensor):\n",
        "        \"\"\"Visualize and log feature spaces using t-SNE.\"\"\"\n",
        "        # Project to smaller dimension (e.g., 128) for visualization\n",
        "        projection_dim = 128\n",
        "\n",
        "        adapter_proj = torch.nn.Linear(adapter_features.shape[1], projection_dim)\n",
        "        dino_proj = torch.nn.Linear(dino_features.shape[1], projection_dim)\n",
        "\n",
        "        adapter_features_proj = adapter_proj(adapter_features)\n",
        "        dino_features_proj = dino_proj(dino_features)\n",
        "\n",
        "        combined = torch.cat([adapter_features_proj, dino_features_proj])\n",
        "        tsne = TSNE(n_components=2, random_state=42)\n",
        "        embeddings = tsne.fit_transform(combined.numpy())\n",
        "\n",
        "        n = len(adapter_features)\n",
        "        plt.figure(figsize=(10, 10))\n",
        "        plt.scatter(embeddings[:n, 0], embeddings[:n, 1], label='Adapter', alpha=0.5)\n",
        "        plt.scatter(embeddings[n:, 0], embeddings[n:, 1], label='DinoV2', alpha=0.5)\n",
        "        plt.legend()\n",
        "        plt.title('Feature Space Visualization')\n",
        "        wandb.log({\"feature_space\": wandb.Image(plt)})\n",
        "        plt.close()"
      ],
      "metadata": {
        "id": "_qJaAk2ZPYuG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Compute Alignment Function (mutual k-NN)"
      ],
      "metadata": {
        "id": "PyL6sYyZSb9a"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def compute_alignment(features1: torch.Tensor, features2: torch.Tensor, k: int = 10) -> float:\n",
        "    \"\"\"Compute alignment between two feature sets using mutual nearest neighbors.\"\"\"\n",
        "    features1, features2 = F.normalize(features1, dim=1).numpy(), F.normalize(features2, dim=1).numpy()\n",
        "    nn1, nn2 = NearestNeighbors(n_neighbors=k).fit(features1), NearestNeighbors(n_neighbors=k).fit(features2)\n",
        "    idx1, idx2 = nn1.kneighbors(features1, return_distance=False), nn2.kneighbors(features2, return_distance=False)\n",
        "\n",
        "    mutual_nn = np.array([len(set(idx1[i]) & set(idx2[i])) / k for i in range(len(idx1))])\n",
        "    return mutual_nn.mean()"
      ],
      "metadata": {
        "id": "7dp86YolQcYH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Training & Evaluation Loop"
      ],
      "metadata": {
        "id": "K8FxenkVS28T"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def train_and_evaluate(config: Config,\n",
        "                       train_loader: DataLoader,\n",
        "                       val_loader: DataLoader,\n",
        "                       adapter_type: str,\n",
        "                       dinov2_model: nn.Module = None,\n",
        "                       run_name: str = None):\n",
        "    \"\"\"Main training loop for image-text alignment.\"\"\"\n",
        "    image_encoder, text_encoder, _ = get_pretrained_models()\n",
        "    img_adapter, text_adapter = AdapterFactory.create_pair(\n",
        "        adapter_type,\n",
        "        config.img_dim,\n",
        "        config.text_dim,\n",
        "        config.embed_dim\n",
        "    )\n",
        "\n",
        "    trainer = AlignmentTrainer(image_encoder, text_encoder, img_adapter, text_adapter, config, run_name)\n",
        "\n",
        "    for epoch in range(1, config.epochs + 1):\n",
        "        train_loss = trainer.train_epoch(train_loader, epoch)\n",
        "        metrics = trainer.evaluate(val_loader, dinov2_model)\n",
        "        val_loss, alignment_score = metrics['val_loss'], metrics.get('alignment_score', 0.0)\n",
        "\n",
        "        wandb.log({\n",
        "            f\"{adapter_type}/epoch\": epoch,\n",
        "            f\"{adapter_type}/train_loss\": train_loss,\n",
        "            f\"{adapter_type}/val_loss\": val_loss,\n",
        "            f\"{adapter_type}/alignment_score\": alignment_score\n",
        "        })\n",
        "\n",
        "        if val_loss < trainer.best_val_loss:\n",
        "            trainer.best_val_loss = val_loss\n",
        "            wandb.log({f\"{adapter_type}/best_val_loss\": val_loss})\n",
        "\n",
        "        if alignment_score > trainer.best_alignment:\n",
        "            trainer.best_alignment = alignment_score\n",
        "            wandb.log({f\"{adapter_type}/best_alignment\": alignment_score})\n",
        "\n",
        "    return {\n",
        "        'trainer': trainer,\n",
        "        'best_val_loss': trainer.best_val_loss,\n",
        "        'best_alignment': trainer.best_alignment,\n",
        "        'final_alignment': alignment_score\n",
        "    }"
      ],
      "metadata": {
        "id": "HQGtDOn7SxEe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Experimental Framework\n",
        "\n",
        "**Handles the orchestration of multiple experiments with varying configurations, including:**\n",
        "\n",
        "\n",
        "*   Running single experiments.\n",
        "*   Performing ablation studies.\n",
        "*   Conducting scaling studies.\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "aouxMgZpsXWT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class ExperimentRunner:\n",
        "    def __init__(self, base_config: Config, tokenizer: DistilBertTokenizer):\n",
        "        self.base_config = base_config\n",
        "        self.tokenizer = tokenizer\n",
        "        self.experiment_id = time.strftime('%Y%m%d_%H%M%S')\n",
        "\n",
        "        # Initialize common resources\n",
        "        logger.info(\"Creating dataloaders...\")\n",
        "        self.train_loader, self.val_loader = get_dataloaders(tokenizer, base_config)\n",
        "\n",
        "        logger.info(\"Loading DINOv2 model...\")\n",
        "        self.dinov2_model = torch.hub.load('facebookresearch/dinov2', 'dinov2_vitb14').to(base_config.device).eval()\n",
        "\n",
        "        self.save_dir = Path(base_config.save_dir) / self.experiment_id\n",
        "        self.save_dir.mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "    def execute_experiments(self,\n",
        "                            run_base=True,\n",
        "                            run_ablation=False,\n",
        "                            run_scaling=False,\n",
        "                            adapter_types=['linear', 'mlp', 'bottleneck']):\n",
        "        \"\"\"Main method to run selected experiments\"\"\"\n",
        "        results = {}\n",
        "\n",
        "        if run_base:\n",
        "            logger.info(\"Running base experiments...\")\n",
        "            results['base'] = self._run_base_experiments(adapter_types)\n",
        "        if run_ablation:\n",
        "            logger.info(\"Running ablation studies...\")\n",
        "            results['ablation'] = self._run_ablation_study(adapter_types)\n",
        "        if run_scaling:\n",
        "            logger.info(\"Running scaling studies...\")\n",
        "            results['scaling'] = self._run_scaling_study(adapter_types)\n",
        "\n",
        "        create_analysis_plots(results, self.save_dir)\n",
        "        save_results(self.save_dir, **results)\n",
        "        return results\n",
        "\n",
        "    def _run_single_experiment(self,\n",
        "                            adapter_type: str,\n",
        "                            config: Config = None,\n",
        "                            train_loader: DataLoader = None,\n",
        "                            val_loader: DataLoader = None,\n",
        "                            experiment_name: str = None) -> dict:\n",
        "        \"\"\"Run a single experiment with given configuration\"\"\"\n",
        "        config = config or self.base_config\n",
        "        train_loader = train_loader or self.train_loader\n",
        "        val_loader = val_loader or self.val_loader\n",
        "        experiment_name = experiment_name or f\"{adapter_type}_{self.experiment_id}\"\n",
        "\n",
        "        # Initialize wandb\n",
        "        wandb.init(\n",
        "            project=config.wandb_project,\n",
        "            name=experiment_name,\n",
        "            config=config.to_dict(),\n",
        "            group=adapter_type,\n",
        "            reinit=True\n",
        "        )\n",
        "\n",
        "        try:\n",
        "            results = train_and_evaluate(\n",
        "                config=config,\n",
        "                train_loader=train_loader,\n",
        "                val_loader=val_loader,\n",
        "                adapter_type=adapter_type,\n",
        "                dinov2_model=self.dinov2_model\n",
        "            )\n",
        "\n",
        "            wandb.log({\n",
        "                f\"final/{adapter_type}/best_val_loss\": results['best_val_loss'],\n",
        "                f\"final/{adapter_type}/best_alignment\": results['best_alignment'],\n",
        "                f\"final/{adapter_type}/final_alignment\": results['final_alignment']\n",
        "            })\n",
        "\n",
        "            wandb.finish()\n",
        "            return results\n",
        "\n",
        "        except Exception as e:\n",
        "            logger.error(f\"Error in experiment {experiment_name}: {str(e)}\")\n",
        "            wandb.finish(exit_code=1)\n",
        "            raise\n",
        "\n",
        "    def run_base_experiments(self, adapter_types: List[str]) -> dict:\n",
        "        \"\"\"Run base experiments for specified adapter types\"\"\"\n",
        "        results = {}\n",
        "        for adapter_type in adapter_types:\n",
        "            results[adapter_type] = self._run_single_experiment(adapter_type)\n",
        "        return results\n",
        "\n",
        "    def run_ablation_study(self, adapter_types: List[str]) -> dict:\n",
        "        \"\"\"Run ablation studies for specified adapter types\"\"\"\n",
        "        ablation_results = {}\n",
        "\n",
        "        for adapter_type in adapter_types:\n",
        "            # Temperature ablation\n",
        "            for temp in self.base_config.temperature_range:\n",
        "                config = copy.deepcopy(self.base_config)\n",
        "                config.temperature = temp\n",
        "                ablation_results[f'{adapter_type}_temperature_{temp}'] = self._run_single_experiment(\n",
        "                    adapter_type=adapter_type,\n",
        "                    config=config,\n",
        "                    experiment_name=f'ablation_{adapter_type}_temp_{temp}'\n",
        "                )\n",
        "\n",
        "            # Learning rate ablation\n",
        "            for lr in self.base_config.learning_rates:\n",
        "                config = copy.deepcopy(self.base_config)\n",
        "                config.lr = lr\n",
        "                ablation_results[f'{adapter_type}_lr_{lr}'] = self._run_single_experiment(\n",
        "                    adapter_type=adapter_type,\n",
        "                    config=config,\n",
        "                    experiment_name=f'ablation_{adapter_type}_lr_{lr}'\n",
        "                )\n",
        "\n",
        "        return ablation_results\n",
        "\n",
        "    def run_scaling_study(self, adapter_types: List[str]) -> dict:\n",
        "        \"\"\"Run scaling studies for specified adapter types\"\"\"\n",
        "        scaling_results = {}\n",
        "\n",
        "        for adapter_type in adapter_types:\n",
        "            # Model size scaling\n",
        "            for size in self.base_config.model_sizes:\n",
        "                config = copy.deepcopy(self.base_config)\n",
        "                config.embed_dim = {'small': 256, 'medium': 512, 'large': 1024}[size]\n",
        "                scaling_results[f'{adapter_type}_size_{size}'] = self._run_single_experiment(\n",
        "                    adapter_type=adapter_type,\n",
        "                    config=config,\n",
        "                    experiment_name=f'scaling_{adapter_type}_size_{size}'\n",
        "                )\n",
        "\n",
        "            # Data size scaling\n",
        "            for data_size in self.base_config.data_sizes:\n",
        "                config = copy.deepcopy(self.base_config)\n",
        "                config.num_samples = data_size\n",
        "\n",
        "                train_loader, val_loader = get_dataloaders(self.tokenizer, config)\n",
        "                scaling_results[f'{adapter_type}_data_{data_size}'] = self._run_single_experiment(\n",
        "                    adapter_type=adapter_type,\n",
        "                    config=config,\n",
        "                    train_loader=train_loader,\n",
        "                    val_loader=val_loader,\n",
        "                    experiment_name=f'scaling_{adapter_type}_data_{data_size}'\n",
        "                )\n",
        "\n",
        "        return scaling_results\n",
        "\n",
        "    def _update_config_for_experiment(self, original_config: Config, updates: Dict[str, Any]) -> Config:\n",
        "        \"\"\"Helper method to create new config with updates\"\"\"\n",
        "        config = copy.deepcopy(original_config)\n",
        "        for key, value in updates.items():\n",
        "            setattr(config, key, value)\n",
        "        return config\n",
        "\n",
        "    def _log_experiment_metadata(self, experiment_name: str, config: Config):\n",
        "        \"\"\"Helper method to log experiment metadata\"\"\"\n",
        "        metadata = {\n",
        "            'experiment_name': experiment_name,\n",
        "            'timestamp': time.strftime('%Y-%m-%d %H:%M:%S'),\n",
        "            'config': config.to_dict()\n",
        "        }\n",
        "        wandb.log({'metadata': metadata})\n",
        "\n",
        "    def _handle_experiment_failure(self, experiment_name: str, error: Exception):\n",
        "        \"\"\"Helper method to handle experiment failures\"\"\"\n",
        "        error_info = {\n",
        "            'error_type': type(error).__name__,\n",
        "            'error_message': str(error),\n",
        "            'experiment_name': experiment_name,\n",
        "            'timestamp': time.strftime('%Y-%m-%d %H:%M:%S')\n",
        "        }\n",
        "        wandb.log({'experiment_error': error_info})\n",
        "        logger.error(f\"Experiment {experiment_name} failed: {str(error)}\")\n",
        "\n",
        "    def _save_experiment_artifacts(self, results: dict):\n",
        "        \"\"\"Helper method to save experiment artifacts\"\"\"\n",
        "        with open(self.save_dir / 'results.pkl', 'wb') as f:\n",
        "            pickle.dump(results, f)\n",
        "\n",
        "        json_results = {k: {\n",
        "            'best_val_loss': float(v['best_val_loss']),\n",
        "            'best_alignment': float(v['best_alignment']),\n",
        "            'final_alignment': float(v['final_alignment'])\n",
        "        } for k, v in results.items()}\n",
        "\n",
        "        with open(self.save_dir / 'results.json', 'w') as f:\n",
        "            json.dump(json_results, f, indent=4)"
      ],
      "metadata": {
        "id": "O-BqDHurWDh5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Visualizations"
      ],
      "metadata": {
        "id": "erB6lhlWYKGY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def create_analysis_plots(results: Dict, save_dir: Path):\n",
        "    \"\"\"Create and save analysis plots for all experiment types\"\"\"\n",
        "    # Initialize wandb if not already running\n",
        "    if wandb.run is None:\n",
        "        wandb.init(project=\"adapter-alignment-v2\", name=\"analysis_plots\")\n",
        "\n",
        "    def _plot_experiment_comparison(results: Dict, name: str):\n",
        "        if not results:\n",
        "            return\n",
        "\n",
        "        fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(15, 5))\n",
        "\n",
        "        # Plot alignment scores\n",
        "        scores = {k: v['best_alignment'] for k, v in results.items()}\n",
        "        sns.barplot(x=list(scores.keys()), y=list(scores.values()), ax=ax1)\n",
        "        ax1.set_title(f'Best Alignment Scores - {name}')\n",
        "        ax1.set_ylabel('Alignment Score')\n",
        "        plt.xticks(rotation=45)\n",
        "\n",
        "        # Plot validation losses\n",
        "        losses = {k: v['best_val_loss'] for k, v in results.items()}\n",
        "        sns.barplot(x=list(losses.keys()), y=list(losses.values()), ax=ax2)\n",
        "        ax2.set_title(f'Best Validation Losses - {name}')\n",
        "        ax2.set_ylabel('Loss')\n",
        "        plt.xticks(rotation=45)\n",
        "\n",
        "        plt.tight_layout()\n",
        "        plt.savefig(save_dir / f'{name}_comparison.png')\n",
        "        wandb.log({f\"{name}_comparison\": wandb.Image(fig)})\n",
        "        plt.close()\n",
        "\n",
        "    def _plot_scaling_trends(scaling_results: Dict):\n",
        "        if not scaling_results:\n",
        "            return\n",
        "\n",
        "        sizes = []\n",
        "        alignments = []\n",
        "        losses = []\n",
        "\n",
        "        for size, result in scaling_results.items():\n",
        "            sizes.append(size)\n",
        "            alignments.append(result['best_alignment'])\n",
        "            losses.append(result['best_val_loss'])\n",
        "\n",
        "        fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(15, 5))\n",
        "\n",
        "        sns.lineplot(x=sizes, y=alignments, marker='o', ax=ax1)\n",
        "        ax1.set_title('Alignment Score vs Scale')\n",
        "\n",
        "        sns.lineplot(x=sizes, y=losses, marker='o', ax=ax2)\n",
        "        ax2.set_title('Validation Loss vs Scale')\n",
        "\n",
        "        plt.tight_layout()\n",
        "        plt.savefig(save_dir / 'scaling_trends.png')\n",
        "        wandb.log({\"scaling_trends\": wandb.Image(fig)})\n",
        "        plt.close()\n",
        "\n",
        "    def _plot_ablation_trends(ablation_results: Dict):\n",
        "        if not ablation_results:\n",
        "            return\n",
        "\n",
        "        fig, axes = plt.subplots(2, 1, figsize=(10, 10))\n",
        "\n",
        "        # Temperature analysis\n",
        "        temps = []\n",
        "        temp_alignments = []\n",
        "        for config, result in ablation_results.items():\n",
        "            if 'temperature' in config:\n",
        "                temps.append(float(config.split('_')[1]))\n",
        "                temp_alignments.append(result['best_alignment'])\n",
        "\n",
        "        if temps:\n",
        "            sns.lineplot(x=temps, y=temp_alignments, marker='o', ax=axes[0])\n",
        "            axes[0].set_title('Alignment Score vs Temperature')\n",
        "\n",
        "        # Learning rate analysis\n",
        "        lrs = []\n",
        "        lr_alignments = []\n",
        "        for config, result in ablation_results.items():\n",
        "            if 'lr' in config:\n",
        "                lrs.append(float(config.split('_')[1]))\n",
        "                lr_alignments.append(result['best_alignment'])\n",
        "\n",
        "        if lrs:\n",
        "            sns.lineplot(x=lrs, y=lr_alignments, marker='o', ax=axes[1])\n",
        "            axes[1].set_title('Alignment Score vs Learning Rate')\n",
        "            axes[1].set_xscale('log')\n",
        "\n",
        "        plt.tight_layout()\n",
        "        plt.savefig(save_dir / 'ablation_trends.png')\n",
        "        wandb.log({\"ablation_trends\": wandb.Image(fig)})\n",
        "        plt.close()\n",
        "\n",
        "    # Plot comparisons for each experiment group\n",
        "    for group_name, group_results in results.items():\n",
        "        _plot_experiment_comparison(group_results, group_name)\n",
        "\n",
        "        if group_name == 'scaling':\n",
        "            _plot_scaling_trends(group_results)\n",
        "        elif group_name == 'ablation':\n",
        "            _plot_ablation_trends(group_results)\n",
        "\n",
        "    # Close wandb run if we initialized it\n",
        "    wandb.finish()"
      ],
      "metadata": {
        "id": "6LEZWp06Yi4u"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Main"
      ],
      "metadata": {
        "id": "9sx27v5WZAQS"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Aux function"
      ],
      "metadata": {
        "id": "Tbz04fBSZMoc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def save_results(save_dir: Path, results: Dict):\n",
        "    \"\"\"Save experimental results to disk\"\"\"\n",
        "    # Create a copy without unpickleable objects\n",
        "    pickle_results = {}\n",
        "    for k, v in results.items():\n",
        "        pickle_results[k] = {}\n",
        "        for inner_k, inner_v in v.items():\n",
        "            if isinstance(inner_v, dict):\n",
        "                pickle_results[k][inner_k] = {\n",
        "                    'best_val_loss': inner_v['best_val_loss'],\n",
        "                    'best_alignment': inner_v['best_alignment'],\n",
        "                    'final_alignment': inner_v['final_alignment']\n",
        "                }\n",
        "\n",
        "    # Save the filtered results\n",
        "    with open(save_dir / 'results.pkl', 'wb') as f:\n",
        "        pickle.dump(pickle_results, f)\n",
        "\n",
        "    # Save human-readable results\n",
        "    json_results = {k: {\n",
        "        inner_k: {\n",
        "            'best_val_loss': float(inner_v['best_val_loss']),\n",
        "            'best_alignment': float(inner_v['best_alignment']),\n",
        "            'final_alignment': float(inner_v['final_alignment'])\n",
        "        }\n",
        "        for inner_k, inner_v in v.items()\n",
        "    } for k, v in results.items()}\n",
        "\n",
        "    with open(save_dir / 'results.json', 'w') as f:\n",
        "        json.dump(json_results, f, indent=4)"
      ],
      "metadata": {
        "id": "28Rvre1kY-_d"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Main function"
      ],
      "metadata": {
        "id": "rcMgWZghZPTi"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def main(experiments_config=None):\n",
        "    if experiments_config is None:\n",
        "        experiments_config = {\n",
        "            'base_experiments': ['linear', 'mlp', 'bottleneck'],\n",
        "            'run_ablation': True,\n",
        "            'ablation_adapters': ['linear', 'mlp', 'bottleneck'],\n",
        "            'run_scaling': True,\n",
        "            'scaling_adapters': ['linear', 'mlp', 'bottleneck']\n",
        "        }\n",
        "\n",
        "    try:\n",
        "        wandb.init(project=\"adapter-alignment-v2\", name=\"main_experiment\")\n",
        "\n",
        "        config = Config()\n",
        "        image_encoder, text_encoder, tokenizer = get_pretrained_models()\n",
        "        experiment_runner = ExperimentRunner(config, tokenizer)\n",
        "\n",
        "        results = {}\n",
        "\n",
        "        if experiments_config['base_experiments']:\n",
        "            logger.info(\"Running base experiments...\")\n",
        "            results['base'] = experiment_runner.run_base_experiments(\n",
        "                experiments_config['base_experiments']\n",
        "            )\n",
        "\n",
        "        if experiments_config['run_ablation']:\n",
        "            logger.info(\"Running ablation studies...\")\n",
        "            results['ablation'] = experiment_runner.run_ablation_study(\n",
        "                experiments_config['ablation_adapters']\n",
        "            )\n",
        "\n",
        "        if experiments_config['run_scaling']:\n",
        "            logger.info(\"Running scaling experiments...\")\n",
        "            results['scaling'] = experiment_runner.run_scaling_study(\n",
        "                experiments_config['scaling_adapters']\n",
        "            )\n",
        "\n",
        "        wandb.finish()\n",
        "        return results, experiment_runner.save_dir\n",
        "\n",
        "    except Exception as e:\n",
        "        if wandb.run is not None:\n",
        "            wandb.finish(exit_code=1)\n",
        "        logger.error(f\"Error: {str(e)}\")\n",
        "        raise"
      ],
      "metadata": {
        "id": "GXQu_mMarqHx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Run"
      ],
      "metadata": {
        "id": "MdmMyCAKaY0L"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "logging.basicConfig(level=logging.INFO)\n",
        "\n",
        "results, save_dir = main({\n",
        "    'base_experiments': ['linear', 'mlp', 'bottleneck'],\n",
        "    'run_ablation': True,\n",
        "    'ablation_adapters': ['linear', 'mlp', 'bottleneck'],\n",
        "    'run_scaling': True,\n",
        "    'scaling_adapters': ['linear', 'mlp', 'bottleneck']\n",
        "})\n",
        "\n",
        "# bottleneck -> large -> 19 GB System Ram"
      ],
      "metadata": {
        "id": "Be-YnLcuYnZ8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "create_analysis_plots(results, save_dir)"
      ],
      "metadata": {
        "id": "qjL_30Spb4NC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "save_results(save_dir, results)"
      ],
      "metadata": {
        "id": "SP_fjQiF4C-I"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}