<!DOCTYPE html>
<html lang="en">

<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <script type="text/javascript" async
        src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/3.2.2/es5/tex-mml-chtml.js"></script>
    <title>Aligning Modalities: Efficient Multimodal Representation Learning</title>
    <link rel="stylesheet" href="styles/style.css">
</head>

<body>
    <header>
        <h1>Towards the Platonic Representation via Multimodal Contrastive Alignment</h1>
    </header>

    <main>
        <section id="introduction">
            <h2>Introduction</h2>
            <p>
                Deep learning models are typically trained to transform raw data into representations optimized for
                specific tasks. Recently, two lines of research have inspired a deeper inquiry into the nature of these
                representations. The CLIP framework demonstrated the utility of aligning representations across
                modalities, using paired image-text data to train joint embeddings for cross-modal retrieval. Meanwhile,
                the Platonic Representation Hypothesis posits that performant models converge toward a shared
                statistical model of reality in their representation spaces, suggesting a potential universality
                underlying learned representations.
            </p>
            <p>
                This project bridges these ideas by exploring whether representations from disparate pre-trained
                unimodal neural networks can be aligned into a shared multimodal latent space, inspired by the joint
                embedding approach of CLIP and motivated by the convergence hypothesis of Platonic Representations. The
                proposed framework uses frozen unimodal encoders (e.g., ResNet-18 for images, GPT-2 for text) with
                learned linear adapters to align representations across modalities. Our aim is to determine if such
                aligned representations better approximate those of larger, more performant models (e.g., DINOv2-small).
            </p>
            <p>
                Inspired by the success of CLIP in aligning representations across modalities and the theoretical
                insights of the Platonic Representation Hypothesis, we propose a framework that aligns pre-trained
                unimodal encoders into a shared multimodal latent space using simple linear adapters. By leveraging
                frozen encoders such as ResNet-18 for images and GPT-2 for text, we aim to achieve alignment without
                retraining these large models, enabling scalability to new modalities with minimal overhead.
            </p>
            <figure>
                <img src="figures/conceptual_contrastive.svg" alt="Conceptual Multimodal Alignment"
                    style="width: 200%; height: auto;">
                <figcaption>
                    <strong>Figure 1.</strong> Conceptual illustration of our multimodal alignment framework. Image
                    representations (from ResNet-18) and text representations (from GPT-2) are aligned into a shared
                    latent space using adapters. Matching pairs are pulled together, while non-matching pairs are pushed
                    apart, approximating a unified embedding inspired by the Platonic Representation Hypothesis.
                </figcaption>
            </figure>


            <h3>Motivation</h3>
            <p>
                Our approach is motivated by three key insights:
            </p>
            <ol>
                <li><strong>Inspiration from CLIP:</strong> The CLIP framework demonstrated that cross-modal
                    representations could be aligned through paired data and contrastive learning. However, it requires
                    joint training of encoders, limiting extensibility to additional modalities. Our method decouples
                    the encoders, focusing instead on aligning their outputs via lightweight adapters.</li>
                <li><strong>Testing the Platonic Representation Hypothesis:</strong> This hypothesis posits that
                    performant models converge toward a shared statistical model of reality in their representation
                    spaces. By aligning diverse unimodal encoders, we provide a testbed for exploring whether this
                    convergence can be achieved explicitly.</li>
                <li><strong>Scalability and Modularity:</strong> Traditional multimodal models often require joint
                    training on extensive datasets. By aligning frozen encoders post hoc, our framework supports modular
                    integration of new modalities, enabling efficient experimentation.</li>
            </ol>


        </section>

        <section id="related-work">
            <h2>Related Work</h2>
            <p>
                Our project builds on several important advances in multimodal representation learning and theoretical
                insights into model convergence:
            </p>
            <ol>
                <li>
                    <strong><a href="https://phillipi.github.io/prh/">Platonic Representation Hypothesis
                            (PRH)</a>:</strong> <a href="https://arxiv.org/abs/2405.07987">Huh et al. (2024)</a>
                    proposed that
                    performant models converge toward a shared statistical model of reality in their representation
                    spaces,
                    regardless of their training modality. This hypothesis underpins our framework, which aims to
                    explicitly align unimodal encoders to test this convergence hypothesis.
                </li>
                <li>
                    <strong><a href="https://openai.com/index/clip/">CLIP</a>:</strong> <a
                        href="https://arxiv.org/abs/2103.00020">Radford et al. (2021) </a>introduced CLIP, a model that
                    learns joint multimodal
                    representations using contrastive learning on paired image-text datasets. CLIP's success
                    demonstrates the power of cross-modal embeddings but requires joint training of encoders, which our
                    framework aims to circumvent.
                </li>
                <li>
                    <strong><a href="https://arxiv.org/abs/2209.15162">Linear Mapping from Image to Text
                            Space</a>:</strong> Merullo et al. (2022) showed that simple
                    linear transformations can align visual representations to text spaces, enabling cross-modal tasks
                    like visual question answering. Their findings inspired our use of linear and lightweight MLP
                    adapters for multimodal alignment.
                </li>
                <li>
                    <strong><a href="https://arxiv.org/abs/2304.07193">DINOv2</a>:</strong> Oquab et al. (2023)
                    introduced DINOv2, a self-supervised vision transformer
                    model that generates robust embeddings. As our performant reference model, DINOv2-small provides a
                    benchmark
                    for evaluating the quality of our aligned multimodal representations.
                </li>
                <li>
                    <strong>Contrastive Representation Learning:</strong> Wang and Isola (2020) provided theoretical
                    insights into
                    contrastive learning by analyzing alignment and uniformity on the hypersphere. Their work shows that
                    contrastive
                    objectives promote both alignment of positive pairs and uniformity of representations across the
                    hypersphere.
                    This analysis informs our use of the <strong>dual-encoder contrastive loss</strong> to drive
                    convergence in the shared latent space.
                </li>
            </ol>
        </section>

        <section id="hypothesis">
            <h2>Hypothesis</h2>
            <p>
                Our work is grounded in the following assumptions and/or hypotheses:
            </p>
            <ul>
                <li>A shared latent space exists where unimodal representations from different encoders can be aligned
                    through linear transformations.</li>
                <li>Aligning these representations produces embeddings that closely approximate those of performant
                    models, such as DINOv2, as measured by kernel alignment metrics.</li>
                <li>Multimodal alignment captures mechanisms of representation convergence, offering empirical evidence
                    for the Platonic Representation Hypothesis.</li>
            </ul>
        </section>


        <section id="mathematical-framework">
            <h2>Mathematical Framework</h2>
            <p>
                At the core of our project is the hypothesis that pre-trained unimodal representations can be aligned
                into a shared multimodal latent space. This section formalizes the mathematical structure underlying our
                framework, detailing how representations are extracted, aligned, and evaluated.
            </p>

            <h3>Multimodal Data Representation</h3>
            <p>
                Let the world generate raw multimodal data:
            </p>
            <p>
                \[
                \mathcal{D}_\text{world} = \left\{\left(\theta^{(i)}, \psi^{(i)}, \phi^{(i)}, \dots \right)
                \right\}_{i=1}^N, \quad \theta \in \mathcal{\Theta}, \psi \in \mathcal{\Psi}, \phi \in \mathcal{\Phi},
                \dots
                \]
            </p>
            <p>
                where \( \mathcal{\Theta}, \mathcal{\Psi}, \mathcal{\Phi} \) represent different modalities (e.g.,
                image, text, audio), and \( \theta, \psi, \phi \) are specific instances of these modalities.
            </p>
            <p>
                For simplicity, we focus on a two-modality setting with images (\( \mathcal{X} \)) and text (\(
                \mathcal{Y} \)):
            </p>
            <p>
                \[
                \mathcal{D}_\text{world} = \left\{\left(x^{(i)}, y^{(i)}\right)\right\}_{i=1}^N, \quad x \in
                \mathcal{X}, y \in \mathcal{Y}.
                \]
            </p>
            <figure>
                <img src="figures/platonic_representation.svg" alt="Platonic Representation Hypothesis"
                    style="width: 150%; height: auto;">
                <figcaption>
                    <strong>Figure 2.</strong> Illustration of the Platonic Representation Hypothesis (PRH).
                    Representations from different modalities (e.g., images, text) are hypothesized to converge toward a
                    shared statistical model — the Platonic representation. Our framework uses contrastive learning to
                    align these representations into a shared latent space, approximating this convergence.
                </figcaption>
            </figure>


            <h3>Learned Adapters</h3>
            <p>
                We use frozen pre-trained encoders to extract representations from each modality:
            </p>
            <p>
                \[
                X_\text{enc}: \mathcal{X} \rightarrow \mathbb{R}^{d_x}, \quad Y_\text{enc}: \mathcal{Y} \rightarrow
                \mathbb{R}^{d_y},
                \]
            </p>
            <p>
                where \( d_x \) and \( d_y \) are the embedding dimensions for images and text, respectively.
            </p>
            <p>
                To project these representations into a shared latent space \( \mathbb{R}^{d_e} \), we introduce learned
                adapters:
            </p>
            <p>
                \[
                W_x : \mathbb{R}^{d_x} \rightarrow \mathbb{R}^{d_e}, \quad W_y : \mathbb{R}^{d_y} \rightarrow
                \mathbb{R}^{d_e}
                \]
            </p>
            <p>
                We explore both <strong>linear adapters</strong> and <strong>2-layer MLP adapters</strong>. The adapted
                encoders:
            </p>
            <p>
                \[
                \begin{aligned}
                f_\text{image}: \mathcal{X} \rightarrow \mathbb{R}^{d_e}, \quad f_\text{image} &= W_x \circ X_\text{enc}
                \\
                g_\text{text}: \mathcal{Y} \rightarrow \mathbb{R}^{d_e}, \quad g_\text{text} &= W_y \circ Y_\text{enc}
                \end{aligned}
                \]
            </p>
            <p>
                \( f_\text{image} \) and \( g_\text{text} \) map images and text, respectively, to the shared latent
                space \( \mathbb{R}^{d_e} \).
            </p>

            <h3>Dual-Encoder Contrastive Objective</h3>
            <p>
                To align the representations, we use a <strong>dual-encoder contrastive loss</strong>, inspired by the
                formulation in the <a href="https://phillipi.github.io/prh/">PRH</a> and Homework 4:
            </p>
            <figure>
                <img src="figures/homework4_contrastive.jpg" alt="Dual-Encoder Contrastive Loss"
                    style="width: 80%; height: auto;">
            </figure>

            <p>
                The loss encourages
                representations from matching pairs to be similar while pushing apart representations from non-matching
                pairs:
            </p>
            <p>
                \[
                \mathcal{L}_{\text{contrastive}} = -\frac{1}{N} \sum_{i=1}^N \log
                \frac{\exp\left(f_\text{image}(x^{(i)})
                \cdot g_\text{text}(y^{(i)}) / \tau\right)}{\sum_{j=1}^N \exp\left(f_\text{image}(x^{(i)}) \cdot
                g_\text{text}(y^{(j)}) / \tau\right)},
                \]
            </p>
            <p>
                where \( \tau \) is the temperature parameter that scales the similarity scores.
            </p>


            <h3>Interpretation and Connection to PRH</h3>
            <p>
                The <strong>Platonic Representation Hypothesis (PRH)</strong> posits that representations from different
                modalities converge toward a shared Platonic representation. In our framework, we hypothesize that
                <strong>contrastive alignment</strong> between the unimodal image \( f_{\text{image}} \) and text \(
                g_{\text{text}} \) encoders can drive this convergence toward the Platonic representation.
            </p>

            <p>
                In the ideal case of perfect contrastive alignment (achieved by minimizing the diameter and maximizing
                the margin), the outputs of \( f_{\text{image}}(x) \) and \( g_{\text{text}}(y) \) for positive
                pairs would map to the <strong>same embedding vector</strong> in the shared latent space i.e. \(
                f_{\text{image}}(x^{(i)})
                = g_{\text{text}}(y^{(i)}) \)
            </p>

            <p>
                Our thesis is that this limiting vector represents the Platonic representation. However, in practice,
                due to finite data, model capacity, and noise, the outputs of \( f_{\text{image}} \) and \(
                g_{\text{text}} \) are typically <strong>close</strong> (as measured by the \( L_2
                \)-distance on the unit hypersphere \( \mathbb{S}^{d_e-1} \)) but not identical i.e. \(
                f_{\text{image}}(x^{(i)})
                \approx g_{\text{text}}(y^{(i)}) \).
            </p>

            <p>
                Given this, we define our <strong>aligned multimodal encoder</strong> \( h_\text{multi} \) as a function
                that averages the embeddings ooutput by the adapted image and text encoders, i.e.:
            </p>
            \[
            h_\text{multi}: \mathcal{X} \times \mathcal{Y} \rightarrow \mathbb{R}^{d_e}, \quad h_\text{multi}(x, y) =
            \frac{1}{2}
            \left(f_{\text{image}}(x) + g_{\text{text}}(y)\right).
            \]
            </p>

            <p>
                We call the output \( h_\text{multi}(x^{(i)}, y^{(i)}) \) the <strong>aligned multimodal
                    representation</strong>
                and claim that this is an <strong>approximation</strong> of the Platonic representation of the \(i\)-th
                data point. While the true
                Platonic representation is unknown, we follow the PRH
                conjecture that performant models, such as the DINOv2 models by <a
                    href="https://arxiv.org/abs/2304.07193">Oquab et al. (2023)</a>, produce representations that are
                converging toward
                it. We will use the features extracted by a pretrained <a
                    href="https://huggingface.co/facebook/dinov2-small">DINOv2-small</a> backbone as a
                <strong>proxy</strong> for the Platonic representation.
            </p>

            <p>
                We evaluate the quality of our aligned multimodal encoder \( h_\text{multi} \) its representations to
                those of the DINOv2-small feature extractor using a <strong>kernel alignment metric</strong>.
            </p>

            <h3>Kernel Alignment Metric</h3>
            <p>
                We characterize representations in terms of their <strong>kernels</strong>, which capture the similarity
                between data points. Two representations are considered <strong>aligned</strong> if their kernels are
                similar for corresponding inputs. For example, if the text encoder \( g_{\text{text}} \) is aligned with
                the image encoder \( f_{\text{image}} \), the similarity between text representations of "apple" and
                "orange" should correspond closely to the similarity between their image representations:
            </p>

            <figure>
                <img src="figures/similarity_metric.jpg" alt="Similarity Metric" style="width: 90%; height: auto;">
            </figure>

            <p>
                To quantify this alignment, we use the <strong>mutual \(k\)-nearest neighbor (mutual-KNN)</strong>
                kernel alignment metric \( m \), as introduced by <a href="https://arxiv.org/abs/2405.07987">Huh et al.
                    (2024)</a>. We define the kernels as:
            </p>

            <ul>
                <li>
                    <strong>Unimodal Kernels:</strong>
                    <p>
                        \[
                        K_X(i, j) = \langle f_{\text{image}}(x^{(i)}), f_{\text{image}}(x^{(j)}) \rangle, \quad K_Y(i,
                        j) =
                        \langle g_{\text{text}}(y^{(i)}), g_{\text{text}}(y^{(j)}) \rangle
                        \]
                    </p>
                </li>
                <li>
                    <strong>Aligned Multimodal Kernel:</strong>
                    <p>
                        \[
                        K_{\text{align}}(i, j) = \langle h_\text{multi}(x^{(i)}, y^{(i)}) , h_\text{multi}(x^{(j)},
                        y^{(j)}) \rangle,
                        \]
                    </p>

                </li>
                <li>
                    <strong>Performant Model Kernel:</strong>
                    <p>
                        \[
                        K_\text{DINOv2}(i, j) = \langle \text{DINOv2}(x^{(i)}), \text{DINOv2}(x^{(j)}) \rangle
                        \]
                    </p>
                    <p>
                        We emphasize that DINOv2-small is an <strong>image-only</strong> model so its kernel is computed
                        using samples from the image modality as input.
                    </p>
                </li>
            </ul>

            <h4>Before Training</h4>
            <p>
                We compute the alignment metrics for the unimodal kernels relative to the performant model kernel:
            </p>
            <p>
                \[
                m(K_X, K_{\text{DINOv2}}), \quad m(K_Y, K_{\text{DINOv2}}).
                \]
            </p>

            <h4>After Training</h4>
            <p>
                We evaluate the mutual-KNN alignment for the aligned multimodal kernel against the performant model
                kernel:
            </p>
            <p>
                \[
                m(K_{\text{align}}, K_{\text{DINOv2}}).
                \]
            </p>

            <h4>Key Hypothesis</h4>
            <p>
                We hypothesize that the aligned multimodal kernel achieves higher similarity with \( K_{\text{DINOv2}}
                \) than the average of the unimodal kernels:
            </p>
            <p>
                \[
                m(K_{\text{align}}, K_{\text{DINOv2}}) > \text{avg}\left(m(K_X, K_{\text{DINOv2}}), m(K_Y,
                K_{\text{DINOv2}})\right).
                \]
            </p>

            <p>
                A stronger version of this hypothesis posits that the aligned multimodal kernel surpasses even the best
                unimodal kernel:
            </p>
            <p>
                \[
                m(K_{\text{align}}, K_{\text{DINOv2}}) > \max\left(m(K_X, K_{\text{DINOv2}}), m(K_Y,
                K_{\text{DINOv2}})\right).
                \]
            </p>

            <p>
                This would suggest an interesting emergent-like property where the whole (multimodal representation) is
                greater than either its parts (unimodal representations) alone,
            </p>

        </section>

        <section id="methodology">
            <h2>Methodology</h2>

            <p>
                All code we used for this project is implemented in this <a
                    href="https://colab.research.google.com/drive/1BkyPko0x-8CL41Z-VSmyxI41B90hB2Fc?usp=sharing">Google
                    Colab notebook</a>.
            </p>

            <h3>Data Pipeline</h3>
            <p>
                We construct our world dataset \( \mathcal{D}_\text{world} \) from first 12,800 samples from
                <strong>Flickr30k</strong>
                dataset, which contains paired image-caption samples. Each image \( x^{(i)} \) in Flickr30k dataset is
                associated with
                5 captions, but for our dataset we used only the first caption \( y^{(i)} \) as a single postive
                anchor The data
                pipeline processes these pairs to generate tensors compatible with our models. Key stages of the
                pipeline include:
            </p>
            <ul>
                <li><strong>Image Processing:</strong>
                    <ul>
                        <li>Resize images to \(224 \times 224\) pixels.</li>
                        <li>Normalize using ImageNet statistics.</li>
                        <li>Convert to tensors suitable for ResNet-18.</li>
                    </ul>
                </li>
                <li><strong>Text Processing:</strong>
                    <ul>

                        <li>Tokenize captions using the DistilBERT tokenizer.</li>
                        <li>Pad or truncate tokens to a maximum length of 64.</li>
                        <li>Convert tokens to tensors suitable for DistilBERT.</li>
                    </ul>
                </li>
                <li><strong>Data Loading:</strong>
                    <ul>
                        <li>Batch size: 256</li>
                        <li>75%-25% train-test split.</li>
                        <li>Optimized for GPU utilization with shuffling and memory pinning.</li>
                    </ul>
                </li>
            </ul>
            <figure>
                <img src="figures/data_pipeline.jpg" alt="Data Pipeline">
                <figcaption><strong>Figure.</strong> Schematic/flow chart diagram illustraing our data pipeline which
                    preprocesses images and text for input into the
                    model.</figcaption>
            </figure>

            <h3>Model Architecture</h3>
            <p>
                The architecture consists of frozen pre-trained encoders and trainable adapters that align
                representations in a shared multimodal space.
            </p>
            <ul>
                <li><strong>Frozen Image Encoder:</strong> ResNet-18 (input type: RGB image; output dimension: 512).
                </li>
                <li><strong>Frozen Text Encoder:</strong> DistilBERT (input type: text string; output dimension: 768).
                </li>
                <li><strong>Performant Model Backbone:</strong> DINOv2-small (input type: RGB image; output
                    dimension: 768).</li>
                <li><strong>Adapters:</strong>
                    <ul>
                        <li><strong>Linear Adapter:</strong> Projects frozen encoder outputs to a shared dimension of
                            768,
                            which is embedding dimension of DINOv2-small's features:
                            \[
                            W_x : \mathbb{R}^{512} \to \mathbb{R}^{768}, \quad W_y : \mathbb{R}^{768} \to
                            \mathbb{R}^{768}
                            \]
                        </li>
                        <li><strong>MLP Adapter:</strong> A two-layer MLP with GELU activation and LayerNorm:
                            \[
                            \operatorname{MLP}(z) = \operatorname{LayerNorm}(\operatorname{GELU}(W_2
                            \operatorname{GELU}(W_1 z))), \quad z \in \mathcal{X} \text{ or } z \in \mathcal{Y}
                            \]
                        </li>
                    </ul>
                </li>
            </ul>
            <p>
                After training, the aligned multimodal encoder may be used to extract features for downstream tasks,
                such as image classification or visual question answering.
            </p>

            <figure>
                <img src="figures/img_txt_concept.svg" alt="Multimodal Alignment Architecture">
                <figcaption><strong>Figure.</strong> Conceptual illustration of our approach to using frozen
                    pretrained encoders to map single modalities (image and text) into shared latent space.
                    We encode image and text pairs using frozen pretrained ResNet-18 and DistilBERT encoders. We use
                    learnable adapters to map from the dimensions of the embeddings of these frozen encoders to a the
                    dimension of the shared multimodal latent space which is set to be equal to the emdedding dimension
                    of a performant model.
                </figcaption>
            </figure>

            <h3>Training Procedure</h3>
            <p>
                We train the adapters for <strong>15 epochs</strong> using a dual-encoder contrastive loss with
                temperature
                scaling (\(\tau = 0.07\)).
            </p>

            <p>
                \[
                \mathcal{L} = -\frac{1}{N} \sum_{i=1}^N \log \frac{\exp\left(f_\text{image}(x^{(i)}) \cdot
                g_\text{text}(y^{(i)}) / \tau\right)}{\sum_{j=1}^N \exp\left(f_\text{image}(x^{(i)}) \cdot
                g_\text{text}(y^{(j)}) / \tau\right)}.
                \]
            </p>
            <p>The training loop consists of:</p>
            <ol>
                <li>Forward pass through frozen encoders and adapters.</li>
                <li>Compute contrastive loss for matching and non-matching pairs.</li>
                <li>Update adapter weights using the AdamW optimizer with:
                    <ul>
                        <li>Learning rate: \(5 \times 10^{-5}\) (cosine decay with warmup).</li>
                        <li>Gradient clipping (norm: 1.0).</li>
                        <li>Weight decay: 0.01.</li>
                    </ul>
                </li>
            </ol>


            <figure>
                <img src="figures/training_pipeline.jpg" alt="Training Procedure Diagram">
                <figcaption><strong>Figure.</strong> Schematic/flow chart diagram of our training piepline starting from
                    the processing of the inputs (images and text captions) to the computation of the contrastive loss.
                </figcaption>
            </figure>


            <h3>Evaluation Metrics</h3>
            <p>
                We evaluate alignment using the <strong>mutual-KNN kernel alignment metric</strong> \(m\):
            </p>
            <ul>
                <ul>
                    <li><strong>Unimodal Kernel Alignment:</strong> \( m(K_X, K_{\text{DINOv2}}) \) and \( m(K_Y,
                        K_{\text{DINOv2}}) \).</li>
                    <li><strong>Multimodal Kernel Alignment:</strong> \( m(K_{\text{align}}, K_{\text{DINOv2}}) \).</li>
                    <li><strong>Test Key Hypothesis:</strong> We test our "Key Hypotheses" state in the mathemtical
                        framework (see figure "Hypothesis Test Diagram").</li>
                    </li>

                </ul>
            </ul>

            <figure>
                <img src="figures/hypothesis_test.jpg" alt="Hypothesis Test Diagram">
                <figcaption><strong>Figure.</strong> We test our key hypothesis by comparing the kernel alignment
                    metric of the aligned multimodal kernel to the unimodal kernels and the DinoV2 kernel.
                </figcaption>
            </figure>
        </section>

        <section id="results">
            <h2>Results</h2>

            <h3>Experiment: Contrastive Alignment Training</h3>
            <p>
                In this experiment, we train our adapters to align image and text representations using the dual-encoder
                contrastive loss outlined in the <a href="#mathematical-framework">Mathematical Framework</a>.
                We evaluate the alignment performance for two types of adapters:
                <strong>linear adapters</strong> and <strong>2-layer MLP adapters</strong>.
            </p>

            <p>
                We train each type of adapter for <strong>15 epochs</strong> on the <strong>Flickr30k</strong> dataset.
                The alignment performance is measured using the <strong>mutual-KNN kernel alignment metric</strong>.
                Specifically, we compute the following three metrics:
            </p>
            <ul>
                <li>\( m(K_X, K_{\text{DINOv2}}) \): Alignment of the image encoder \( f_\text{image} \)kernel to the
                    DINOv2 kernel.</li>
                <li>\( m(K_Y, K_{\text{DINOv2}}) \): Alignment of the text encoder \( g_\text{text} \) kernel to the
                    DINOv2 kernel.</li>
                <li>\( m(K_{\text{align}}, K_{\text{DINOv2}}) \): Alignment of the multimodal encoder \( h_\text{mult}
                    \) kernel to the DINOv2
                    kernel.</li>
            </ul>

            <h4>Result 1: Kernel Alignment Improves with Training</h4>
            <p>
                The first key finding of this experiment is that the mutual-KNN kernel alignment metric improves
                significantly after training for both the linear and MLP adapters. This demonstrates that our
                contrastive alignment objective effectively aligns the image and text representations.
            </p>

            <figure>
                <img src="figures/kernel_metric_barplot.svg" alt="Kernel Metric Bar Plot">
                <figcaption>
                    <strong>Figure.</strong> Bar plot showing the mutual-KNN kernel alignment metric before and after
                    training for the linear (left) and MLP (right) adapters. For both adapter types and
                    all encoders, kernel alignment to the DINOv2-small model improves after training with the
                    dual encoder contrastive objective.
                </figcaption>
            </figure>

            <p>
                We use PCA to visualize the alignment process between the DINOv2-small embeddings and the aligned
                multimodal
                embeddings. First, we apply the DINOv2-small feature extractor to our test set and compute the principal
                components (PCs) of the resulting \(3200 \times 784\) matrix of embeddings. During training, at each
                epoch,
                we project the aligned multimodal embeddings onto the same PCs and plot the first two principal
                component
                scores for both sets of embeddings. This 2D visualization shows how the multimodal representations
                evolve
                to become more aligned with the DINOv2 embeddings.
            </p>

            <figure>
                <img src="figures/pca_alignment.svg" alt="PCA Alignment Visualization">
                <figcaption>
                    <strong>Figure.</strong> PCA plots of the DINOv2 embeddings (blue) and aligned multimodal
                    representations (yellow)
                    at the first epoch (left) and last epoch (right) of training. The multimodal representations become
                    progressively more aligned with the DINOv2 embeddings during training.
                </figcaption>
            </figure>

            <h4>Result 2: Key Hypotheses Supported</h4>
            <p>
                We also test the key hypotheses outlined in the <a href="#mathematical-framework">Mathematical
                    Framework</a>:
            </p>
            <ul>
                <li>
                    <strong>Weak Hypothesis:</strong> The aligned multimodal kernel achieves higher similarity with
                    the DINOv2 kernel than the average of the unimodal kernels:
                    \[
                    m(K_{\text{align}}, K_{\text{DINOv2}}) > \text{avg}(m(K_X, K_{\text{DINOv2}}), m(K_Y,
                    K_{\text{DINOv2}})).
                    \]
                </li>
                <li>
                    <strong>Strong Hypothesis:</strong> The aligned multimodal kernel surpasses even the best unimodal
                    kernel:
                    \[
                    m(K_{\text{align}}, K_{\text{DINOv2}}) > \max(m(K_X, K_{\text{DINOv2}}), m(K_Y, K_{\text{DINOv2}})).
                    \]
                </li>
            </ul>

            <p>
                The results show that both the weak and strong hypotheses are supported. This indicates that the
                multimodal representations, which incorporate both image and text information, are more closely aligned
                with the DINOv2 embeddings than either the image or text representations alone.
            </p>

            <figure>
                <img src="figures/key_hypothesis_results.svg" alt="Key Hypothesis Results">
                <figcaption>
                    <strong>Figure.</strong> The mutual-KNN kernel alignment metric for the aligned multimodal kernel
                    surpasses both the average and the best of the unimodal kernels, supporting both the weak and
                    strong hypotheses for both adapter types.
                </figcaption>
            </figure>

            <p>
                This result is particularly compelling given that DINOv2 is a self-supervised vision model trained
                <em>without any text supervision</em>. The fact that multimodal alignment with text leads to
                representations
                that are more similar to DINOv2, we believe, supports the <strong>Platonic Representation
                    Hypothesis</strong> —
                suggesting a shared statistical model of reality across modalities.
            </p>

        </section>


        <section id="discussion">
            <h2>Discussion</h2>

            <p>
                In this work, we investigated whether representations from pre-trained unimodal encoders can be aligned
                into a shared multimodal latent space using lightweight adapters. Our experiments provide evidence
                supporting the <strong>Platonic Representation Hypothesis (PRH)</strong> proposed by Huh et al. (2024).
                Specifically, we observed that aligning image and text embeddings via contrastive learning leads to
                multimodal representations that are more closely aligned with the DINOv2-small features compared to
                unimodal representations alone.
            </p>

            <h3>Key Insights</h3>

            <p>
                Our approach was inspired by the <strong>CLIP framework</strong> (Radford et al., 2021), which
                demonstrated the power of aligning cross-modal representations using paired image-text data. Unlike
                CLIP, which requires joint training of encoders, our method achieves alignment post hoc with frozen
                encoders and lightweight adapters, providing a scalable alternative.
            </p>

            <p>
                Our experiments revealed three key insights:
            </p>

            <ul>
                <li>
                    <strong>Surprising Cross-Modal Alignment:</strong>
                    Despite <strong>DINOv2-small</strong> being a self-supervised vision model with no text supervision,
                    aligning image and text representations improved their similarity to DINOv2 embeddings. This
                    suggests an underlying universality in representation spaces, supporting the PRH's notion of
                    convergence toward a shared statistical model of reality. The result is notable as it implies that
                    contrastive alignment between modalities — even when one modality was not part of the performant
                    model’s training — can enhance representational alignment.
                </li>

                <li>
                    <strong>Empirical Support for Key Hypotheses:</strong>
                    Our results validated both the <strong>weak</strong> and <strong>strong hypotheses</strong>
                    formulated in the mathematical framework. The aligned multimodal kernel \(K_{\text{align}}\)
                    consistently surpassed the average and the best of the unimodal kernels (\(K_X\) and \(K_Y\)),
                    highlighting that the combined information from multiple modalities can lead to representations that
                    better approximate those of performant models.
                </li>

                <li>
                    <strong>Efficiency and Modularity:</strong>
                    By using <strong>frozen pre-trained encoders</strong> and lightweight adapters, our method achieves
                    efficient multimodal alignment without the need for joint end-to-end training. This modular approach
                    allows for straightforward integration of additional modalities, such as audio, making it a scalable
                    solution for future multimodal systems.
                </li>
            </ul>

            <h3>Potential Limitations</h3>

            <ul>
                <li>
                    <strong>Dataset Bias:</strong>
                    The <strong>Flickr30k dataset</strong> primarily contains images and captions depicting people in
                    everyday scenarios. This limited diversity may constrain the generalizability of our findings.
                    Exploring alignment on more varied datasets could yield further insights.
                </li>

                <li>
                    <strong>Restricted Modalities:</strong>
                    Our study focused on aligning image and text modalities. Extending this framework to incorporate
                    audio or video data would provide a more comprehensive test of the alignment mechanism and PRH.
                </li>
            </ul>

            <h3>Conclusion</h3>

            <p>
                Our work demonstrates that multimodal alignment via contrastive learning is a promising
                approach for achieving efficient, scalable representation learning. The observed convergence toward
                DINOv2 embeddings supports the notion of a <strong>universal Platonic representation</strong>, where
                aligning multiple modalities reveals shared underlying structures in representation spaces. This result
                builds upon insights from both the <strong>CLIP framework</strong> and the <strong>Platonic
                    Representation Hypothesis</strong>, underscoring the potential of multimodal alignment in advancing
                our understanding of representation learning.
            </p>
        </section>



        <!-- <section id="conclusion">
            <h2>Conclusion</h2>
            <p>
                This work demonstrates that aligning pre-trained unimodal encoders using simple linear adapters into a
                shared multimodal latent space effectively supports the Platonic Representation Hypothesis. By applying
                a contrastive alignment objective, we show that the resulting multimodal representations achieve
                improved kernel alignment with a performant model's embeddings, specifically DINOv2-small, surpassing
                both image-only and text-only baselines. This suggests that such alignment can approximate a shared
                statistical model of reality across modalities. Our approach, which decouples encoders and leverages
                lightweight alignment, offers a scalable and modular foundation for efficient multimodal representation
                learning and sets the stage for future research into richer multimodal datasets and additional
                modalities.
            </p>
        </section> -->

        <!-- <section id="discussion">
            <h2>Discussion</h2>
            <p>
                In this project, we tested the Platonic Representation Hypothesis (PRH) by aligning pre-trained unimodal
                representations from images and text into a shared multimodal latent space. Our framework leveraged
                frozen encoders (ResNet-18 and DistilBERT) with lightweight linear and MLP adapters, trained using
                contrastive learning to align the representations.
            </p>

            <h3>Multimodal Alignment and the Platonic Representation Hypothesis</h3>
            <p>
                Our findings demonstrate that contrastive alignment between image and text representations improves
                their similarity to the DINOv2-small model, a high-performing self-supervised image encoder. This result
                is particularly surprising given that DINOv2 was trained without any text data. The fact that aligning
                image and text representations results in closer convergence to DINOv2 supports the PRH, suggesting a
                shared statistical model of reality across modalities.
            </p>

            <h3>Empirical Validation of Key Hypotheses</h3>
            <p>
                We evaluated two key hypotheses: the weak hypothesis (aligned multimodal representations surpass the
                average of unimodal alignments) and the strong hypothesis (aligned multimodal representations surpass
                the best unimodal alignment). Both hypotheses were supported, indicating that the multimodal alignment
                benefits from combining image and text modalities. This emergent behavior highlights that the whole
                (multimodal representation) can be greater than its parts (unimodal representations).
            </p>

            <h3>Cross-Modality Benefits</h3>
            <p>
                The alignment process enables cross-modality benefits, particularly for text representations. Despite
                DINOv2 being a vision-only model, the aligned multimodal representations incorporating text information
                became more similar to DINOv2 embeddings. This suggests that multimodal alignment can unlock richer
                representations, even for modalities not explicitly present during the performant model's training.
            </p>

            <h3>Limitations</h3>
            <p>
                One limitation of our study is the choice of dataset. The Flickr30k dataset primarily features images of
                people performing everyday activities, which may constrain the diversity of the learned representations.
                Expanding to datasets with broader content, such as COCO or OpenImages, could enhance the robustness of
                the alignment.
            </p>

            <h3>Future Directions</h3>
            <p>
                Future work could explore integrating additional modalities, such as audio, to further test the
                scalability of the alignment framework. Additionally, analyzing the internal mechanisms of convergence
                and the role of adapter architecture may provide deeper insights into alignment-driven representation
                learning.
            </p>

            <p>
                Overall, our results highlight the potential of efficient, post hoc multimodal alignment in bridging the
                gap between disparate unimodal representations and a unified, high-performing representation space.
            </p>
        </section> -->



        <section id="future-directions">
            <h2>Future Directions</h2>

            <h3>1. Leveraging Multiple Captions for Alignment</h3>
            <p>
                In our current work, we use only the first caption from the Flickr30k dataset as a positive anchor for
                each image. To improve alignment quality, we will utilize all five captions per image. For each data
                point \( i \), we redefine the input as a sixtuplet \( (x^{(i)}, y_1^{(i)}, y_2^{(i)}, \dots, y_5^{(i)})
                \), where \( y_k^{(i)} \) represents the \( k \)-th caption.
            </p>
            <p>
                The contrastive loss then becomes:
            </p>
            <p>
                \[
                \mathcal{L}_{\text{multi-caption}} = -\frac{1}{N} \sum_{i=1}^N \log \frac{\exp\left(\frac{1}{5\tau}
                \sum_{k=1}^5 f_\text{image}(x^{(i)}) \cdot g_\text{text}(y_k^{(i)})\right)}{\sum_{j=1}^N
                \exp\left(\frac{1}{5\tau} \sum_{k=1}^5 f_\text{image}(x^{(i)}) \cdot g_\text{text}(y_k^{(j)})\right)}.
                \]
            </p>
            <p>
                This approach leverages the diversity of captions to potentially create more robust and semantically
                aligned representations.
            </p>

            <h3>2. Scaling to Richer Multimodal Datasets</h3>
            <p>
                We plan to extend our experiments beyond images and text by integrating a third modality, such as audio.
                For example, using the <strong><a
                        href="https://www.robots.ox.ac.uk/~vgg/data/vggsound/">VGGSound</strong></a> dataset and a
                pre-trained audio encoder like
                <strong><a href="https://huggingface.co/docs/transformers/en/model_doc/wav2vec2">Wav2Vec</strong></a>,
                we can evaluate whether incorporating audio further improves kernel alignment
                metrics compared to the two-modality setting.
            </p>
            <p>
                After that, we consider expanding our approach to as many modalities as there exists a suitable dataset
                for. To achieve this, we will generalize the dual-encoder contrastive loss to a <strong>multi-encoder
                    contrastive loss</strong> for \( M \) modalities. Specifically, the loss can be expressed as:
            </p>
            <p>
                \[
                \mathcal{L}_{\text{multi}} = -\frac{1}{N} \sum_{i=1}^N \log \frac{\exp\left(\prod_{m=1}^M f_m^{(i)} /
                \tau\right)}{\sum_{j=1}^N \exp\left(\prod_{m=1}^M f_m^{(j)} / \tau\right)},
                \]
            </p>
            <p>
                where \( f_m \) represents the embeddings from the \( m \)-th modality encoder, and the product of
                embeddings can be interpreted as a generalized <a
                    href="https://math.stackexchange.com/questions/1137102/dot-product-for-3-vectors">Hadamard
                    product</a>.
            </p>
            <p>
                A potential challenge is that VGGSound provides video IDs and timestamps rather than direct downloads,
                complicating dataset acquisition.
            </p>

            <h3>3. Zero-Shot Evaluation with Pre-Trained Heads</h3>
            <p>
                We plan to evaluate the aligned
                representations directly using pre-trained classifier heads (e.g., those available in the <a
                    href="https://github.com/facebookresearch/dinov2?tab=readme-ov-file">DINOv2 GitHub repository</a>).
                Specifically, we will replace DINOv2-small's feature extractor with our aligned multimodal encoder \(
                h_\text{multi} \) and measure classification performance on tasks like ImageNet. This approach will
                assess the quality of the aligned embeddings in a purely <strong>zero-shot</strong> setting.
            </p>


        </section>


    </main>

    <footer>
        <p> Quilee Simeon & Gabe Manso | MIT Deep Learning Final Project Blog | December 2024</p>
    </footer>
</body>


<style>
    figure {
        display: flex;
        /* Arrange images side by side */
        flex-direction: column;
        /* Ensure caption appears below images */
        align-items: center;
        /* Center the content */
        gap: 10px;
        /* Add space between images and caption */
    }

    .figures-container {
        display: flex;
        /* Arrange figures side by side */
        justify-content: center;
        /* Center the figures on the page */
        gap: 20px;
        /* Add spacing between the figures */
    }

    figure img {
        width: 400px;
        /* Adjust image width */
        height: auto;
        /* Maintain aspect ratio */
    }

    figcaption {
        text-align: center;
        /* Center-align the caption text */
        font-size: 14px;
        /* Adjust caption font size */
        color: #555;
        /* Add a softer color for the caption */
    }
</style>

</html>