<!DOCTYPE html>
<html lang="en">

<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <script type="text/javascript" async
        src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/3.2.2/es5/tex-mml-chtml.js"></script>
    <title>Aligning Modalities: Efficient Multimodal Representation Learning</title>
    <link rel="stylesheet" href="styles/style.css">
</head>

<body>
    <header>
        <h1>Aligning Modalities: Efficient Multimodal Representation Learning</h1>
    </header>

    <main>
        <section id="introduction">
            <h2>Introduction</h2>
            <p>
                Deep learning models are typically trained to transform raw data into representations optimized for
                specific tasks. Recently, two lines of research have inspired a deeper inquiry into the nature of these
                representations. The CLIP framework demonstrated the utility of aligning representations across
                modalities, using paired image-text data to train joint embeddings for cross-modal retrieval. Meanwhile,
                the Platonic Representation Hypothesis posits that performant models converge toward a shared
                statistical model of reality in their representation spaces, suggesting a potential universality
                underlying learned representations.
            </p>
            <p>
                This project bridges these ideas by exploring whether representations from disparate pre-trained
                unimodal neural networks can be aligned into a shared multimodal latent space, inspired by the joint
                embedding approach of CLIP and motivated by the convergence hypothesis of Platonic Representations. The
                proposed framework uses frozen unimodal encoders (e.g., ResNet-18 for images, GPT-2 for text) with
                learned linear adapters to align representations across modalities. Our aim is to determine if such
                aligned representations better approximate those of larger, more performant models (e.g., DinoV2).
            </p>
            <p>
                Inspired by the success of CLIP in aligning representations across modalities and the theoretical
                insights of the Platonic Representation Hypothesis, we propose a framework that aligns pre-trained
                unimodal encoders into a shared multimodal latent space using simple linear adapters. By leveraging
                frozen encoders such as ResNet-18 for images and GPT-2 for text, we aim to achieve alignment without
                retraining these large models, enabling scalability to new modalities with minimal overhead.
            </p>
            <figure>
                <img src="figures/img_txt_concept.svg" alt="Multimodal Alignment Architecture">
                <figcaption>
                    <strong>Figure 1.</strong> Conceptual illustration of our multimodal alignment framework. Image and
                    text representations
                    from pre-trained encoders are aligned into a shared latent space using learned adapters. The
                    dimension
                    of the latent space set to that of the performant model (DinoV2) for comparison.
                </figcaption>
            </figure>


            <h3>Motivation</h3>
            <p>
                Our approach is motivated by three key insights:
            </p>
            <ol>
                <li><strong>Inspiration from CLIP:</strong> The CLIP framework demonstrated that cross-modal
                    representations could be aligned through paired data and contrastive learning. However, it requires
                    joint training of encoders, limiting extensibility to additional modalities. Our method decouples
                    the encoders, focusing instead on aligning their outputs via lightweight adapters.</li>
                <li><strong>Testing the Platonic Representation Hypothesis:</strong> This hypothesis posits that
                    performant models converge toward a shared statistical model of reality in their representation
                    spaces. By aligning diverse unimodal encoders, we provide a testbed for exploring whether this
                    convergence can be achieved explicitly.</li>
                <li><strong>Scalability and Modularity:</strong> Traditional multimodal models often require joint
                    training on extensive datasets. By aligning frozen encoders post hoc, our framework supports modular
                    integration of new modalities, enabling efficient experimentation.</li>
            </ol>
        </section>

        <section id="related-work">
            <h2>Related Work</h2>
            <p>
                Our project builds on several important advances in multimodal representation learning and theoretical
                insights into model convergence:
            </p>
            <ol>
                <li>
                    <strong>Platonic Representation Hypothesis:</strong> Huh et al. (2024) proposed that performant
                    models converge toward a shared statistical model of reality in their representation spaces,
                    regardless of their training modality. This hypothesis underpins our framework, which aims to
                    explicitly align unimodal encoders to test this convergence hypothesis.
                </li>
                <li>
                    <strong>CLIP:</strong> Radford et al. (2021) introduced CLIP, a model that learns joint multimodal
                    representations using contrastive learning on paired image-text datasets. CLIP's success
                    demonstrates the power of cross-modal embeddings but requires joint training of encoders, which our
                    framework aims to circumvent.
                </li>
                <li>
                    <strong>Linear Mapping from Image to Text Space:</strong> Merullo et al. (2022) showed that simple
                    linear transformations can align visual representations to text spaces, enabling cross-modal tasks
                    like visual question answering. Their findings inspired our use of linear adapters for modality
                    alignment.
                </li>
                <li>
                    <strong>Grounding Language Models to Images:</strong> Koh et al. (2023) extended the idea of
                    multimodal alignment by mapping language models to visual inputs and outputs. This work highlights
                    the potential of lightweight transformations for cross-modal integration.
                </li>
                <li>
                    <strong>DINOv2:</strong> Oquab et al. (2023) introduced DINOv2, a self-supervised vision model that
                    generates robust embeddings. As our performant reference model, DINOv2 provides a benchmark for
                    evaluating the quality of our aligned multimodal representations.
                </li>
                <!-- TODO: Add this paper to the related work section: 
                Tongzhou Wang and Phillip Isola. Understanding contrastive representation learning through alignment and uniformity on the hypersphere. In International Conference on Machine Learning, pages 9929â€“9939. PMLR, 2020. -->
            </ol>
        </section>

        <section id="hypothesis">
            <h2>Hypothesis</h2>
            <p>
                Our work is grounded in the following hypotheses:
            </p>
            <ul>
                <li>A shared latent space exists where unimodal representations from different encoders can be aligned
                    through linear transformations.</li>
                <li>Aligning these representations produces embeddings that closely approximate those of performant
                    models, such as DinoV2, as measured by kernel alignment metrics.</li>
                <li>Multimodal alignment captures mechanisms of representation convergence, offering empirical evidence
                    for the Platonic Representation Hypothesis.</li>
            </ul>
        </section>

        <section id="mathematical-framework">
            <h2>Mathematical Framework</h2>
            <p>
                At the core of our project is the hypothesis that pre-trained unimodal representations can be aligned
                into a shared multimodal latent space. This section formalizes the mathematical structure underlying our
                framework, detailing how representations are extracted, aligned, and evaluated.
            </p>

            <h3>Multimodal Data Representation</h3>
            <p>
                Let the world generate raw multimodal data:
            </p>
            <p>
                \[
                \mathcal{D}_\text{world} = \left\{\left(\theta^{(i)}, \psi^{(i)}, \phi^{(i)}, \dots \right)
                \right\}_{i=1}^N, \quad \theta \in \mathcal{\Theta}, \psi \in \mathcal{\Psi}, \phi \in \mathcal{\Phi},
                \dots
                \]
            </p>
            <p>
                where \( \mathcal{\Theta}, \mathcal{\Psi}, \mathcal{\Phi} \) represent different modalities (e.g.,
                image, text, audio), and \( \theta, \psi, \phi \) are specific instances of these modalities.
            </p>
            <p>
                For simplicity, we focus on a two-modality setting with images (\( \mathcal{X} \)) and text (\(
                \mathcal{Y} \)):
            </p>
            <p>
                \[
                \mathcal{D}_\text{world} = \left\{\left(x^{(i)}, y^{(i)}\right)\right\}_{i=1}^N, \quad x \in
                \mathcal{X}, y \in \mathcal{Y}.
                \]
            </p>
            <figure>
                <img src="figures/platonic_representation.svg" alt="Platonic Representation Hypothesis">
                <figcaption>
                    <strong>Figure 2.</strong> The Platonic Representation Hypothesis (Huh et al., 2024) suggests that
                    representations from different modalities converge to a shared Platonic representation. We
                    hypothesize that contrastive learning can actively drive this convergence by aligning
                    representations from different modalities into a shared latent space.
                </figcaption>
            </figure>

            <h3>Learned Adapters</h3>
            <p>
                We use frozen pre-trained encoders to extract representations from each modality:
            </p>
            <p>
                \[
                X_\text{enc}: \mathcal{X} \rightarrow \mathbb{R}^{d_x}, \quad Y_\text{enc}: \mathcal{Y} \rightarrow
                \mathbb{R}^{d_y},
                \]
            </p>
            <p>
                where \( d_x \) and \( d_y \) are the embedding dimensions for images and text, respectively.
            </p>
            <p>
                To project these representations into a shared latent space \( \mathbb{R}^{d_e} \), we introduce learned
                adapters:
            </p>
            <p>
                \[
                W_x : \mathbb{R}^{d_x} \rightarrow \mathbb{R}^{d_e}, \quad W_y : \mathbb{R}^{d_y} \rightarrow
                \mathbb{R}^{d_e}.
                \]
            </p>
            <p>
                We explore both <strong>linear adapters</strong> and <strong>2-layer MLP adapters</strong>. The adapted
                encoders are:
            </p>
            <p>
                \[
                f_\text{img} = W_x \circ X_\text{enc}, \quad g_\text{text} = W_y \circ Y_\text{enc},
                \]
            </p>
            <p>
                which map images and text to the shared latent space \( \mathbb{R}^{d_e} \).
            </p>

            <h3>Dual-Encoder Contrastive Objective</h3>
            <p>
                To align the representations, we use a <strong>dual-encoder contrastive loss</strong>, inspired by the
                formulation in <a href="https://phillipi.github.io/prh/">PRH</a> and Homework 4.
            </p>
            <!-- <figure>
                <img src="figures/homework4_contrastive.jpg" alt="Dual-Encoder Contrastive Loss">
            </figure> -->
            <figure>
                <img src="figures/conceptual_contrastive.svg" alt="Conceptual Multimodal Alignment">
                <figcaption>
                    <strong>Figure 3.</strong> Matching image-text pairs from the Flickr30k dataset are contrastively
                    aligned in
                    the latent space, while non-matching pairs are pushed apart. The aim is to approximate the Platonic
                    representations, which we use the embeddings of a performatn model (DinoV2) as a proxy for.
                </figcaption>
            </figure>

            <p>
                The loss encourages
                representations from matching pairs to be similar while pushing apart representations from non-matching
                pairs:
            </p>
            <p>
                \[
                \mathcal{L}_{\text{contrastive}} = -\frac{1}{N} \sum_{i=1}^N \log \frac{\exp\left(f_\text{img}(x^{(i)})
                \cdot g_\text{text}(y^{(i)}) / \tau\right)}{\sum_{j=1}^N \exp\left(f_\text{img}(x^{(i)}) \cdot
                g_\text{text}(y^{(j)}) / \tau\right)},
                \]
            </p>
            <p>
                where \( \tau \) is the temperature parameter that scales the similarity scores.
            </p>


            <h3>Interpretation and Connection to PRH</h3>
            <p>
                The <strong>Platonic Representation Hypothesis (PRH)</strong> posits that representations from different
                modalities converge toward a shared "Platonic" representation. In our framework, we hypothesize that
                <strong>contrastive alignment</strong> between the image encoder \( f_{\text{img}} \) and the text
                encoder \( g_{\text{text}} \) can drive this convergence toward the Platonic representation.
            </p>

            <p>
                In the ideal case of perfect contrastive alignment (achieved by minimizing the diameter and maximizing
                the margin), the outputs of \( f_{\text{img}}(x) \) and \( g_{\text{text}}(y) \) for matching pairs
                would map to the <strong>same vector</strong> in the shared latent space:
            </p>

            <p>
                \[
                f_{\text{img}}(x^{(i)}) = g_{\text{text}}(y^{(i)}).
                \]
            </p>

            <p>
                Our thesis is that this limiting vector represents the Platonic representation. However, in practice,
                due to finite data, model capacity, and noise, the outputs of \( f_{\text{img}} \) and \(
                g_{\text{text}} \) are typically <strong>close</strong> (as measured by the \( L_2
                \)-distance on the unit hypersphere \( \mathbb{S}^{d_e-1} \)) but not identical.
            </p>

            <p>
                Given this, we propose two strategies for obtaining the aligned multimodal representation \( r^{(i)} \):
            </p>

            <ol>
                <li>
                    <strong>Selection:</strong> Choose the representation that is closer to the performant model's
                    representation (e.g., DinoV2):
                    <p>
                        \[
                        r^{(i)} =
                        \begin{cases}
                        f_{\text{img}}(x^{(i)}) & \text{if } \operatorname{sim}(f_{\text{img}}(x^{(i)}),
                        \text{DinoV2}(x^{(i)})) > \operatorname{sim}(g_{\text{text}}(y^{(i)}), \text{DinoV2}(x^{(i)}))
                        \\
                        g_{\text{text}}(y^{(i)}) & \text{otherwise}.
                        \end{cases}
                        \]
                    </p>
                </li>
                <li>
                    <strong>Averaging:</strong> Take the average of the image and text embeddings:
                    <p>
                        \[
                        r^{(i)} = \frac{1}{2} \left(f_{\text{img}}(x^{(i)}) + g_{\text{text}}(y^{(i)})\right).
                        \]
                    </p>
                </li>
            </ol>

            <p>
                We claim that this aligned multimodal representation \( r^{(i)} \) is an <strong>approximation</strong>
                of the Platonic representation. While the true Platonic representation is unknown, we follow the PRH
                conjecture that performant models like DinoV2 produce representations that are converging toward it.
                Therefore, we use DinoV2â€™s representation as a <strong>proxy</strong> for the Platonic representation.
            </p>

            <p>
                We evaluate the quality of the aligned multimodal representation \( r^{(i)} \) by comparing it to the
                DinoV2 representation using a kernel alignment metric.
            </p>

            <h3>Kernel Alignment Metric</h3>
            <p>
                We characterize representations in terms of their <strong>kernels</strong>, which capture the similarity
                between data points. Two representations are considered <strong>aligned</strong> if their kernels are
                similar for corresponding inputs. For example, if the text encoder \( g_{\text{text}} \) is aligned with
                the image encoder \( f_{\text{img}} \), the similarity between text representations of "apple" and
                "orange" should correspond closely to the similarity between their image representations.
            </p>

            <p>
                To quantify this alignment, we use the <strong>mutual \(k\)-nearest neighbor (mutual-KNN) kernel
                    alignment metric</strong> \( m \), as introduced by <a href="https://arxiv.org/abs/2405.07987">Huh
                    et al. (2024)</a>. We define the kernels as:
            </p>

            <ul>
                <li>
                    <strong>Unimodal Kernels:</strong>
                    <p>
                        \[
                        K_X(i, j) = \langle X_\text{enc}(x^{(i)}), X_\text{enc}(x^{(j)}) \rangle, \quad K_Y(i, j) =
                        \langle Y_\text{enc}(y^{(i)}), Y_\text{enc}(y^{(j)}) \rangle.
                        \]
                    </p>
                </li>
                <li>
                    <strong>Aligned Multimodal Kernel:</strong>
                    <p>
                        \[
                        K_{\text{repr}}(i, j) = \langle r^{(i)}, r^{(j)} \rangle,
                        \]
                    </p>
                    where \( r^{(i)} \) is the aligned multimodal representation.
                </li>
                <li>
                    <strong>Performant Model Kernel:</strong>
                    <p>
                        \[
                        K_\text{DinoV2}(i, j) = \langle \text{DinoV2}(x^{(i)}), \text{DinoV2}(x^{(j)}) \rangle.
                        \]
                    </p>
                </li>
            </ul>

            <h4>Before Training</h4>
            <p>
                We compute the alignment metrics for the unimodal kernels relative to the performant model kernel:
            </p>
            <p>
                \[
                m(K_X, K_{\text{DinoV2}}), \quad m(K_Y, K_{\text{DinoV2}}).
                \]
            </p>

            <h4>After Training</h4>
            <p>
                We evaluate the mutual-KNN alignment for the aligned multimodal kernel against the performant model
                kernel:
            </p>
            <p>
                \[
                m(K_{\text{repr}}, K_{\text{DinoV2}}).
                \]
            </p>

            <h4>Key Hypothesis</h4>
            <p>
                We hypothesize that the aligned multimodal kernel achieves higher similarity with \( K_{\text{DinoV2}}
                \) than the average of the unimodal kernels:
            </p>
            <p>
                \[
                m(K_{\text{repr}}, K_{\text{DinoV2}}) > \text{avg}\left(m(K_X, K_{\text{DinoV2}}), m(K_Y,
                K_{\text{DinoV2}})\right).
                \]
            </p>

            <p>
                A stronger version of this hypothesis posits that the aligned multimodal kernel surpasses even the best
                unimodal kernel:
            </p>
            <p>
                \[
                m(K_{\text{repr}}, K_{\text{DinoV2}}) > \max\left(m(K_X, K_{\text{DinoV2}}), m(K_Y,
                K_{\text{DinoV2}})\right).
                \]
            </p>

            <p>
                This would suggest an emergent property where the whole (multimodal representation) is greater than the
                sum of its parts (unimodal representations), aligning more closely with the hypothesized Platonic
                representation.
            </p>

        </section>

        <section id="methodology">
            <p>
                All code we used for this project is implemented in this <a
                    href="https://colab.research.google.com/drive/1BkyPko0x-8CL41Z-VSmyxI41B90hB2Fc?usp=sharing">Google
                    Colab notebook</a>.
            </p>
            <h2>Methodology</h2>

            <h3>Data Pipeline</h3>
            <p>
                We use the <strong>Flickr30k</strong> dataset, which contains paired image-caption samples. The data
                pipeline processes these pairs to generate tensors compatible with our models. Key stages of the
                pipeline include:
            </p>
            <ul>
                <li><strong>Image Processing:</strong>
                    <ul>
                        <li>Resize images to \(224 \times 224\) pixels.</li>
                        <li>Normalize using ImageNet statistics.</li>
                        <li>Convert to tensors suitable for ResNet-18.</li>
                    </ul>
                </li>
                <li><strong>Text Processing:</strong>
                    <ul>
                        <li>Tokenize captions using the DistilBERT tokenizer.</li>
                        <li>Pad or truncate tokens to a maximum length of 64.</li>
                        <li>Convert tokens to tensors suitable for DistilBERT.</li>
                    </ul>
                </li>
                <li><strong>Data Loading:</strong>
                    <ul>
                        <li>Batch size: 256</li>
                        <li>75%-25% train-validation split.</li>
                        <li>Optimized for GPU utilization with shuffling and pinning memory.</li>
                    </ul>
                </li>
            </ul>
            <figure>
                <img src="figures/data_pipeline.png" alt="Data Pipeline">
                <figcaption><strong>Figure 1.</strong> The data pipeline preprocesses images and text for input into the
                    model.</figcaption>
            </figure>

            <h3>Model Architecture</h3>
            <p>
                The architecture consists of frozen pre-trained encoders and trainable adapters that align
                representations in a shared multimodal space.
            </p>
            <ul>
                <li><strong>Image Encoder:</strong> ResNet-18 (frozen, output dimension: 512).</li>
                <li><strong>Text Encoder:</strong> DistilBERT (frozen, output dimension: 768).</li>
                <li><strong>Adapters:</strong>
                    <ul>
                        <li><strong>Linear Adapter:</strong> Projects encoder outputs to a shared dimension (384):
                            \[
                            W_x : \mathbb{R}^{512} \to \mathbb{R}^{384}, \quad W_y : \mathbb{R}^{768} \to
                            \mathbb{R}^{384}.
                            \]
                        </li>
                        <li><strong>MLP Adapter:</strong> A two-layer MLP with GELU activation and LayerNorm:
                            \[
                            \operatorname{MLP}(x) = \operatorname{LayerNorm}(\operatorname{GELU}(W_2
                            \operatorname{GELU}(W_1 x))).
                            \]
                        </li>
                    </ul>
                </li>
            </ul>
            <figure>
                <img src="figures/model_architecture.png" alt="Model Architecture">
                <figcaption><strong>Figure 2.</strong> Frozen encoders and trainable adapters align image and text
                    representations.</figcaption>
            </figure>

            <h3>Training Procedure</h3>
            <p>
                We train the adapters using a <strong>bidirectional contrastive loss</strong> with temperature scaling
                (\(\tau = 0.07\)):
            </p>
            <p>
                \[
                \mathcal{L} = -\frac{1}{N} \sum_{i=1}^N \log \frac{\exp\left(f_\text{img}(x^{(i)}) \cdot
                g_\text{text}(y^{(i)}) / \tau\right)}{\sum_{j=1}^N \exp\left(f_\text{img}(x^{(i)}) \cdot
                g_\text{text}(y^{(j)}) / \tau\right)}.
                \]
            </p>
            <p>The training loop consists of:</p>
            <ol>
                <li>Forward pass through frozen encoders and adapters.</li>
                <li>Compute contrastive loss for matching and non-matching pairs.</li>
                <li>Update adapter weights using the AdamW optimizer with:
                    <ul>
                        <li>Learning rate: \(5 \times 10^{-5}\) (cosine decay with warmup).</li>
                        <li>Gradient clipping (norm: 1.0).</li>
                        <li>Weight decay: 0.01.</li>
                    </ul>
                </li>
            </ol>
            <figure>
                <img src="figures/img_txt_concept.svg" alt="Multimodal Alignment Architecture">
                <figcaption><strong>Figure 3.</strong> Contrastive alignment of image and text representations in a
                    shared latent space.</figcaption>
            </figure>

            <h3>Evaluation Metrics</h3>
            <p>
                We evaluate alignment using the <strong>mutual-KNN kernel alignment metric</strong> \(m\):
            </p>
            <ul>
                <li><strong>Unimodal Kernels:</strong> Measure similarity between ResNet-18 and DistilBERT
                    representations and DinoV2.</li>
                <li><strong>Aligned Kernel:</strong> Measure similarity of aligned representations to DinoV2.</li>
                <li><strong>Downstream Task:</strong> Classification on CIFAR-10 using aligned embeddings as feature
                    inputs.</li>
            </ul>
            <figure>
                <img src="figures/kernel_heatmaps.png" alt="Kernel Alignment Heatmaps">
                <figcaption><strong>Figure 4.</strong> Kernel similarity heatmaps before and after training.
                </figcaption>
            </figure>
        </section>

        <section id="results">
            <h2>Results</h2>

            <h3>Experiment 1: Alignment Training</h3>
            <p>
                In this experiment, we train the adapters to align image and text representations using the contrastive
                loss. The key findings are:
            </p>
            <ul>
                <li><strong>Kernel Alignment Improvement:</strong> The mutual-KNN alignment metric improves
                    significantly after training, supporting our hypothesis.</li>
            </ul>

            <figure>
                <img src="figures/kernel_metric_barplot.svg" alt="Kernel Metric Bar Plot">
                <figcaption><strong>Figure 5.</strong> Bar plot showing the kernel alignment metric before and after
                    training.</figcaption>
                <!-- TODO: Better caption this based on wha's shwn in the figure. -->
            </figure>

            <p>
                <strong>Visualization of Aligned Representations:</strong> PCA plots show the improved alignment of
                image and text embeddings after training.
            </p>

            <figure>
                <img src="figures/pca_alignment.png" alt="PCA of Aligned Representations">
                <figcaption><strong>Figure 6.</strong> PCA of image and text representations before and after alignment.
                </figcaption>
            </figure>

            <h3>Experiment 2: Downstream Classification Task</h3>
            <p>
                We evaluate the quality of the aligned embeddings by using them as feature inputs for a CIFAR-10
                classification task. The results show:
            </p>
            <ul>
                <li><strong>Baseline (DinoV2):</strong> 97.96% accuracy.</li>
                <li><strong>Aligned Embeddings:</strong> 76.49% accuracy.</li>
                <li><strong>Image-Only Embeddings:</strong> 78.21% accuracy.</li>
            </ul>

            <figure>
                <img src="figures/downstream_results.png" alt="Downstream Classification Results">
                <figcaption><strong>Figure 7.</strong> CIFAR-10 classification performance using different embeddings.
                </figcaption>
            </figure>

            <p>
                These results indicate that while the aligned embeddings perform worse than DinoV2, they capture
                meaningful multimodal features, supporting the effectiveness of our alignment approach.
            </p>
        </section>


        <section id="future-directions">
            <h2>Future Directions</h2>
            <ul>
                <li>Scale experiments to richer multimodal datasets and benchmarks.</li>
                <li>Investigate cross-architecture generalization of aligned embeddings.</li>
                <li>Analyze mechanistic insights into alignment-driven representation convergence.</li>
            </ul>
        </section>

        <section id="conclusion">
            <h2>Conclusion</h2>
            <p>
                This project illustrates a scalable and efficient approach to aligning unimodal encoders into a shared
                multimodal space. By leveraging pre-trained models and lightweight adapters, we unlock potential
                applications in resource-efficient AI and multimodal representation learning. Our findings highlight the
                promise of this methodology and pave the way for further exploration into the convergence of
                representations across modalities.
            </p>
        </section>

        <section id="discussion">
            <h2>Discussion</h2>
            <p>
                In this project, we build upon a central premise of the Platonic Representation Hypothesis (PRH)
                proposed by Huh et al. (2024): the idea that the representations learned by performant models converge
                toward an idealized, universal representation of the world â€” a <strong>Platonic representation</strong>.
                We take this hypothesis as a starting point and assume that the representations produced by
                <strong>DinoV2</strong> serve as a reasonable approximation of this Platonic representation.
            </p>

            <p>
                Our primary conjecture is that one mechanism to achieve this Platonic representation is through the
                alignment of representations from different modalities into a shared latent space. Specifically, by
                using contrastive learning to align representations from <strong>pre-trained image and text
                    encoders</strong> (ResNet-18 and DistilBERT), we hypothesize that the resulting multimodal
                representation can approximate the Platonic representation more closely than either of the unimodal
                representations alone.
            </p>

            <h3>Driving Convergence via Alignment</h3>
            <p>
                The PRH suggests that convergence of representations occurs naturally due to factors such as:
            </p>
            <ul>
                <li>Training on diverse, multi-task datasets.</li>
                <li>Increasingly performant and large-scale model architectures.</li>
                <li>Utilization of multimodal data during training.</li>
            </ul>
            <p>
                Our work extends this hypothesis by proposing an <strong>explicit algorithmic mechanism</strong> for
                driving representational convergence: aligning pre-trained unimodal representations using
                <strong>contrastive learning</strong>. Instead of relying on joint training of large multimodal models,
                our approach demonstrates that:
            </p>
            <ul>
                <li>Lightweight linear and MLP adapters can effectively align frozen unimodal encoders.</li>
                <li>Contrastive alignment in a shared latent space can achieve meaningful convergence toward the
                    Platonic representation.</li>
            </ul>

            <p>
                This approach offers a more efficient alternative to end-to-end multimodal training. By aligning
                representations post hoc, we enable <strong>modular scalability</strong>, where new modalities can be
                incorporated without retraining the entire model.
            </p>

            <h3>Empirical Insights</h3>
            <p>
                Our results provide partial validation of our hypotheses. Specifically, we observed that the
                <strong>aligned multimodal representations</strong> exhibit:
            </p>
            <ul>
                <li><strong>Improved Similarity</strong> to DinoV2 embeddings compared to the average of unimodal
                    representations.</li>
                <li>A <strong>Cross-Modality Benefit</strong> where text representations, which DinoV2 was not trained
                    to process, became more similar to the image representations.</li>
            </ul>
            <p>
                However, the aligned multimodal representation did not surpass the best-performing unimodal
                representation (the image modality) in terms of kernel alignment. This caveat suggests that while
                alignment can drive convergence, there may still be modality-specific factors that influence the quality
                of representations.
            </p>

            <h3>Implications and Future Directions</h3>
            <p>
                Our findings suggest that multimodal alignment via contrastive learning is a promising direction for
                achieving efficient and scalable multimodal representation learning. This work also opens up several
                avenues for further research:
            </p>
            <ul>
                <li><strong>Incorporating Additional Modalities:</strong> Expanding the framework to include audio,
                    video, or graph data could further test the robustness of the alignment mechanism.</li>
                <li><strong>Downstream Task Evaluation:</strong> Assessing the utility of aligned representations on
                    tasks like visual question answering, retrieval, or classification.</li>
                <li><strong>Mechanistic Analysis:</strong> Investigating the internal mechanisms of convergence and how
                    specific adapter architectures influence the alignment process.</li>
            </ul>

            <p>
                Ultimately, this work supports the broader goal of understanding how different modalities can be
                integrated to form a cohesive, universal representation of the world â€” a pursuit that lies at the heart
                of the Platonic Representation Hypothesis.
            </p>
        </section>

    </main>

    <footer>
        <p>Multimodal Alignment Analysis | Blog | November 2024</p>
    </footer>
</body>



<!-- <section id="initial-results">
    <h2>Initial Results</h2>
    <p>
        Our experiments with image and text modalities have yielded promising results. By aligning
        representations from ResNet-18 and DistilBERT, we observed:
    </p>
    <ul>
        <li><strong>Alignment Success:</strong> Aligned representations demonstrated improved similarity to
            DinoV2 embeddings.</li>
        <li><strong>Cross-Modality Insights:</strong> Text embeddings, which DinoV2 was not trained to process,
            gained representational properties closer to its image embeddings.</li>
        <li><strong>Caveat:</strong> While the aligned representation exceeded the average similarity of
            unimodal kernels to DinoV2, it fell short of the maximum similarity (i.e., the image modality
            kernel).</li>
    </ul>
    <figure>
        <img src="figures/training_loss_metrics.png" alt="Training Metrics">
        <figcaption>Training loss and kernel alignment metrics over epochs.</figcaption>
    </figure>

    <h3>Metrics</h3>
    <p>
        Key kernel alignment metrics include:
    </p>
    <ul>
        <li>Text-to-DinoV2 similarity: Low, due to modality mismatch.</li>
        <li>Image-to-DinoV2 similarity: High, reflecting shared training data.</li>
        <li>Aligned multimodal similarity: Intermediate, surpassing the average of unimodal similarities.</li>
    </ul>
    <div class="figures-container">
        <figure>
            <img src="figures/mlp/2d/m2d.gif" alt="2D MLP Adapter">
            <figcaption>MLP Adapter</figcaption>
        </figure>
        <figure>
            <img src="figures/mlp/3d/m3d.gif" alt="3D MLP Adapter">
            <figcaption>MLP Adapter</figcaption>
        </figure>
    </div>
    <div class="figures-container">
        <figure>
            <img src="figures/linear/2d/l2d.gif" alt="2D Linear Adapter">
            <figcaption>Linear Adapter</figcaption>
        </figure>
        <figure>
            <img src="figures/linear/3d/l3d.gif" alt="3D Linear Adapter">
            <figcaption>Linear Adapter</figcaption>
        </figure>
    </div>
    <p>
        The bar plot in Figure 5 visually summarizes the kernel alignment metrics before and after training the
        adapters.
        We can immediately inspect whether our hypotheses stated in the Mathematical Framework section are
        supported or not.
    </p>
    <figure>
        <img src="figures/bar_plot_sketch.jpg" alt="Kernel Alignment Metric Before and After Trainign">
        <figcaption><strong>Figure 5.</strong>Placeholder: Bar plot showing kernel alignment for text, image,
            and multimodal embeddings
            before and after training adapters to align unimodal representations into a latent multimodal
            representation using contrastive loss. The top subplot is for the linear adapter. The bottom suplot
            is for the MLP adapter.
            Each suplplot shows bars for before and after training of the adapters for the image (hacthed bars)
            and text (solid bars) modalities.
            The y-axis is the mutual-kNN kernel alignment metric \(m\) between the modality embedding and the
            DinoV2 embedding
            (which we assume approximates the Platonic representation).</figcaption>
    </figure>
</section>

<section id="proposed-extensions">
    <h2>Proposed Extensions</h2>
    <h3>1. Downstream Task Evaluation</h3>
    <p>
        Evaluate aligned embeddings on downstream tasks, such as classification, using pre-trained heads.
        Hypotheses include:
    </p>
    <ul>
        <li>Slight performance degradation compared to DinoV2.</li>
        <li>Aligned multimodal embeddings outperform unimodal embeddings.</li>
    </ul>

    <h3>2. Adding a Third Modality</h3>
    <p>
        Expand the framework to include audio representations from models like Wav2Vec2. Test whether
        integrating a third modality enhances alignment and demonstrates additive representation benefits.
    </p>

    <figure>
        <img src="figures/multimodal-alignment.svg" alt="Multimodal Alignment Process">
        <figcaption>Placeholder: Diagram illustrating multimodal alignment across three modalities (image, text,
            and audio).</figcaption>
    </figure>
</section> -->

<style>
    figure {
        display: flex;
        /* Arrange images side by side */
        flex-direction: column;
        /* Ensure caption appears below images */
        align-items: center;
        /* Center the content */
        gap: 10px;
        /* Add space between images and caption */
    }

    .figures-container {
        display: flex;
        /* Arrange figures side by side */
        justify-content: center;
        /* Center the figures on the page */
        gap: 20px;
        /* Add spacing between the figures */
    }

    figure img {
        width: 400px;
        /* Adjust image width */
        height: auto;
        /* Maintain aspect ratio */
    }

    figcaption {
        text-align: center;
        /* Center-align the caption text */
        font-size: 14px;
        /* Adjust caption font size */
        color: #555;
        /* Add a softer color for the caption */
    }
</style>

</html>