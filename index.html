<!DOCTYPE html>
<html lang="en">

<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Aligning Modalities: Efficient Multimodal Representation Learning</title>
    <link rel="stylesheet" href="styles/style.css">
</head>

<body>
    <header>
        <h1>Aligning Modalities: Efficient Multimodal Representation Learning</h1>
    </header>

    <main>
        <section id="introduction">
            <h2>Introduction</h2>
            <p>
                In recent years, deep learning has revolutionized the way we process and understand multimodal data,
                which combines information from diverse sources like images, text, and audio. However, the creation of
                such systems often demands immense computational resources and end-to-end training of massive neural
                networks. Our project aims to tackle these challenges by introducing a modular and efficient approach to
                multimodal representation learning.
            </p>
            <p>
                Inspired by the success of CLIP in aligning representations across modalities and the theoretical
                insights of the Platonic Representation Hypothesis, we propose a framework that aligns pre-trained
                unimodal encoders into a shared multimodal latent space using simple linear adapters. By leveraging
                frozen encoders such as ResNet-18 for images and GPT-2 for text, we aim to achieve alignment without
                retraining these large models, enabling scalability to new modalities with minimal overhead.
            </p>

            <figure>
                <img src="figures/img_txt_concept.svg" alt="Multimodal Alignment Process">
                <figcaption>Placeholder: Diagram illustrating multimodal alignment across 2 modalities (image, text,
                    and audio).</figcaption>
            </figure>

            <h3>Motivation</h3>
            <p>
                Our approach is motivated by three key insights:
            </p>
            <ol>
                <li><strong>Learning from CLIP:</strong> The CLIP framework demonstrated that cross-modal
                    representations could be aligned through paired data and contrastive learning. However, it requires
                    joint training of encoders, limiting extensibility to additional modalities. Our method decouples
                    the encoders, focusing instead on aligning their outputs via lightweight adapters.</li>
                <li><strong>Testing the Platonic Representation Hypothesis:</strong> This hypothesis posits that
                    performant models converge toward a shared statistical model of reality in their representation
                    spaces. By aligning diverse unimodal encoders, we provide a testbed for exploring whether this
                    convergence can be achieved explicitly.</li>
                <li><strong>Scalability and Modularity:</strong> Traditional multimodal models often require joint
                    training on extensive datasets. By aligning frozen encoders post hoc, our framework supports modular
                    integration of new modalities, enabling efficient experimentation.</li>
            </ol>
        </section>

        <section id="hypothesis">
            <h2>Hypothesis</h2>
            <p>
                Our work is grounded in the following hypotheses:
            </p>
            <ul>
                <li>A shared latent space exists where unimodal representations from different encoders can be aligned
                    through linear transformations.</li>
                <li>Aligning these representations produces embeddings that closely approximate those of performant
                    models, such as DinoV2, as measured by kernel alignment metrics.</li>
                <li>Multimodal alignment captures mechanisms of representation convergence, offering empirical evidence
                    for the Platonic Representation Hypothesis.</li>
            </ul>
        </section>

        <section id="methodology">
            <h2>Methodology</h2>

            <h3>Mathematical Framework</h3>
            <p>
                At the heart of our project is the hypothesis that the representation spaces of unimodal models can be
                aligned into a shared multimodal latent space. This section formalizes the key mathematical elements
                underpinning this approach.
            </p>

            <h4>Multimodal Data Representation</h4>
            <p>
                Let the world generate data across multiple modalities:
            </p>
            <code>
                ùíü<sub>world</sub> = { (x<sup>(i)</sup>, y<sup>(i)</sup>) | i = 1, ..., N }
            </code>
            <p>
                where <em>x</em> represents an instance in the image modality (<em>ùí≥</em>), and <em>y</em> represents
                an instance in the text modality (<em>ùí¥</em>). For each modality, frozen pre-trained encoders are used
                to extract representations:
            </p>
            <code>
                X<sub>enc</sub>: ùí≥ ‚Üí ‚Ñù<sup>d<sub>x</sub></sup>, <br>
                Y<sub>enc</sub>: ùí¥ ‚Üí ‚Ñù<sup>d<sub>y</sub></sup>
            </code>
            <p>
                These functions map raw data into feature spaces ‚Ñù<sup>d<sub>x</sub></sup> and
                ‚Ñù<sup>d<sub>y</sub></sup>, where <em>d<sub>x</sub></em> and <em>d<sub>y</sub></em> are the
                dimensionalities of the image and text feature spaces, respectively.
            </p>

            <h4>Learned Adapters</h4>
            <p>
                To align representations into a shared latent space ‚Ñù<sup>d<sub>e</sub></sup>, we introduce linear
                adapters:
            </p>
            <code>
                W<sub>x</sub>: ‚Ñù<sup>d<sub>x</sub></sup> ‚Üí ‚Ñù<sup>d<sub>e</sub></sup>, <br>
                W<sub>y</sub>: ‚Ñù<sup>d<sub>y</sub></sup> ‚Üí ‚Ñù<sup>d<sub>e</sub></sup>
            </code>
            <p>
                These adapters are the only trainable components of the system. Their weights are optimized to minimize
                the distance between paired representations in the shared latent space and maximize the separation
                between unpaired representations.
            </p>

            <h4>Contrastive Learning Objective</h4>
            <p>
                The contrastive loss function encourages the model to align matching pairs (x<sup>(i)</sup>,
                y<sup>(i)</sup>) while pushing apart non-matching pairs:
            </p>
            <code>
                ‚Ñí = - (1/N) ‚àë<sub>i=1</sub><sup>N</sup> log (
                    exp( ‚ü®W<sub>x</sub>X<sub>enc</sub>(x<sup>(i)</sup>), W<sub>y</sub>Y<sub>enc</sub>(y<sup>(i)</sup>)‚ü© / œÑ ) /
                    ‚àë<sub>j=1</sub><sup>N</sup> exp( ‚ü®W<sub>x</sub>X<sub>enc</sub>(x<sup>(i)</sup>), W<sub>y</sub>Y<sub>enc</sub>(y<sup>(j)</sup>)‚ü© / œÑ )
                )
            </code>
            <p>
                Here, <em>œÑ</em> is a temperature parameter controlling the sharpness of similarity scores, and ‚ü®¬∑,¬∑‚ü©
                denotes the cosine similarity between embeddings.
            </p>

            <h4>Kernel Alignment Metrics</h4>
            <p>
                To evaluate the learned multimodal representation, we define kernel functions for the unimodal encoders
                and the aligned representations:
            </p>
            <ul>
                <li>
                    <strong>Unimodal Kernels:</strong> Pre-trained encoder kernels capture the similarity of instances
                    within a modality:
                    <code>
                        K<sub>X</sub>(i, j) = ‚ü®X<sub>enc</sub>(x<sup>(i)</sup>), X<sub>enc</sub>(x<sup>(j)</sup>)‚ü©, <br>
                        K<sub>Y</sub>(i, j) = ‚ü®Y<sub>enc</sub>(y<sup>(i)</sup>), Y<sub>enc</sub>(y<sup>(j)</sup>)‚ü©
                    </code>
                </li>
                <li>
                    <strong>Aligned Kernel:</strong> The kernel of the shared multimodal representation is:
                    <code>
                        K<sub>aligned</sub>(i, j) = ‚ü®r<sup>(i)</sup>, r<sup>(j)</sup>‚ü©, where r<sup>(i)</sup> = W<sub>x</sub>X<sub>enc</sub>(x<sup>(i)</sup>)
                    </code>
                </li>
                <li>
                    <strong>Performant Model Kernel:</strong> The kernel of a performant model (e.g., DinoV2) serves as
                    a reference:
                    <code>
                        K<sub>DinoV2</sub>(i, j) = ‚ü®DinoV2(x<sup>(i)</sup>), DinoV2(x<sup>(j)</sup>)‚ü©
                    </code>
                </li>
            </ul>
            <p>
                The kernel alignment metric compares the similarity between aligned multimodal kernels and the
                performant model kernel:
            </p>
            <code>
                m(K<sub>aligned</sub>, K<sub>DinoV2</sub>) > avg( m(K<sub>X</sub>, K<sub>DinoV2</sub>), m(K<sub>Y</sub>, K<sub>DinoV2</sub>) )
            </code>

            <h4>Key Hypothesis</h4>
            <p>
                We hypothesize that the learned aligned representation kernel achieves higher similarity with the
                performant model kernel than the unimodal kernels, demonstrating the success of multimodal alignment and
                the principles underlying representation convergence.
            </p>

            <h3>Data Pipeline</h3>
            <p>
                The data pipeline processes paired image and text data from the Flickr30k dataset, preparing it for
                input into our model.
                This dataset is ideal for aligning visual and textual modalities due to its paired image-caption
                structure.
                Key stages of the pipeline include:
            </p>
            <ul>
                <li> <strong>Image Processing:</strong> Images are resized to 224x224 pixels, normalized to match the
                    preprocessing of ResNet-18,
                    and converted to tensors for batch input. </li>
                <li> <strong>Text Processing:</strong> Captions are tokenized using a BERT tokenizer, padded for uniform
                    length,
                    and converted to tensors. </li>
                <li> <strong>Data Loading:</strong> Configured with a batch size of 64, the data loader shuffles,
                    batches, and optimizes data for
                    efficient GPU utilization. </li>
            </ul>

            <figure>
                <img src="figures/data_pipeline.png" alt="Data Pipeline">
                <figcaption>
                    Illustration of the data pipeline: image and text processing leading to batched inputs for encoders.
                </figcaption>
            </figure>

            <h3>Model Architecture</h3>
            <p>
                Our model leverages frozen pre-trained encoders for each modality, integrated with lightweight trainable
                adapters to map features into a shared multimodal space.
            </p>
            <ul>
                <li>
                    <strong>Pre-trained Encoders:</strong> ResNet-18 extracts 512-dimensional image features, while
                    DistilBERT generates 768-dimensional text embeddings.
                </li>
                <li>
                    <strong>Linear Adapters:</strong> Trainable linear projections align features into a shared latent
                    space of 768 dimensions. Adapter variations include:
                    <ul>
                        <li>Linear adapters (e.g., 512 ‚Üí 768).</li>
                        <li>MLP adapters (e.g., 512 ‚Üí 1024 ‚Üí 768).</li>
                    </ul>
                </li>
            </ul>
            <figure>
                <img src="figures/model_architecture.png" alt="Model Architecture">
                <figcaption>Diagram of the model architecture: frozen encoders, adapters, and shared multimodal latent
                    space.</figcaption>
            </figure>

            <h3>Training Procedure</h3>
            <p>
                The alignment process uses contrastive learning to minimize the distance between embeddings of matching
                image-caption pairs while maximizing it for mismatched pairs.
            </p>
            <ol>
                <li>
                    <strong>Contrastive Loss:</strong> A temperature-scaled similarity metric determines the loss,
                    ensuring meaningful alignment across modalities:
                    <br>
                    <code>
                    L = - (1/N) Œ£ log(exp(similarity(x, y) / œÑ) / Œ£ exp(similarity(x, y')))
                    </code>
                </li>
                <li>
                    <strong>Training Loop:</strong> The pipeline processes batches through the encoders and adapters,
                    computes gradients, and updates adapter parameters.
                </li>
                <li>
                    <strong>Evaluation:</strong> Kernel alignment metrics measure the similarity between the learned
                    multimodal representations and DinoV2 embeddings.
                </li>
            </ol>
            <figure>
                <img src="figures/training_loss_metrics.png" alt="Training Metrics">
                <figcaption>Placeholder: Training & Validation loss and alignment metric (mutual kNN) over epochs.
                </figcaption>
            </figure>

            <h3>Evaluation Metrics</h3>
            <ul>
                <li>
                    <strong>Unimodal Kernel Similarity:</strong> Measures the initial similarity of image and text
                    kernels to DinoV2.
                </li>
                <li>
                    <strong>Aligned Kernel Similarity:</strong> Evaluates the final alignment of multimodal kernels with
                    DinoV2 after training.
                </li>
            </ul>
            <figure>
                <img src="figures/kernel_heatmaps.png" alt="Kernel Similarity Heatmaps">
                <figcaption>Heatmaps comparing the similarity of multimodal kernels with DinoV2 embeddings.</figcaption>
            </figure>

            <h3>Implementation Details</h3>
            <ul>
                <li><strong>Frameworks:</strong> PyTorch for modeling, Hugging Face Transformers for text encoding, and
                    Weights & Biases for tracking experiments.</li>
                <li><strong>Hyperparameters:</strong>
                    <ul>
                        <li>Batch size: 64</li>
                        <li>Learning rate: 1e-4</li>
                        <li>Temperature parameter: œÑ = 0.07</li>
                        <li>Epochs: 50</li>
                    </ul>
                </li>
            </ul>
        </section>


        <section id="initial-results">
            <h2>Initial Results</h2>
            <p>
                Our experiments with image and text modalities have yielded promising results. By aligning
                representations from ResNet-18 and DistilBERT, we observed:
            </p>
            <ul>
                <li><strong>Alignment Success:</strong> Aligned representations demonstrated improved similarity to
                    DinoV2 embeddings.</li>
                <li><strong>Cross-Modality Insights:</strong> Text embeddings, which DinoV2 was not trained to process,
                    gained representational properties closer to its image embeddings.</li>
                <li><strong>Efficiency:</strong> Models trained on smaller datasets converged faster and achieved higher
                    alignment scores.</li>
            </ul>

            <h3>Metrics</h3>
            <p>
                Key kernel alignment metrics include:
            </p>
            <ul>
                <li>Text-to-DinoV2 similarity: Low, due to modality mismatch.</li>
                <li>Image-to-DinoV2 similarity: High, reflecting shared training data.</li>
                <li>Aligned multimodal similarity: Intermediate, surpassing the average of unimodal similarities.</li>
            </ul>
            <div class="figures-container">
                <figure>
                    <img src="figures/mlp/2d/m2d.gif" alt="2D MLP Adapter">
                    <figcaption>MLP Adapter</figcaption>
                </figure>
                <figure>
                    <img src="figures/mlp/3d/m3d.gif" alt="3D MLP Adapter">
                    <figcaption>MLP Adapter</figcaption>
                </figure>
            </div>
            <div class="figures-container">
                <figure>
                    <img src="figures/linear/2d/l2d.gif" alt="2D Linear Adapter">
                    <figcaption>Linear Adapter</figcaption>
                </figure>
                <figure>
                    <img src="figures/linear/3d/l3d.gif" alt="3D Linear Adapter">
                    <figcaption>Linear Adapter</figcaption>
                </figure>
            </div>
            <figure>
                <img src="figures/kernel_alignment_epochs.png" alt="Kernel Alignment Over Steps">
                <figcaption>Placeholder: Line plot showing kernel alignment for text, image, and multimodal embeddings
                    over training steps.</figcaption>
            </figure>
        </section>

        <section id="proposed-extensions">
            <h2>Proposed Extensions</h2>
            <h3>1. Downstream Task Evaluation</h3>
            <p>
                Evaluate aligned embeddings on downstream tasks, such as classification, using pre-trained heads.
                Hypotheses include:
            </p>
            <ul>
                <li>Slight performance degradation compared to DinoV2.</li>
                <li>Aligned multimodal embeddings outperform unimodal embeddings.</li>
            </ul>

            <h3>2. Adding a Third Modality</h3>
            <p>
                Expand the framework to include audio representations from models like Wav2Vec2. Test whether
                integrating a third modality enhances alignment and demonstrates additive representation benefits.
            </p>

            <figure>
                <img src="figures/multimodal-alignment.svg" alt="Multimodal Alignment Process">
                <figcaption>Placeholder: Diagram illustrating multimodal alignment across three modalities (image, text,
                    and audio).</figcaption>
            </figure>
        </section>

        <section id="future-directions">
            <h2>Future Directions</h2>
            <ul>
                <li>Scale experiments to richer multimodal datasets and benchmarks.</li>
                <li>Investigate cross-architecture generalization of aligned embeddings.</li>
                <li>Analyze mechanistic insights into alignment-driven representation convergence.</li>
            </ul>
        </section>

        <section id="conclusion">
            <h2>Conclusion</h2>
            <p>
                This project illustrates a scalable and efficient approach to aligning unimodal encoders into a shared
                multimodal space. By leveraging pre-trained models and lightweight adapters, we unlock potential
                applications in resource-efficient AI and multimodal representation learning. Our findings highlight the
                promise of this methodology and pave the way for further exploration into the convergence of
                representations across modalities.
            </p>
        </section>
    </main>

    <footer>
        <p>Multimodal Alignment Analysis | Blog | November 2024</p>
    </footer>
</body>

<style>
    figure {
        display: flex;
        /* Arrange images side by side */
        flex-direction: column;
        /* Ensure caption appears below images */
        align-items: center;
        /* Center the content */
        gap: 10px;
        /* Add space between images and caption */
    }

    .figures-container {
        display: flex;
        /* Arrange figures side by side */
        justify-content: center;
        /* Center the figures on the page */
        gap: 20px;
        /* Add spacing between the figures */
    }

    figure img {
        width: 400px;
        /* Adjust image width */
        height: auto;
        /* Maintain aspect ratio */
    }

    figcaption {
        text-align: center;
        /* Center-align the caption text */
        font-size: 14px;
        /* Adjust caption font size */
        color: #555;
        /* Add a softer color for the caption */
    }
</style>

</html>