<!DOCTYPE html>
<html lang="en">

<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <script type="text/javascript" async
        src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/3.2.2/es5/tex-mml-chtml.js"></script>
    <title>Aligning Modalities: Efficient Multimodal Representation Learning</title>
</head>

<body>
    <header>
        <h1>Towards the Platonic Representation via Multimodal Contrastive Alignment</h1>
    </header>

    <main>


        <section id="introduction">
            <h2>Introduction</h2>
            <p>
                Deep learning models are typically trained to transform raw data into representations optimized for
                specific tasks. Recently, two lines of research have inspired a deeper inquiry into the nature of these
                representations. The <strong>CLIP framework</strong> demonstrated the utility of aligning
                representations across
                modalities, using paired image-text data to train joint embeddings for cross-modal retrieval. Meanwhile,
                the <strong>Platonic Representation Hypothesis</strong> posits that performant models converge toward a
                shared
                statistical model of reality in their representation spaces, suggesting a potential universality
                underlying learned representations.
            </p>
            <p>
                This project bridges these ideas by exploring whether representations from disparate pre-trained
                unimodal neural networks can be aligned into a shared multimodal latent space, inspired by the joint
                embedding approach of CLIP and motivated by the convergence hypothesis of Platonic Representations. The
                proposed framework uses frozen unimodal encoders with learned linear adapters to align representations
                across modalities. Our aim is to determine if such aligned representations better approximate those of
                larger, more performant models.
            </p>
            <p>
                Inspired by the success of CLIP in aligning representations across modalities and the theoretical
                insights of the Platonic Representation Hypothesis, we propose a framework that aligns pre-trained
                unimodal encoders into a shared multimodal latent space using simple linear adapters. By leveraging
                frozen encoders such as ResNet-18 for images and DistilBERT for text, we aim to achieve alignment
                without retraining a larger state-of-the-art model like DINOv2, enabling scalability to new modalities
                with minimal overhead.
            </p>
            <figure>
                <img src="figures/conceptual_contrastive.jpg" alt="Conceptual Multimodal Alignment"
                    style="width: 100%; max-width: 1200px; margin: 0 auto;">
                <figcaption>
                    <strong>Figure 1.</strong> Conceptual illustration of our multimodal alignment framework.
                    Representations from ResNet-18 (images) and DistilBERT (text) on the image-caption samples from the
                    Flickr30k dataset are aligned into a shared latent space using lightweight adapters. Matching
                    image-caption pairs are
                    pulled together, while non-matching pairs are pushed
                    apart to create a unified multimodal embedding.
                </figcaption>
            </figure>
        </section>

        <section id="motivation">
            <h3>Motivation</h3>
            <p>
                Our approach is motivated by three key insights:
            </p>
            <ol>
                <li>
                    <strong>Inspiration from CLIP:</strong> CLIP demonstrated that cross-modal representations can be
                    aligned using paired data and contrastive learning. However, joint training limits scalability to
                    new modalities. Our method decouples the encoders, aligning their outputs via lightweight adapters.
                </li>
                <li>
                    <strong>Testing the Platonic Representation Hypothesis:</strong> This hypothesis suggests that
                    models converge toward a shared statistical model of reality. Aligning diverse unimodal encoders
                    tests whether this convergence can be explicitly achieved.
                </li>
                <li>
                    <strong>Scalability and Modularity:</strong> By aligning frozen encoders post hoc, our framework
                    supports efficient integration of new modalities without the need for extensive joint training.
                </li>
            </ol>
        </section>

        <section id="related-work">
            <h2>Related Work</h2>
            <p>
                Our project builds on several important advances in multimodal representation learning and theoretical
                insights into model convergence:
            </p>
            <ol>
                <li>
                    <strong><a href="https://phillipi.github.io/prh/">Platonic Representation Hypothesis
                            (PRH)</a>:</strong> <a href="https://arxiv.org/abs/2405.07987">Huh et al. (2024)</a>
                    proposed that
                    performant models converge toward a shared statistical model of reality in their representation
                    spaces,
                    regardless of their training modality. This hypothesis underpins our framework, which aims to
                    explicitly align unimodal encoders to test this convergence hypothesis.
                </li>
                <li>
                    <strong><a href="https://openai.com/index/clip/">CLIP</a>:</strong> <a
                        href="https://arxiv.org/abs/2103.00020">Radford et al. (2021) </a>introduced CLIP, a model that
                    learns joint multimodal
                    representations using contrastive learning on paired image-text datasets. CLIP's success
                    demonstrates the power of cross-modal embeddings but requires joint training of encoders, which our
                    framework aims to circumvent.
                </li>
                <li>
                    <strong>Linear Mapping from Image to Text Space:</strong> <a
                        href="https://arxiv.org/abs/2209.15162">Merullo et al. (2022)</a> showed that
                    simple linear transformations can align visual representations to text spaces, enabling cross-modal
                    tasks
                    like visual question answering. Their findings inspired our use of linear and lightweight MLP
                    adapters for multimodal alignment.
                </li>
                <li>
                    <strong>DINOv2:</strong> <a href="https://arxiv.org/abs/2304.07193">Oquab et al. (2023)</a>
                    introduced DINOv2, a self-supervised vision transformer
                    model that generates robust embeddings. As our performant reference model, we use DINOv2-small to
                    provide a benchmark for evaluating the quality of our aligned multimodal representations.
                </li>
                <li>
                    <strong>Contrastive Representation Learning:</strong>
                    <a href="https://arxiv.org/abs/2005.10242">Wang and Isola (2020)</a> provided theoretical
                    insights into
                    contrastive learning by analyzing alignment and uniformity on the hypersphere. Their work shows that
                    contrastive
                    objectives promote both alignment of positive pairs and uniformity of representations across the
                    hypersphere.
                    This analysis informs our use of the <strong>dual-encoder contrastive loss</strong> to drive
                    convergence in the shared latent space.
                </li>
            </ol>
        </section>

        <section id="hypothesis">
            <h2>Hypothesis</h2>
            <p>
                Our work is grounded in the following hypotheses:
            </p>
            <ul>
                <li>
                    A shared latent space exists where unimodal representations from different encoders can be aligned
                    through linear transformations.
                </li>
                <li>
                    Aligning these representations produces embeddings that closely approximate those of performant
                    models, such as DINOv2, as measured by kernel alignment metrics.
                </li>
                <li>
                    Multimodal alignment captures mechanisms of representation convergence, providing empirical evidence
                    for the Platonic Representation Hypothesis.
                </li>
            </ul>
        </section>

        <section id="mathematical-framework">
            <h2>Mathematical Framework</h2>
            <p>
                At the core of our project is the hypothesis that pre-trained unimodal representations can be aligned
                into a shared multimodal latent space. This section formalizes the mathematical structure underlying our
                framework, detailing how representations are extracted, aligned, and evaluated.
            </p>

            <h3>Multimodal Data Representation</h3>
            <p>
                Let the world generate raw multimodal data:
            </p>
            <p>
                \[
                \mathcal{D}_\text{world} = \left\{\left(\theta^{(i)}, \psi^{(i)}, \phi^{(i)},
                \dots\right)\right\}_{i=1}^N, \quad \theta \in \mathcal{\Theta}, \psi \in \mathcal{\Psi}, \phi \in
                \mathcal{\Phi}, \dots
                \]
            </p>
            <p>
                where \( \mathcal{\Theta}, \mathcal{\Psi}, \mathcal{\Phi} \) represent different modalities (e.g.,
                image, text, audio), and \( \theta, \psi, \phi \) are specific instances of these modalities.
            </p>
            <p>
                For simplicity, we focus on a two-modality setting with images (\( \mathcal{X} \)) and text (\(
                \mathcal{Y} \)):
            </p>
            <p>
                \[
                \mathcal{D}_\text{world} = \left\{\left(x^{(i)}, y^{(i)}\right)\right\}_{i=1}^N, \quad x \in
                \mathcal{X}, y \in \mathcal{Y}.
                \]
            </p>
            <figure>
                <img src="figures/platonic_representation.svg" alt="Platonic Representation Hypothesis"
                    style="width: 80%; max-width: 1200px; margin: 0 auto;">
                <figcaption>
                    <strong>Figure 2.</strong> Conceptual illustration of the connection between the Platonic
                    Representation Hypothesis (PRH) and our project framework. Representations from different modalities
                    (e.g., images, text) are hypothesized to converge toward a shared statistical model — the Platonic
                    representation. Our framework uses contrastive learning to
                    explicitly align these representations into a shared latent space, approximating this convergence.
                </figcaption>
            </figure>

            <h3>Learned Adapters</h3>
            <p>
                We use frozen pre-trained encoders to extract representations:
            </p>
            <p>
                \[
                X_\text{enc}: \mathcal{X} \rightarrow \mathbb{R}^{d_x}, \quad Y_\text{enc}: \mathcal{Y} \rightarrow
                \mathbb{R}^{d_y},
                \]
            </p>
            <p>
                where \( d_x \) and \( d_y \) are the embedding dimensions for images and text, respectively. We employ:
            </p>
            <ul>
                <li><strong>ResNet-18</strong> trained on ImageNet1K for images (\( d_x = 512 \)).</li>
                <li><strong>DistilBERT</strong> trained on BookCorpus for text (\( d_y = 768 \)).</li>
            </ul>
            <p>
                To project these into a shared latent space \( \mathbb{R}^{d_e} \), we introduce adapters:
            </p>
            <p>
                \[
                W_x : \mathbb{R}^{d_x} \rightarrow \mathbb{R}^{d_e}, \quad W_y : \mathbb{R}^{d_y} \rightarrow
                \mathbb{R}^{d_e}.
                \]
            </p>
            <p>
                The adapted encoders are defined as:
            </p>
            <p>
                \[
                f_\text{image}(x) = W_x \circ X_\text{enc}(x), \quad g_\text{text}(y) = W_y \circ Y_\text{enc}(y),
                \]
            </p>
            <p>
                where \( f_\text{image} \) and \( g_\text{text} \) map images and text to the shared latent space \(
                \mathbb{R}^{d_e} \).
            </p>

            <h3>Dual-Encoder Contrastive Objective</h3>
            <p>
                To align the representations, we use a <strong>dual-encoder contrastive loss</strong>, inspired by the
                coursework (see Homework 4). This loss encourages positive pairs to be similar while pushing apart
                negative pairs:
            </p>
            <p>
                \[
                \mathcal{L}_{\text{contrastive}} = -\frac{1}{N} \sum_{i=1}^N \log
                \frac{\exp\left(f_\text{image}(x^{(i)}) \cdot g_\text{text}(y^{(i)}) / \tau\right)}{\sum_{j=1}^N
                \exp\left(f_\text{image}(x^{(i)}) \cdot g_\text{text}(y^{(j)}) / \tau\right)},
                \]
            </p>
            <p>
                where \( \tau \) is the temperature parameter that controls the sharpness of the similarity scores.
            </p>

            <details>
                <summary>Homework 4 Connection</summary>
                <figure>
                    <img src="figures/homework4_contrastive.jpg" alt="Dual-Encoder Contrastive Learning"
                        style="width: 80%; height: auto;">
                    <figcaption>Dual-Encoder contrastive learning formulation from Homework 4.</figcaption>
                </figure>
            </details>

            <p>
                Typically, contrastive learning applies to different "views" generated through transformations
                or augmentations of samples from a <em>single modality</em>. In our approach, we generalize this concept
                by treating <em>different modalities</em> (image and text) as distinct views of a
                <strong>hypothesized common underlying reality</strong>, inspired by the Platonic Representation
                Hypothesis.
            </p>

            <h3>Connection to the Platonic Representation Hypothesis</h3>
            <p>
                The <strong>Platonic Representation Hypothesis (PRH)</strong> posits that representations from different
                modalities converge toward a shared Platonic representation. In our framework, we hypothesize that
                <strong>contrastive alignment</strong> between the unimodal image and text encoders drives this
                convergence.
            </p>
            <p>
                Our aligned multimodal encoder is defined as:
            </p>
            <p>
                \[
                h_\text{multi}(x, y) = \lambda \cdot f_{\text{image}}(x) + (1 - \lambda) \cdot g_{\text{text}}(y),
                \]
            </p>
            <p>
                where \( \lambda \) is a learnable parameter controlling the balance between the two modalities.
            </p>

            <h3>Performant Model Benchmark</h3>
            <p>
                To evaluate the quality of our aligned multimodal representations, we use the
                <strong>DINOv2-small</strong> encoder as a benchmark. Specifically, we define the function implemented
                by the DINOv2-small feature extractor as:
            </p>
            <p>
                \[
                \text{DINOv2}: \mathcal{X} \rightarrow \mathbb{R}^{d_e}.
                \]
            </p>
            <p>
                The DINOv2 encoder serves as a performant reference model, providing high-quality image representations.
                By comparing our aligned multimodal representations to the DINOv2 representations, we measure how
                effectively our contrastive alignment approximates this benchmark.
            </p>

            <h3>Kernel Alignment Metric</h3>
            <p>
                We characterize representations in terms of their <strong>kernels</strong>, which capture the similarity
                between data points. Two representations are considered <strong>aligned</strong> if their kernels are
                similar for corresponding inputs.
                Given the kernels:
            </p>
            <ul>
                <li>\( K_X(i, j) = \langle f_{\text{image}}(x^{(i)}), f_{\text{image}}(x^{(j)}) \rangle \)</li>
                <li>\( K_Y(i, j) = \langle g_{\text{text}}(y^{(i)}), g_{\text{text}}(y^{(j)}) \rangle \)</li>
                <li>\( K_{\text{align}}(i, j) = \langle h_\text{multi}(x^{(i)}, y^{(i)}), h_\text{multi}(x^{(j)},
                    y^{(j)}) \rangle \)</li>
                <li>\( K_{\text{DINOv2}}(i, j) = \langle \text{DINOv2}(x^{(i)}), \text{DINOv2}(x^{(j)}) \rangle \)</li>
            </ul>

            <p>
                we evaluate alignment performance using the <strong>mutual-KNN kernel alignment metric</strong> \( m \).
            </p>

            <p>
                Specifically, we compute the following values:
            </p>
            <ul>
                <li><strong>Unimodal Alignment:</strong></li>
                <ul>
                    <li>\( m(K_X, K_{\text{DINOv2}}) \): Alignment of the image encoder \( f_\text{image} \) kernel with
                        the DINOv2 kernel.</li>
                    <li>\( m(K_Y, K_{\text{DINOv2}}) \): Alignment of the text encoder \( g_\text{text} \) kernel with
                        the DINOv2 kernel.</li>
                </ul>
                <li><strong>Multimodal Alignment:</strong></li>
                <ul>
                    <li>\( m(K_{\text{align}}, K_{\text{DINOv2}}) \): Alignment of the multimodal encoder \(
                        h_\text{multi} \) kernel with the DINOv2 kernel.</li>
                </ul>
            </ul>

            <p>
                We conjecture that the aligned multimodal kernel will have higher similarity with \( K_{\text{DINOv2}}
                \) than the unimodal kernels.
            </p>


        </section>


        <section id="methodology">
            <h2>Methodology</h2>

            <p>
                All code for this project is available in our
                <a href="https://colab.research.google.com/drive/1BkyPko0x-8CL41Z-VSmyxI41B90hB2Fc?usp=sharing">Google
                    Colab notebook</a>.
            </p>

            <h3>Data Pipeline</h3>
            <p>
                We use the first 12,800 samples of the <strong><a
                        href="https://huggingface.co/datasets/nlphuji/flickr30k">Flickr30k</a></strong> dataset, which
                contains paired image-caption samples. Each image is paired with the first caption, forming a single
                positive pair \( (x^{(i)}, y^{(i)}) \).
            </p>

            <ul>
                <li><strong>Image Processing:</strong>
                    <ul>
                        <li>Resize to 224x224.</li>
                        <li>Normalize using ImageNet statistics.</li>
                        <li>Convert to tensors for ResNet-18.</li>
                    </ul>
                </li>
                <li><strong>Text Processing:</strong>
                    <ul>
                        <li>Tokenize with DistilBERT tokenizer.</li>
                        <li>Pad/truncate to 64 tokens.</li>
                        <li>Convert to tensors for DistilBERT.</li>
                    </ul>
                </li>
                <li><strong>Data Loading:</strong>
                    <ul>
                        <li>Batch size: 256.</li>
                        <li>Train-validation split: 75%-25%.</li>
                    </ul>
                </li>
            </ul>

            <details>
                <summary><em>Expand for schematic of data pipeline.</em></summary>
                <img src="figures/data_pipeline.svg" alt="Data Pipeline" style="width: 100%; max-width: 800px;">
            </details>

            <h3>Model Architecture</h3>
            <p>
                We use frozen pre-trained encoders with trainable adapters to align representations in a shared
                multimodal space:
            </p>
            <ul>
                <li><strong>Frozen Encoders:</strong>
                    <ul>
                        <li>Image Encoder: ResNet-18 (\( d_x = 512 \)).</li>
                        <li>Text Encoder: DistilBERT (\( d_y = 768 \)).</li>
                    </ul>
                </li>
                <li><strong>Adapters:</strong>
                    <ul>
                        <li>Linear Adapter: Matrix multiplication by \( W_x \) or \( W_y \) plus bias term.</li>
                        <li>MLP Adapter: 2-layer MLP with GELU and LayerNorm.</li>
                    </ul>
                </li>
                <li><strong>Reference Model:</strong> DINOv2-small (\( d_e = 768 \)).</li>
            </ul>

            <h3>Training Procedure</h3>
            <p>
                We train the adapters using the dual-encoder contrastive loss for <strong>100 epochs</strong> on
                Flickr30k with a temperature parameter \( \tau = 0.02 \) in the contrastive loss. Other relevant
                hyperparameter settings:
            </p>

            <ul>
                <li><strong>Optimizer:</strong> AdamW</li>
                <li><strong>Learning Rate:</strong> \(1 \times 10^{-4}\) (cosine decay with warmup).</li>
                <li><strong>Gradient Clipping:</strong> 0.5.</li>
                <li><strong>Weight Decay:</strong> 0.001 (linear adapter) and 0.005 (MLP adapter).</li>
                <li><strong>Label Smoothing:</strong> 0.05.</li>
                <li><strong>Dropout:</strong> 0.1.</li>
            </ul>
            <details>
                <summary><em>Expand for schematic of training pipeline.</em></summary>
                <img src="figures/training_pipeline.svg" alt="Training Pipeline">
            </details>

            <h3>Evaluation Metrics</h3>

            <p>
                We will test <strong>two key hypotheses</strong> based on the mutual-kNN values we
                described in the Mathematical Framework, before and after training:
            </p>

            <ul>
                <li><strong>Weak Hypothesis (\( H_1 \)):</strong> The aligned multimodal kernel achieves higher
                    similarity with the DINOv2 kernel than the average of the unimodal kernels:
                    \[
                    m(K_{\text{align}}, K_{\text{DINOv2}}) > \text{avg}(m(K_X, K_{\text{DINOv2}}), m(K_Y,
                    K_{\text{DINOv2}})).
                    \]
                </li>
                <li><strong>Strong Hypothesis (\( H_2 \)):</strong> The aligned multimodal kernel surpasses even the
                    best unimodal kernel:
                    \[
                    m(K_{\text{align}}, K_{\text{DINOv2}}) > \max(m(K_X, K_{\text{DINOv2}}), m(K_Y, K_{\text{DINOv2}})).
                    \]
                </li>
            </ul>
            <details>
                <summary><em>Expand for schematic of hypothesis testing.</em></summary>
                <img src="figures/hypothesis_test.svg" alt="Hypothesis Testing" style="width: 100%; max-width: 800px;">
            </details>
        </section>

        <section id="results">
            <h2>Results</h2>

            <h3>Kernel Alignment Improves with Training</h3>
            <p>
                Our first key finding is that the <strong>mutual-KNN kernel alignment metric</strong> improves after
                training for both the linear and MLP adapters. This demonstrates that our contrastive alignment
                objective effectively aligns the image and text representations.
            </p>

            <figure>
                <img src="figures/kernel_metric_barplot.jpg" alt="Kernel Metric Bar Plot"
                    style="width: 100%; max-width: 1200px; margin: 0 auto;">
                <figcaption>
                    <strong>Figure 3.</strong> Bar plot showing the mutual-KNN kernel alignment metric before and after
                    training for the (A) linear and (B) 2-layer MLP adapters. For both adapter types and all encoders,
                    kernel alignment to the DINOv2-small encoder improves after training with the dual-encoder
                    contrastive objective.
                </figcaption>
            </figure>

            <p>
                We observe that prior to training, the alignment scores are relatively low, particularly for the text
                encoder. After training, there is a significant increase in alignment for both the image and text
                encoders. The multimodal alignment scores exceed the individual unimodal scores, suggesting that the
                adapters successfully bridge the gap between the modalities.
            </p>

            <h3>Visualization of Alignment</h3>
            <p>
                To further illustrate the alignment process, we apply dimensionality reduction to visualize the
                embeddings produced by
                the DINOv2-small encoder and our aligned multimodal encoder. To explain using principal component
                analysis (PCA), we compute the principal components (PCs) of the DINOv2-small embeddings and project the
                multimodal embeddings onto the
                same PCs. Multidimensional scaling (MDS) is a similar technique but nonlinear.
            </p>

            <p>
                The visualization shows that during training, the aligned multimodal embeddings progressively move
                closer to the DINOv2 embeddings, supporting the hypothesis that our contrastive objective aligns the
                representations effectively.
            </p>

            <figure>
                <img src="figures/pca_alignment.jpg" alt="MDS Alignment Visualization"
                    style="width: 100%; max-width: 1200px; margin: 0 auto;">
                <figcaption>
                    <strong>Figure 4.</strong> MDS plots of the embeddings from the DINOv2-small encoder (blue) and our
                    aligned multimodal encoder (yellow) at the first epoch (left) and last epoch (right) of training,
                    for both types of adapters: (A) linear and (B) MLP adapters. The multimodal representations become
                    progressively more aligned with the DINOv2 embeddings during training.
                </figcaption>
            </figure>

            <h3>Key Hypotheses Supported</h3>
            <p>
                Our results validate both the weak hypothesis (\( H_1 \)) and the strong hypothesis (\( H_2 \)).
                Specifically:
            </p>

            <ul>
                <li><strong>Weak Hypothesis (\( H_1 \)):</strong> The aligned multimodal kernel achieves higher
                    similarity with the DINOv2 kernel than the average of the unimodal kernels.</li>
                <li><strong>Strong Hypothesis (\( H_2 \)):</strong> The aligned multimodal kernel surpasses even the
                    best unimodal kernel.</li>
            </ul>

            <p>
                This indicates that the multimodal representations, which incorporate both image and text information,
                are more closely aligned with the DINOv2 embeddings than either the image or text representations alone.
            </p>

            <figure>
                <img src="figures/key_hypothesis_results.jpg" alt="Key Hypothesis Results"
                    style="width: 100%; max-width: 1200px; margin: 0 auto;">
                <figcaption>
                    <strong>Figure 5.</strong> The mutual-KNN kernel alignment metric for our aligned multimodal encoder
                    surpasses both the (left) average of and the (right) better of the two unimodal encoders, supporting
                    both the weak and strong hypotheses. Results are shown for both (A) linear adapters and (B) 2-layer
                    MLP adapters.
                </figcaption>
            </figure>

            <p>
                Notably, the multimodal alignment scores consistently exceed the individual image and text alignment
                scores, indicating an emergent property where the whole (multimodal representation) is greater than the
                sum of its parts (unimodal representations). This finding is particularly compelling given that the
                DINOv2 model is a vision-only model trained without any text supervision.
            </p>

            <p>
                Our results suggest that incorporating multimodal information enhances the quality of the learned
                representations, supporting the <strong>Platonic Representation Hypothesis</strong>. This hypothesis
                posits a shared statistical model of reality underlying different modalities. The fact that multimodal
                alignment produces representations closer to DINOv2 — a high-performing vision model — reinforces the
                idea that such a shared model exists.
            </p>

            <p>
                In summary, our findings demonstrate that aligning image and text representations using simple linear or
                MLP adapters can produce embeddings that not only match but exceed the performance of unimodal
                representations when compared to a performant model like DINOv2.
            </p>

            <details>
                <summary>
                    <strong>Bonus Experiment</strong>
                </summary>
                <h3>Downstream Classification Performance</h3>
                <p>
                    To evaluate the quality of our aligned representations in a downstream task setting, we conducted
                    experiments on the
                    CIFAR-10 image classification dataset. Our evaluation pipeline involved training linear classifiers
                    on frozen features
                    extracted from three models: DINOv2-small (our performant baseline), the unimodal ResNet-18 image
                    encoder, and our
                    aligned multimodal encoder.
                </p>

                <p>
                    The classifiers were intentionally kept simple to evaluate the quality of the learned
                    representations rather than
                    the classification architecture. Each classifier consisted of a two-layer MLP with architecture:
                </p>

                <p>
                    \[
                    \text{Linear}(d_e \rightarrow 512) \rightarrow \text{ReLU} \rightarrow \text{Dropout}(0.1)
                    \rightarrow
                    \text{Linear}(512 \rightarrow 10)
                    \]
                </p>

                <p>
                    where \( d_e = 768 \) is the embedding dimension. The classifiers were trained for one epoch using
                    the AdamW optimizer
                    with a learning rate of \( 1 \times 10^{-3} \).
                </p>

                <h4>Results on CIFAR-10</h4>

                <p>
                    Our experiments with the linear adapter yielded the following results:
                </p>
                <ul>
                    <li>DINOv2-small (baseline): 98.05% accuracy</li>
                    <li>ResNet-18 image encoder: 77.84% accuracy (79.4% relative to DINOv2)</li>
                    <li><strong>Aligned multimodal encoder: 75.69% accuracy (77.2% relative to DINOv2)</strong></li>
                </ul>

                <p>
                    For the MLP adapter:
                </p>
                <ul>
                    <li>DINOv2-small (baseline): 97.95% accuracy</li>
                    <li>ResNet-18 image encoder: 74.91% accuracy (76.5% relative to DINOv2)</li>
                    <li><strong>Aligned multimodal encoder: 72.21% accuracy (73.7% relative to DINOv2)</strong></li>
                </ul>

                <p>
                    The linear adapter demonstrated marginally better performance, with both adapters showing similar
                    patterns in their
                    relative performance compared to DINOv2. Notably, while the aligned representations perform slightly
                    below the
                    unimodal image encoder (by approximately 2-3% absolute), they maintain strong relative performance
                    compared to
                    DINOv2 (>73% relative accuracy for both adapters).
                </p>

                <p>
                    This minor performance gap is expected and can be interpreted positively: our aligned
                    representations successfully
                    maintain most of the task-relevant visual information while incorporating additional modalities,
                    achieving 77.2%
                    and 73.7% of DINOv2's performance for linear and MLP adapters respectively. This suggests that the
                    alignment
                    process preserves core visual features while potentially enriching the representation space with
                    complementary
                    information from the text modality.
                </p>

                <p>
                    These results align with our kernel analysis findings, demonstrating that our aligned
                    representations effectively
                    capture meaningful visual features despite not being explicitly optimized for image classification.
                    The fact that
                    we can achieve >73% relative performance compared to a state-of-the-art vision model while
                    incorporating text
                    modality suggests that our alignment approach successfully balances modality-specific and shared
                    information in
                    the representation space.
                </p>

                <img src="figures/downstream_diagram.svg" alt="Downstream Classification Performance">
            </details>
        </section>

        <section id="discussion">
            <h2>Discussion</h2>

            <p>
                In this work, we investigated whether representations from pre-trained unimodal encoders can be aligned
                into a shared multimodal latent space using lightweight adapters. Our results support the
                <strong>Platonic Representation Hypothesis (PRH)</strong> proposed by Huh et al. (2024). Specifically,
                we observed
                that aligning image and text embeddings via contrastive learning produces multimodal representations
                more closely aligned with DINOv2-small features compared to unimodal representations alone.
            </p>

            <p>
                Inspired by the <strong>CLIP framework</strong> (Radford et al., 2021), which aligns cross-modal
                representations
                through joint training, our approach achieves alignment post hoc using frozen encoders and lightweight
                adapters. This offers a scalable alternative to joint training for multimodal learning.
            </p>

            <h3>Key Insights</h3>

            <ul>
                <li>
                    <strong>Cross-Modal Alignment:</strong> Despite <strong>DINOv2-small</strong> being a
                    self-supervised vision model
                    trained without text supervision, aligning image and text representations improved their similarity
                    to
                    DINOv2 embeddings. This suggests a universal structure in representation spaces, reinforcing the
                    PRH's
                    concept of convergence toward a shared statistical model of reality.
                </li>
                <li>
                    <strong>Generalized Contrastive Views:</strong> Our dual-encoder contrastive objective extends the
                    traditional
                    notion of “views” in contrastive learning to distinct modalities, treating them as different
                    perspectives
                    of an underlying reality. This insight differentiates our work from conventional contrastive
                    learning.
                </li>
                <li>
                    <strong>Empirical Hypothesis Validation:</strong> Our findings validate both the
                    <strong>weak</strong> and
                    <strong>strong hypotheses</strong>, demonstrating that multimodal alignment yields representations
                    that
                    outperform unimodal representations when compared to DINOv2 embeddings.
                </li>
            </ul>

            <h3>Non-Trivial Alignment to DINOv2</h3>

            <p>
                Importantly, our multimodal encoder was not explicitly optimized to match DINOv2-small embeddings.
                The fact that the aligned encoder \( h_{\text{multi}}(x, y) \) produces representations closer to DINOv2
                compared to the image encoder alone indicates a genuine benefit of multimodal alignment, rather than
                mere replication of image features.
            </p>

            <h3>Potential Limitations</h3>

            <ul>
                <li>
                    <strong>Dataset Bias:</strong> The <strong>Flickr30k</strong> dataset primarily features images of
                    people in
                    everyday situations. This limited diversity may affect the generalizability of our findings.
                    Exploring
                    more diverse datasets could offer further insights.
                </li>
                <li>
                    <strong>Restricted Modalities:</strong> Our study focused on image and text alignment. Extending the
                    framework to additional modalities, such as audio or video, could provide a broader validation of
                    the alignment mechanism and PRH.
                </li>
            </ul>

            <h3>Conclusion</h3>

            <p>
                Our work demonstrates that multimodal alignment via contrastive learning is a promising
                approach for achieving efficient, scalable representation learning. The observed convergence toward
                DINOv2 embeddings supports the notion of a <strong>universal Platonic representation</strong>, where
                aligning multiple modalities reveals shared underlying structures in representation spaces. This result
                builds upon insights from both the <strong>CLIP framework</strong> and the <strong>Platonic
                    Representation Hypothesis</strong>, underscoring the potential of multimodal alignment in advancing
                our understanding of representation learning.
            </p>
        </section>

        <section id="future-directions">
            <h2>Future Directions</h2>

            <h3>1. Leveraging Multiple Captions for Alignment</h3>
            <p>
                In this work, we used only the first caption from the Flickr30k dataset per image. To improve alignment,
                we plan to use all five captions per image. Each data point \( i \) will be represented as
                \( (x^{(i)}, y_1^{(i)}, \dots, y_5^{(i)}) \), where \( y_k^{(i)} \) is the \( k \)-th caption. The
                contrastive loss becomes:
            </p>
            <p>
                \[
                \mathcal{L}_{\text{multi-caption}} = -\frac{1}{N} \sum_{i=1}^N \log
                \frac{\exp\left(\frac{1}{5\tau} \sum_{k=1}^5 f_\text{image}(x^{(i)}) \cdot
                g_\text{text}(y_k^{(i)})\right)}
                {\sum_{j=1}^N \exp\left(\frac{1}{5\tau} \sum_{k=1}^5 f_\text{image}(x^{(i)}) \cdot
                g_\text{text}(y_k^{(j)})\right)}.
                \]
            </p>

            <h3>2. Scaling to Richer Multimodal Datasets</h3>
            <p>
                We aim to extend our experiments to include a third modality, such as audio. For example, using the
                <strong><a href="https://www.robots.ox.ac.uk/~vgg/data/vggsound/">VGGSound</a></strong> dataset and a
                pre-trained
                audio encoder like <strong><a
                        href="https://huggingface.co/docs/transformers/en/model_doc/wav2vec2">Wav2Vec</a></strong>,
                we can evaluate whether adding audio improves alignment compared to the image-text setting.
            </p>
            <p>
                We will also generalize the dual-encoder loss to a <strong>multi-encoder contrastive loss</strong> for
                \( M \) modalities:
            </p>
            <p>
                \[
                \mathcal{L}_{\text{multi-encoder}} = -\frac{1}{N} \sum_{i=1}^N
                \log \frac{\exp\left(\sum_k \prod_{m=1}^M (f_m^{(i)})_k / \tau\right)}
                {\sum_{j=1}^N \exp\left(\sum_k \prod_{m=1}^M (f_m^{(j)})_k / \tau\right)}.
                \]
            </p>

            <h3>3. Zero-Shot Evaluation with Pre-Trained Heads</h3>
            <p>
                We plan to evaluate the quality of our aligned representations using pre-trained classifier heads from
                the <a href="https://github.com/facebookresearch/dinov2?tab=readme-ov-file">DINOv2 GitHub
                    repository</a>.
                Specifically, we will replace the DINOv2-small encoder with our aligned multimodal encoder \(
                h_\text{multi} \)
                and test zero-shot classification performance on ImageNet.
            </p>
        </section>


    </main>

    <footer>
        <p>Quilee Simeon & Gabe Manso | MIT Deep Learning Final Project Blog | December 2024</p>
    </footer>

    <style>
        /* General Styling */
        body {
            font-family: Arial, sans-serif;
            line-height: 1.6;
            color: #333;
            margin: 0;
            padding: 0;
            background-color: #f9f9f9;
        }

        /* Header Styling */
        header {
            background: #005f73;
            color: white;
            padding: 20px 0;
            text-align: center;
        }

        header h1 {
            margin: 0;
            font-size: 2.5em;
        }

        /* Section Styling */
        main {
            padding: 20px;
            max-width: 800px;
            margin: 0 auto;
            background: white;
            border-radius: 8px;
            box-shadow: 0 2px 5px rgba(0, 0, 0, 0.1);
        }

        section {
            margin-bottom: 20px;
        }

        section h2 {
            border-bottom: 2px solid #005f73;
            padding-bottom: 5px;
            color: #005f73;
            font-size: 1.5em;
        }

        /* List Styling */
        ul {
            padding-left: 20px;
            list-style-type: disc;
        }

        ul li {
            margin-bottom: 10px;
        }

        /* Footer Styling */
        footer {
            text-align: center;
            padding: 10px 0;
            background: #005f73;
            color: white;
            font-size: 0.9em;
        }

        /* Code and Pre-formatted Text */
        code,
        pre {
            background: #e0f4f4;
            padding: 2px 5px;
            border-radius: 3px;
            font-family: "Courier New", Courier, monospace;
            font-size: 0.95em;
        }

        /* Image and Visualization Placeholder Styling */
        img {
            max-width: 100%;
            height: auto;
            margin: 10px 0;
        }

        p em {
            font-style: italic;
            color: #666;
        }

        figure {
            display: flex;
            flex-direction: column;
            align-items: center;
            gap: 10px;
        }

        .figures-container {
            display: flex;
            justify-content: center;
            gap: 20px;
        }

        figure img {
            width: 400px;
            height: auto;
        }

        figcaption {
            text-align: center;
            font-size: 14px;
            color: #555;
        }
    </style>
</body>

</html>