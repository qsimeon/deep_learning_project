<!DOCTYPE html>
<html lang="en">

<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <script type="text/javascript" async
        src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/3.2.2/es5/tex-mml-chtml.js"></script>
    <title>Aligning Modalities: Efficient Multimodal Representation Learning</title>
    <link rel="stylesheet" href="styles/style.css">
</head>

<body>
    <header>
        <h1>Aligning Modalities: Efficient Multimodal Representation Learning</h1>
    </header>

    <main>
        <section id="introduction">
            <h2>Introduction</h2>
            <p>
                Deep learning models are typically trained to transform raw data into representations optimized for
                specific tasks. Recently, two lines of research have inspired a deeper inquiry into the nature of these
                representations. The CLIP framework demonstrated the utility of aligning representations across
                modalities, using paired image-text data to train joint embeddings for cross-modal retrieval. Meanwhile,
                the Platonic Representation Hypothesis posits that performant models converge toward a shared
                statistical model of reality in their representation spaces, suggesting a potential universality
                underlying learned representations.
            </p>
            <p>
                This project bridges these ideas by exploring whether representations from disparate pre-trained
                unimodal neural networks can be aligned into a shared multimodal latent space, inspired by the joint
                embedding approach of CLIP and motivated by the convergence hypothesis of Platonic Representations. The
                proposed framework uses frozen unimodal encoders (e.g., ResNet-18 for images, GPT-2 for text) with
                learned linear adapters to align representations across modalities. Our aim is to determine if such
                aligned representations better approximate those of larger, more performant models (e.g., DinoV2).
            </p>
            <p>
                Inspired by the success of CLIP in aligning representations across modalities and the theoretical
                insights of the Platonic Representation Hypothesis, we propose a framework that aligns pre-trained
                unimodal encoders into a shared multimodal latent space using simple linear adapters. By leveraging
                frozen encoders such as ResNet-18 for images and GPT-2 for text, we aim to achieve alignment without
                retraining these large models, enabling scalability to new modalities with minimal overhead.
            </p>
            <figure>
                <img src="figures/img_txt_concept.svg" alt="Multimodal Alignment Process">
                <figcaption><strong>Figure 1.</strong> Diagram illustrating multimodal alignment across 2 modalities
                    (image and text).
                </figcaption>
            </figure>

            <h3>Motivation</h3>
            <p>
                Our approach is motivated by three key insights:
            </p>
            <ol>
                <li><strong>Inspiration from CLIP:</strong> The CLIP framework demonstrated that cross-modal
                    representations could be aligned through paired data and contrastive learning. However, it requires
                    joint training of encoders, limiting extensibility to additional modalities. Our method decouples
                    the encoders, focusing instead on aligning their outputs via lightweight adapters.</li>
                <li><strong>Testing the Platonic Representation Hypothesis:</strong> This hypothesis posits that
                    performant models converge toward a shared statistical model of reality in their representation
                    spaces. By aligning diverse unimodal encoders, we provide a testbed for exploring whether this
                    convergence can be achieved explicitly.</li>
                <li><strong>Scalability and Modularity:</strong> Traditional multimodal models often require joint
                    training on extensive datasets. By aligning frozen encoders post hoc, our framework supports modular
                    integration of new modalities, enabling efficient experimentation.</li>
            </ol>
        </section>

        <section id="related-work">
            <h2>Related Work</h2>
            <p>
                Our project builds on several important advances in multimodal representation learning and theoretical
                insights into model convergence:
            </p>
            <ol>
                <li>
                    <strong>Platonic Representation Hypothesis:</strong> Huh et al. (2024) proposed that performant
                    models converge toward a shared statistical model of reality in their representation spaces,
                    regardless of their training modality. This hypothesis underpins our framework, which aims to
                    explicitly align unimodal encoders to test this convergence hypothesis.
                </li>
                <li>
                    <strong>CLIP:</strong> Radford et al. (2021) introduced CLIP, a model that learns joint multimodal
                    representations using contrastive learning on paired image-text datasets. CLIP's success
                    demonstrates the power of cross-modal embeddings but requires joint training of encoders, which our
                    framework aims to circumvent.
                </li>
                <li>
                    <strong>Linear Mapping from Image to Text Space:</strong> Merullo et al. (2022) showed that simple
                    linear transformations can align visual representations to text spaces, enabling cross-modal tasks
                    like visual question answering. Their findings inspired our use of linear adapters for modality
                    alignment.
                </li>
                <li>
                    <strong>Grounding Language Models to Images:</strong> Koh et al. (2023) extended the idea of
                    multimodal alignment by mapping language models to visual inputs and outputs. This work highlights
                    the potential of lightweight transformations for cross-modal integration.
                </li>
                <li>
                    <strong>DINOv2:</strong> Oquab et al. (2023) introduced DINOv2, a self-supervised vision model that
                    generates robust embeddings. As our performant reference model, DINOv2 provides a benchmark for
                    evaluating the quality of our aligned multimodal representations.
                </li>
                <!-- TODO: Add this paper to the related work section: 
                Tongzhou Wang and Phillip Isola. Understanding contrastive representation learning through alignment and uniformity on the hypersphere. In International Conference on Machine Learning, pages 9929â€“9939. PMLR, 2020. -->
            </ol>
        </section>

        <section id="hypothesis">
            <h2>Hypothesis</h2>
            <p>
                Our work is grounded in the following hypotheses:
            </p>
            <ul>
                <li>A shared latent space exists where unimodal representations from different encoders can be aligned
                    through linear transformations.</li>
                <li>Aligning these representations produces embeddings that closely approximate those of performant
                    models, such as DinoV2, as measured by kernel alignment metrics.</li>
                <li>Multimodal alignment captures mechanisms of representation convergence, offering empirical evidence
                    for the Platonic Representation Hypothesis.</li>
            </ul>
        </section>

        <section id="mathematical-framework">
            <h2>Mathematical Framework</h2>
            <p>
                At the heart of our project is the hypothesis that pre-trained unimodal representations can be aligned
                into a shared multimodal latent space. This section formalizes the underlying mathematical structure of
                our framework, detailing how representations are extracted, aligned, and evaluated.
            </p>

            <h3>Multimodal Data Representation</h3>
            <p>
                Let the world generate raw multimodal data:
            </p>
            <p>
                \[
                \mathcal{D}_\text{world} = \left\{\left(\theta^{(i)}, \psi^{(i)}, \phi^{(i)}, \dots \right)
                \right\}_{i=1}^N, \quad
                \theta \in \mathcal{\Theta}, \psi \in \mathcal{\Psi}, \phi \in \mathcal{\Phi}, \dots
                \]
            </p>
            <p>
                where \( \{ \mathcal{\Theta}, \mathcal{\Psi}, \mathcal{\Phi}, \dots \} \) represent different modalities
                (e.g., image,
                text,
                audio, ...), and \( \{ \theta, \psi, \phi, \dots \} \) are specific instances of the corresponding
                modalities.
            </p>
            <p>
                For simplicity, we focus in this project on a two-modality world with image and text data:
            </p>
            <p>
                \[
                \mathcal{D}_\text{world} = \left\{\left(x^{(i)}, y^{(i)}\right) \right\}_{i=1}^N, \quad x \in
                \mathcal{X}, y \in \mathcal{Y}
                \]
            </p>
            <p>
                where \(\mathcal{X}\) is the image modality and \(\mathcal{Y}\) is the text modality.
            </p>
            <img src="figures/platonic_representation.svg" , alt="Platonic Representation Hypothesis">
            <figcaption>
                <strong>Figure 2.</strong> Apapted from the blog post of <a href="https://phillipi.github.io/prh/">The
                    Platonic Representation Hypothesis (PRH)</a> by <a href="https://arxiv.org/abs/2405.07987 ">Huh et
                    al. (2024)</a>. The PRH conjectures that representations learned on each modality on its own will
                converge to
                a similar representation and the limit this convergence is the Platonic representation. We posit that we
                can drive this convergence by aligning representations of different modalities into a shared latent
                space via constrastve learning.
            </figcaption>

            <h3>Learned Adapters</h3>
            <p>
                For each modality, we use frozen pre-trained encoders to extract representations:
            </p>
            <p>
                \[
                X_\text{enc}: \mathcal{X} \rightarrow \mathbb{R}^{d_x}, \quad
                Y_\text{enc}: \mathcal{Y} \rightarrow \mathbb{R}^{d_y} \quad
                \]
            </p>
            <p>
                where \( d_x, d_y \) are the embedding dimensions of the image and text inputs, respectively.
            </p>
            <p>
                To align these representations into a shared multimodal latent space \( \mathbb{R}^{d_e} \), we
                introduce learned
                adapters:
            </p>
            <p>
                \[
                W_x : \mathbb{R}^{d_x} \rightarrow \mathbb{R}^{d_e}, \quad
                W_y : \mathbb{R}^{d_y} \rightarrow \mathbb{R}^{d_e} \quad
                \]
            </p>
            <p>
                We experiment with the adapters \( W_x, W_y\) being either linear trasnformations or simple 2-layer
                multi-layer perceptron (MLP).
            </p>
            <p>
                where \( d_e \) is the dimensionality of the shared latent space.
                The compositions of the frozen encoders and corresponding adapters produces new encoders which we call
                \(
                f_\text{img}, g_\text{text} \) to match the notation used in the <a
                    href="https://phillipi.github.io/prh/">PRH blog</a>.
            </p>
            <p>
                We define the adapted encoders as follows:
            </p>
            <p>
                \[
                \begin{aligned}
                f_\text{img}&: \mathcal{X} \rightarrow \mathbb{R}^{d_e}, \quad f_\text{img} := W_x \circ
                X_\text{enc} \\
                g_\text{text}&: \mathcal{Y} \rightarrow \mathbb{R}^{d_e}, \quad g_\text{text} := W_y \circ
                Y_\text{enc}
                \end{aligned}
                \]
            </p>
            <p>
                These functions map images and text into the shared latent space \( \mathbb{R}^{d_e} \).
            </p>

            <h3>Dual-Encoder Contrastive Objective</h3>
            <p>
                Inspired by the <strong>dual-encoder contrastive loss</strong> from Homework 4 and the <a
                    href="https://phillipi.github.io/prh/"><strong>PRH</strong></a>, we aim to align representations
                such that \( f_{\text {img
                }}\left(x^{(i)}\right)$ and $g_{\text
                {text }}\left(y^{(i)}\right) \) are close in the shared space. The objective function is:
            </p>
            <p>
                \[
                \mathcal{L}_{\text {contrastive }}=-\frac{1}{N} \sum_{i=1}^N \log \frac{\exp \left(f_{\text {img
                }}\left(x^{(i)}\right) \cdot g_{\text {text }}\left(y^{(i)}\right) / \tau\right)}{\sum_{j=1}^N \exp
                \left(f_{\text {img }}\left(x^{(i)}\right) \cdot g_{\text {text }}\left(y^{(j)}\right) / \tau\right)}
                \]
            </p>

            <p>
                where \( \cdot \) is the dot product and \( \tau \) is a temperature scaling parameter.
            </p>
            <p>
                This objective encourages matching pairs $\left(x^{(i)}, y^{(i)}\right)$ to have high similarity, while
                mismatched pairs $\left(x^{(i)}, y^{(j)}\right), j \neq i$ are pushed apart.
            </p>
            <img src="figures/homework4_contrastive.jpg" , alt="Dual Encoder Constrastve Loss">
            <figcaption>
                <strong>Figure 3.</strong> Adapted from Homework 4.
                <!-- # TODO: give better caption. -->
            </figcaption>

            <p>
                Intuitively, this contrastive loss encourages the adapters to map matching pairs \( (x(i), y(i)) \)
                to the same embedding vector in the latent space pushing apart the embedding vectors for non-matching
                pairs \(
                (x(i), y(j)), j \neq i \).
                This process is illustrated in Figure 3. We refer to the embedding vectors in the shared latent space as
                the aligned and/or multimodal representations.
            </p>
            <img src="figures/conceptual_contrastive.svg" , alt="Aligning Image Text Dataset Flickr30k">
            <figcaption>
                <strong>Figure 3.</strong> Conceptual illustration of contrastive learning: aligning image and text
                representations from the Flickr30k dataset.
            </figcaption>


            <h3>Interpretation and Connection to PRH</h3>
            <p>
                The <strong>Platonic Representation Hypothesis</strong> posits that representations from different
                modalities will converge towards a shared "Platonic" representation. In the context of our framework:
            <ul>
                <li>We hypothesize that <strong>contrastive alignment</strong> between \( f_{\text {img }} \) and \(
                    g_{\text
                    {text }} \) can drive this convergence towards the Platonic representation.</li>
                <li>In the limit of perfect contrastive alignment (minimum diameter, max margin), \( f_{\text {img }}(x)
                    \)
                    and \( g_{\text {text }}(y) \) would ideally map to the <strong>same vector </strong> in the shared
                    latent space. Our thesis is that this theoretical limiting vector is precisely the Platonic
                    representation.</li>
            </ul>
            </p>


            <p>
                Due to finite data and model limitations, \(f_{\text {img }}(x)\) and \(g_{\text {text }}(y)\) are
                typically <strong>close</strong> (as measured by \( L_2 \)-distance on \( \mathbb{S}^{d_e-1} \))
                <strong>but not identical</strong> . Thus, we consider two options for the aligned multimodal
                representation:
            <ol>
                <li><strong>Select</strong> the encoder (image or text) that produces the embedding closer to the
                    performant model's
                    embedding (e.g., DinoV2):
                    <!-- TODO: use \cases to write this. -->
                    <p>
                        \[
                        r^{(i)} = f_{\text {img }}(x^{(i)}) \text{ if} \operatorname{sim}\left( f_{\text {img
                        }}(x^{(i)}), \text{DinoV2}(x^{(i)}) \right) > \operatorname{sim}\left( g_{\text {text
                        }}(y^{(i)}), \text{DinoV2}(x^{(i)}) \right), \text { otherwise }
                        r^{(i)} = g_{\text{text }}(y^{(i)})
                        \]
                    </p>
                </li>
                <li><strong>Average</strong> the embeddings:
                    <p>
                        \[ r^{(i)}=\frac{1}{2}\left(f_{\text {img
                        }}\left(x^{(i)}\right)+g_{\text
                        {text }}\left(y^{(i)}\right)\right)
                        \]
                    </p>
                </li>
            </ol>
            </p>
            <p>
                We claim that this aligned multimodal representation is an <strong>approximation</strong> of the
                Platonic
                representation. Now we don't actually know what the Platonic representation is, but we assume /take for
                granted the
                conjecture of the PRH that the representations of perfomant models are converging towards it. So we
                treat
                performant model's representation (e.g., DinoV2) as a proxy for the Platonic representation. We evaluate
                the quality of
                the aligned multimodal representation by comparing it to the performant model's representation using a
                kernel alignment metric.
            </p>


            <h3>Kernel Alignment Metric</h4>
                <p>
                    We characterize representations in terms of their <strong>kernels</strong>. Kernels capture how
                    models measure the distance/\( \operatorname{sim} \)ilarity between inputs.
                    Two representations are considered the same if their kernels are the same for corresponding inputs.
                    We then say the representations are <strong>aligned</strong>.
                    For example, if a text encoder
                    \( f_{\text{text}} \) is aligned with an image encoder \( f_{\text{img}} \), we expect the
                    \( \operatorname{sim} \)ilarity of apples and oranges in text to correspond closely to the \(
                    \operatorname{sim} \)ilarity
                    of their image representations:
                </p>
                <figure>
                    <img src="figures/alignment_metric.jpg" , alt="Representation Similarity">
                </figure>

                <p>
                    To quantify alignment, we use the mutual \(k\)-nearest neighbor (mutual-KNN) kernel alignment metric
                    \(m\) as used by <a href="https://arxiv.org/abs/2405.07987 ">Huh et al. (2024)</a>. The kernels are
                    defined as:
                </p>
                <ul>
                    <li><strong>Unimodal Kernels:</strong>
                        \[
                        K_X(i, j) = \langle X_\text{enc}(x^{(i)}), X_\text{enc}(x^{(j)}) \rangle, \quad
                        K_Y(i, j) = \langle Y_\text{enc}(y^{(i)}), Y_\text{enc}(y^{(j)}) \rangle.
                        \]
                    </li>
                    <li><strong>Aligned Multimodal Kernel:</strong>
                        \[
                        K_{\text {repr }}(i, j)=\left\langle r^{(i)}, r^{(j)}\right\rangle
                        \]
                        where \( r^{(i)} \) is the aligned multimodal representation as described in section
                        "Interpretation and Connection to PRH".
                        <!-- how to reference to a section in HTML? -->
                    </li>
                    <li><strong>Performant Model Kernel:</strong>
                        \[
                        K_\text{DinoV2}(i, j) = \langle \text{DinoV2}(x^{(i)}), \text{DinoV2}(x^{(j)}) \rangle.
                        \]
                    </li>
                </ul>

                <h4>Before Training</h4>
                <p>
                    Compute alignment metrics for unimodal kernels relative to the reference performant model kernel \(
                    K_{\text{DinoV2}} \):
                </p>
                <p>
                    \[
                    m(K_X, K_{\text{DinoV2}}), \quad m(K_Y, K_{\text{DinoV2}})
                    \]
                </p>

                <h4>After Training</h4>
                <p>
                    Evaluate the mutual-KNN alignment for the aligned multimodal kernel \( K_{\text{repr}} \) against \(
                    K_{\text{DinoV2}} \):
                </p>
                <p>
                    \[
                    m(K_{\text{repr}}, K_{\text{DinoV2}})
                    \]
                </p>


                <h4>Key Hypothesis</h4>
                <p>
                    The aligned multimodal kernel achieve <strong>higher similarity</strong> with \( K_{\text{DinoV2}}
                    \) than the unimodal kernels:
                </p>
                <p>
                    \[
                    m(K_{\text{repr}}, K_{\text{DinoV2}}) > \text{avg}\big(m(K_X, K_{\text{DinoV2}}), m(K_Y,
                    K_{\text{DinoV2}})\big)
                    \]
                </p>
                <p>
                    <a href="https://arxiv.org/abs/2405.07987 ">Huh et al. (2024)</a> identify that representations are
                    converging across modalities i.e. models are
                    learning an increasingly modality agnostic representation of the world.
                    Our hypothesis goes beyond that and suggests multimodal alignment via contrastive learning as an
                    algorithm to drive representational convergence
                    towards a more platonic repesentation.
                </p>
                <p>
                    A stronger version of our hypothesis is if the aligned multimodal kernel surpasses even the better
                    unimodal kernel:
                </p>
                <p>
                    \[
                    m(K_{\text{repr}}, K_{\text{DinoV2}}) > \max\big(m(K_X, K_{\text{DinoV2}}), m(K_Y,
                    K_{\text{DinoV2}})\big)
                    \]
                </p>
                <p>
                    This would suggest an emergent-like
                    "whole is greater than the sum of its parts" property where the multimodal kernel
                    is more similar to the hypothesized platonic representation than either unimodal kernel alone.
                </p>
        </section>

        <section id="methodology">
            <h2>Methodology</h2>

            <h3>Data Pipeline</h3>
            <p>
                The data pipeline processes paired image and text data from the Flickr30k dataset, which contains paired
                image-caption samples, making it ideal for our two-modality prototype. Key stages include:
            </p>
            <ul>
                <li><strong>Image Processing:</strong> Images are resized to \( 224 \times 224 \) pixels, normalized to
                    match ResNet-18 preprocessing, and converted to tensors.</li>
                <li><strong>Text Processing:</strong> Captions are tokenized using the BERT tokenizer, padded to uniform
                    length, and converted to tensors.</li>
                <li><strong>Data Loading:</strong> Configurations include a batch size of 64, shuffling, and
                    optimizations for GPU utilization.</li>
            </ul>
            <figure>
                <img src="figures/data_pipeline.png" alt="Data Pipeline">
                <figcaption>Data pipeline: preprocessing images and captions for input to the model.</figcaption>
            </figure>

            <h3>Model Architecture</h3>
            <p>
                While the mathemical formulation il;;ustrated the use pf linear adapters, our implementation also
                explores
                the use of simple 2-layer multi-layer perceptron (MLP) adapters:
            </p>
            <p>
                \[
                f_\text{img} := \operatorname{MLP}_x \circ X_\text{enc}, \quad
                g_\text{text} := \operatorname{MLP}_y \circ Y_\text{enc} \quad
                \]
            </p>
            <p>
                The architecture combines frozen pre-trained encoders with trainable adapters to project representations
                into a shared multimodal space:
            </p>
            <ul>
                <li><strong>Encoders:</strong> ResNet-18 for images (\( d_\text{enc} = \mathbb{R}^{512} \)) and
                    DistilBERT for text (\(
                    d_\text{enc} = \mathbb{R}^{768} \)).</li>
                <li><strong>Adapters:</strong>
                    <ul>
                        <li>Linear adapters (\( \mathbb{R}^{d_\text{enc}} \to \mathbb{R}^{384} \)).</li>
                        <li>MLP adapters (\( \mathbb{R}^{d_\text{enc}} \to \mathbb{R}^{2048} \to \mathbb{R}^{1024} \to
                            \mathbb{R}^{384} \)).</li>
                    </ul>
                </li>
            </ul>
            <figure>
                <img src="figures/model_architecture.png" alt="Model Architecture">
                <figcaption>Model architecture: frozen encoders, adapters, and shared multimodal latent space.
                </figcaption>
            </figure>

            <h3>Training Procedure</h3>
            <p>
                We use contrastive learning to align representations. The training loop involves:
            </p>
            <ol>
                <li>Processing paired batches through the encoders and adapters.</li>
                <li>Computing contrastive loss to align matching pairs and separate mismatched pairs:
                    <br>
                    \[
                    \mathcal{L} = -\frac{1}{N} \sum_{i=1}^N \log \frac{\exp\left(\langle W_x X_\text{enc}(x^{(i)}), W_y
                    Y_\text{enc}(y^{(i)}) \rangle / \tau \right)}{\sum_{j=1}^N \exp\left(\langle W_x
                    X_\text{enc}(x^{(i)}), W_y Y_\text{enc}(y^{(j)}) \rangle / \tau \right)}.
                    \]
                </li>
                <li>Updating adapter weights using gradient descent.</li>
            </ol>
            <figure>
                <img src="figures/training_loss_metrics.png" alt="Training Metrics">
                <figcaption>Training loss and kernel alignment metrics over epochs.</figcaption>
            </figure>

            <h3>Evaluation Metrics</h3>
            <p>
                We evaluate the learned representations using the mutual-KNN kernel alignment metric (\( m \)),
                comparing kernels before and after training:
            </p>
            <ul>
                <li><strong>Unimodal Kernel Similarity:</strong> Baseline similarity of ResNet-18 and DistilBERT
                    representations to DINOv2 embeddings.</li>
                <li><strong>Aligned Kernel Similarity:</strong> Final similarity of the aligned/multimodal
                    representations to
                    DINOv2 embeddings.</li>
            </ul>
            <figure>
                <img src="figures/kernel_heatmaps.png" alt="Kernel Similarity Heatmaps">
                <figcaption>Heatmaps showing kernel similarities for unimodal and multimodal embeddings with DINOv2.
                </figcaption>
            </figure>

            <h3>Implementation Details</h3>
            <p>
                All code we used for this project is implemented in this <a
                    href="https://colab.research.google.com/drive/1BkyPko0x-8CL41Z-VSmyxI41B90hB2Fc?usp=sharing">Google
                    Colab notebook</a>.
            </p>
            <ul>
                <li><strong>Frameworks:</strong> PyTorch for modeling, Hugging Face Transformers for text encoding, and
                    Weights & Biases for experiment tracking.</li>
                <li><strong>Hyperparameters:</strong>
                    <ul>
                        <li>Batch size: 64</li>
                        <li>Learning rate: \( 1 \times 10^{-4} \)</li>
                        <li>Temperature (\( \tau \)): 0.07</li>
                        <li>Epochs: 50</li>
                    </ul>
                </li>
            </ul>
        </section>


        <section id="initial-results">
            <h2>Initial Results</h2>
            <p>
                Our experiments with image and text modalities have yielded promising results. By aligning
                representations from ResNet-18 and DistilBERT, we observed:
            </p>
            <ul>
                <li><strong>Alignment Success:</strong> Aligned representations demonstrated improved similarity to
                    DinoV2 embeddings.</li>
                <li><strong>Cross-Modality Insights:</strong> Text embeddings, which DinoV2 was not trained to process,
                    gained representational properties closer to its image embeddings.</li>
                <li><strong>Caveat:</strong> While the aligned representation exceeded the average similarity of
                    unimodal kernels to DinoV2, it fell short of the maximum similarity (i.e., the image modality
                    kernel).</li>
            </ul>

            <h3>Metrics</h3>
            <p>
                Key kernel alignment metrics include:
            </p>
            <ul>
                <li>Text-to-DinoV2 similarity: Low, due to modality mismatch.</li>
                <li>Image-to-DinoV2 similarity: High, reflecting shared training data.</li>
                <li>Aligned multimodal similarity: Intermediate, surpassing the average of unimodal similarities.</li>
            </ul>
            <div class="figures-container">
                <figure>
                    <img src="figures/mlp/2d/m2d.gif" alt="2D MLP Adapter">
                    <figcaption>MLP Adapter</figcaption>
                </figure>
                <figure>
                    <img src="figures/mlp/3d/m3d.gif" alt="3D MLP Adapter">
                    <figcaption>MLP Adapter</figcaption>
                </figure>
            </div>
            <div class="figures-container">
                <figure>
                    <img src="figures/linear/2d/l2d.gif" alt="2D Linear Adapter">
                    <figcaption>Linear Adapter</figcaption>
                </figure>
                <figure>
                    <img src="figures/linear/3d/l3d.gif" alt="3D Linear Adapter">
                    <figcaption>Linear Adapter</figcaption>
                </figure>
            </div>
            <p>
                The bar plot in Figure 5 visually summarizes the kernel alignment metrics before and after training the
                adapters.
                We can immediately inspect whether our hypotheses stated in the Mathematical Framework section are
                supported or not.
            </p>
            <figure>
                <img src="figures/bar_plot_sketch.jpg" alt="Kernel Alignment Metric Before and After Trainign">
                <figcaption><strong>Figure 5.</strong>Placeholder: Bar plot showing kernel alignment for text, image,
                    and multimodal embeddings
                    before and after training adapters to align unimodal representations into a latent multimodal
                    representation using contrastive loss. The top subplot is for the linear adapter. The bottom suplot
                    is for the MLP adapter.
                    Each suplplot shows bars for before and after training of the adapters for the image (hacthed bars)
                    and text (solid bars) modalities.
                    The y-axis is the mutual-kNN kernel alignment metric \(m\) between the modality embedding and the
                    DinoV2 embedding
                    (which we assume approximates the Platonic representation).</figcaption>
            </figure>
        </section>

        <section id="proposed-extensions">
            <h2>Proposed Extensions</h2>
            <h3>1. Downstream Task Evaluation</h3>
            <p>
                Evaluate aligned embeddings on downstream tasks, such as classification, using pre-trained heads.
                Hypotheses include:
            </p>
            <ul>
                <li>Slight performance degradation compared to DinoV2.</li>
                <li>Aligned multimodal embeddings outperform unimodal embeddings.</li>
            </ul>

            <h3>2. Adding a Third Modality</h3>
            <p>
                Expand the framework to include audio representations from models like Wav2Vec2. Test whether
                integrating a third modality enhances alignment and demonstrates additive representation benefits.
            </p>

            <figure>
                <img src="figures/multimodal-alignment.svg" alt="Multimodal Alignment Process">
                <figcaption>Placeholder: Diagram illustrating multimodal alignment across three modalities (image, text,
                    and audio).</figcaption>
            </figure>
        </section>

        <section id="future-directions">
            <h2>Future Directions</h2>
            <ul>
                <li>Scale experiments to richer multimodal datasets and benchmarks.</li>
                <li>Investigate cross-architecture generalization of aligned embeddings.</li>
                <li>Analyze mechanistic insights into alignment-driven representation convergence.</li>
            </ul>
        </section>

        <section id="conclusion">
            <h2>Conclusion</h2>
            <p>
                This project illustrates a scalable and efficient approach to aligning unimodal encoders into a shared
                multimodal space. By leveraging pre-trained models and lightweight adapters, we unlock potential
                applications in resource-efficient AI and multimodal representation learning. Our findings highlight the
                promise of this methodology and pave the way for further exploration into the convergence of
                representations across modalities.
            </p>
        </section>

        <section id="discussion">
            <h2>Discussion</h2>
            <p>
                In this project, we build upon a central premise of the Platonic Representation Hypothesis (PRH)
                proposed by Huh et al. (2024): the idea that the representations learned by performant models converge
                toward an idealized, universal representation of the world â€” a <strong>Platonic representation</strong>.
                We take this hypothesis as a starting point and assume that the representations produced by
                <strong>DinoV2</strong> serve as a reasonable approximation of this Platonic representation.
            </p>

            <p>
                Our primary conjecture is that one mechanism to achieve this Platonic representation is through the
                alignment of representations from different modalities into a shared latent space. Specifically, by
                using contrastive learning to align representations from <strong>pre-trained image and text
                    encoders</strong> (ResNet-18 and DistilBERT), we hypothesize that the resulting multimodal
                representation can approximate the Platonic representation more closely than either of the unimodal
                representations alone.
            </p>

            <h3>Driving Convergence via Alignment</h3>
            <p>
                The PRH suggests that convergence of representations occurs naturally due to factors such as:
            </p>
            <ul>
                <li>Training on diverse, multi-task datasets.</li>
                <li>Increasingly performant and large-scale model architectures.</li>
                <li>Utilization of multimodal data during training.</li>
            </ul>
            <p>
                Our work extends this hypothesis by proposing an <strong>explicit algorithmic mechanism</strong> for
                driving representational convergence: aligning pre-trained unimodal representations using
                <strong>contrastive learning</strong>. Instead of relying on joint training of large multimodal models,
                our approach demonstrates that:
            </p>
            <ul>
                <li>Lightweight linear and MLP adapters can effectively align frozen unimodal encoders.</li>
                <li>Contrastive alignment in a shared latent space can achieve meaningful convergence toward the
                    Platonic representation.</li>
            </ul>

            <p>
                This approach offers a more efficient alternative to end-to-end multimodal training. By aligning
                representations post hoc, we enable <strong>modular scalability</strong>, where new modalities can be
                incorporated without retraining the entire model.
            </p>

            <h3>Empirical Insights</h3>
            <p>
                Our results provide partial validation of our hypotheses. Specifically, we observed that the
                <strong>aligned multimodal representations</strong> exhibit:
            </p>
            <ul>
                <li><strong>Improved Similarity</strong> to DinoV2 embeddings compared to the average of unimodal
                    representations.</li>
                <li>A <strong>Cross-Modality Benefit</strong> where text representations, which DinoV2 was not trained
                    to process, became more similar to the image representations.</li>
            </ul>
            <p>
                However, the aligned multimodal representation did not surpass the best-performing unimodal
                representation (the image modality) in terms of kernel alignment. This caveat suggests that while
                alignment can drive convergence, there may still be modality-specific factors that influence the quality
                of representations.
            </p>

            <h3>Implications and Future Directions</h3>
            <p>
                Our findings suggest that multimodal alignment via contrastive learning is a promising direction for
                achieving efficient and scalable multimodal representation learning. This work also opens up several
                avenues for further research:
            </p>
            <ul>
                <li><strong>Incorporating Additional Modalities:</strong> Expanding the framework to include audio,
                    video, or graph data could further test the robustness of the alignment mechanism.</li>
                <li><strong>Downstream Task Evaluation:</strong> Assessing the utility of aligned representations on
                    tasks like visual question answering, retrieval, or classification.</li>
                <li><strong>Mechanistic Analysis:</strong> Investigating the internal mechanisms of convergence and how
                    specific adapter architectures influence the alignment process.</li>
            </ul>

            <p>
                Ultimately, this work supports the broader goal of understanding how different modalities can be
                integrated to form a cohesive, universal representation of the world â€” a pursuit that lies at the heart
                of the Platonic Representation Hypothesis.
            </p>
        </section>

    </main>

    <footer>
        <p>Multimodal Alignment Analysis | Blog | November 2024</p>
    </footer>
</body>

<style>
    figure {
        display: flex;
        /* Arrange images side by side */
        flex-direction: column;
        /* Ensure caption appears below images */
        align-items: center;
        /* Center the content */
        gap: 10px;
        /* Add space between images and caption */
    }

    .figures-container {
        display: flex;
        /* Arrange figures side by side */
        justify-content: center;
        /* Center the figures on the page */
        gap: 20px;
        /* Add spacing between the figures */
    }

    figure img {
        width: 400px;
        /* Adjust image width */
        height: auto;
        /* Maintain aspect ratio */
    }

    figcaption {
        text-align: center;
        /* Center-align the caption text */
        font-size: 14px;
        /* Adjust caption font size */
        color: #555;
        /* Add a softer color for the caption */
    }
</style>

</html>