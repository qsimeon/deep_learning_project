<!DOCTYPE html>
<html lang="en">

<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <script type="text/javascript" async
        src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/3.2.2/es5/tex-mml-chtml.js"></script>
    <title>Aligning Modalities: Efficient Multimodal Representation Learning</title>
    <link rel="stylesheet" href="styles/style.css">
</head>

<body>
    <header>
        <h1>Aligning Modalities: Efficient Multimodal Representation Learning</h1>
    </header>

    <main>
        <section id="introduction">
            <h2>Introduction</h2>
            <p>
                Deep learning models are typically trained to transform raw data into representations optimized for
                specific tasks. Recently, two lines of research have inspired a deeper inquiry into the nature of these
                representations. The CLIP framework demonstrated the utility of aligning representations across
                modalities, using paired image-text data to train joint embeddings for cross-modal retrieval. Meanwhile,
                the Platonic Representation Hypothesis posits that performant models converge toward a shared
                statistical model of reality in their representation spaces, suggesting a potential universality
                underlying learned representations.
            </p>
            <p>
                This project bridges these ideas by exploring whether representations from disparate pre-trained
                unimodal neural networks can be aligned into a shared multimodal latent space, inspired by the joint
                embedding approach of CLIP and motivated by the convergence hypothesis of Platonic Representations. The
                proposed framework uses frozen unimodal encoders (e.g., ResNet-18 for images, GPT-2 for text) with
                learned linear adapters to align representations across modalities. Our aim is to determine if such
                aligned representations better approximate those of larger, more performant models (e.g., DinoV2).
            </p>
            <p>
                Inspired by the success of CLIP in aligning representations across modalities and the theoretical
                insights of the Platonic Representation Hypothesis, we propose a framework that aligns pre-trained
                unimodal encoders into a shared multimodal latent space using simple linear adapters. By leveraging
                frozen encoders such as ResNet-18 for images and GPT-2 for text, we aim to achieve alignment without
                retraining these large models, enabling scalability to new modalities with minimal overhead.
            </p>
            <figure>
                <img src="figures/img_txt_concept.svg" alt="Multimodal Alignment Process">
                <figcaption><strong>Figure 1.</strong> Diagram illustrating multimodal alignment across 2 modalities
                    (image and text).
                </figcaption>
            </figure>

            <h3>Motivation</h3>
            <p>
                Our approach is motivated by three key insights:
            </p>
            <ol>
                <li><strong>Inspiration from CLIP:</strong> The CLIP framework demonstrated that cross-modal
                    representations could be aligned through paired data and contrastive learning. However, it requires
                    joint training of encoders, limiting extensibility to additional modalities. Our method decouples
                    the encoders, focusing instead on aligning their outputs via lightweight adapters.</li>
                <li><strong>Testing the Platonic Representation Hypothesis:</strong> This hypothesis posits that
                    performant models converge toward a shared statistical model of reality in their representation
                    spaces. By aligning diverse unimodal encoders, we provide a testbed for exploring whether this
                    convergence can be achieved explicitly.</li>
                <li><strong>Scalability and Modularity:</strong> Traditional multimodal models often require joint
                    training on extensive datasets. By aligning frozen encoders post hoc, our framework supports modular
                    integration of new modalities, enabling efficient experimentation.</li>
            </ol>
        </section>

        <section id="related-work">
            <h2>Related Work</h2>
            <p>
                Our project builds on several important advances in multimodal representation learning and theoretical
                insights into model convergence:
            </p>
            <ol>
                <li>
                    <strong>Platonic Representation Hypothesis:</strong> Huh et al. (2024) proposed that performant
                    models converge toward a shared statistical model of reality in their representation spaces,
                    regardless of their training modality. This hypothesis underpins our framework, which aims to
                    explicitly align unimodal encoders to test this convergence hypothesis.
                </li>
                <li>
                    <strong>CLIP:</strong> Radford et al. (2021) introduced CLIP, a model that learns joint multimodal
                    representations using contrastive learning on paired image-text datasets. CLIP's success
                    demonstrates the power of cross-modal embeddings but requires joint training of encoders, which our
                    framework aims to circumvent.
                </li>
                <li>
                    <strong>Linear Mapping from Image to Text Space:</strong> Merullo et al. (2022) showed that simple
                    linear transformations can align visual representations to text spaces, enabling cross-modal tasks
                    like visual question answering. Their findings inspired our use of linear adapters for modality
                    alignment.
                </li>
                <li>
                    <strong>Grounding Language Models to Images:</strong> Koh et al. (2023) extended the idea of
                    multimodal alignment by mapping language models to visual inputs and outputs. This work highlights
                    the potential of lightweight transformations for cross-modal integration.
                </li>
                <li>
                    <strong>DINOv2:</strong> Oquab et al. (2023) introduced DINOv2, a self-supervised vision model that
                    generates robust embeddings. As our performant reference model, DINOv2 provides a benchmark for
                    evaluating the quality of our aligned multimodal representations.
                </li>
            </ol>
        </section>

        <section id="hypothesis">
            <h2>Hypothesis</h2>
            <p>
                Our work is grounded in the following hypotheses:
            </p>
            <ul>
                <li>A shared latent space exists where unimodal representations from different encoders can be aligned
                    through linear transformations.</li>
                <li>Aligning these representations produces embeddings that closely approximate those of performant
                    models, such as DinoV2, as measured by kernel alignment metrics.</li>
                <li>Multimodal alignment captures mechanisms of representation convergence, offering empirical evidence
                    for the Platonic Representation Hypothesis.</li>
            </ul>
        </section>

        <section id="mathematical-framework">
            <h2>Mathematical Framework</h2>
            <p>
                At the heart of our project is the hypothesis that pre-trained unimodal representations can be aligned
                into a shared multimodal latent space. This section formalizes the underlying mathematical structure of
                our framework, detailing how representations are extracted, aligned, and evaluated.
            </p>

            <h3>Multimodal Data Representation</h3>
            <p>
                Let the world generate raw multimodal data:
            </p>
            <p>
                \[
                \mathcal{D}_\text{world} = \left\{\left(\theta^{(i)}, \psi^{(i)}, \phi^{(i)}, \dots \right)
                \right\}_{i=1}^N, \quad
                \theta \in \mathcal{\Theta}, \psi \in \mathcal{\Psi}, \phi \in \mathcal{\Phi}, \dots
                \]
            </p>
            <p>
                where \( \{ \mathcal{\Theta}, \mathcal{\Psi}, \mathcal{\Phi}, \dots \} \) represent different modalities
                (e.g., image,
                text,
                audio, ...), and \( \{ \theta, \psi, \phi, \dots \} \) are specific instances of the corresponding
                modalities.
            </p>
            <p>
                For simplicity, we focus in this project on a two-modality world with image and text data:
            </p>
            <p>
                \[
                \mathcal{D}_\text{world} = \left\{\left(x^{(i)}, y^{(i)}\right) \right\}_{i=1}^N, \quad x \in
                \mathcal{X}, y \in \mathcal{Y}
                \]
            </p>
            <p>
                where \(\mathcal{X}\) is the image modality and \(\mathcal{Y}\) is the text modality.
            </p>
            <img src="figures/platonic_representation.svg" , alt="Platonic Representation Hypothesis">
            <figcaption>
                <strong>Figure 2.</strong> Apapted from the blog post of <a href="https://phillipi.github.io/prh/">The
                    Platonic Representation Hypothesis (PRH)</a> by <a href="https://arxiv.org/abs/2405.07987 ">Huh et
                    al. (2024)</a>. The PRH conjectures that representations learned on each modality on its own will
                converge to
                a similar representation and the limit this convergence is the Platonic representation. We posit that we
                can drive this convergence by aligning representations of different modalities into a shared latent
                space via constrastve learning.
            </figcaption>

            <h3>Learned Adapters</h3>
            <p>
                For each modality, we use frozen pre-trained encoders to extract representations:
            </p>
            <p>
                \[
                X_\text{enc}: \mathcal{X} \rightarrow \mathbb{R}^{d_x}, \quad
                Y_\text{enc}: \mathcal{Y} \rightarrow \mathbb{R}^{d_y} \quad
                \]
            </p>
            <p>
                where \( d_x, d_y \) are the embedding dimensions of the image and text inputs, respectively.
            </p>
            <p>
                To align representations into a shared latent space \( \mathbb{R}^{d_e} \), we introduce learned linear
                adapters:
            </p>
            <p>
                \[
                W_x : \mathbb{R}^{d_x} \rightarrow \mathbb{R}^{d_e}, \quad
                W_y : \mathbb{R}^{d_y} \rightarrow \mathbb{R}^{d_e} \quad
                \]
            </p>
            <p>
                where \( d_e \) is the dimensionality of the shared latent space.
                The compositions of the encoders and corresponding adapters are analagous to the functions \(
                f_\text{img}, g_\text{text} \)
                in the PRH (Figure 1), i.e:
            </p>
            <p>
                \[
                f_\text{img} = W_x \circ X_\text{enc}, \quad
                g_\text{text} = W_y \circ Y_\text{enc} \quad
                \]
            </p>
            <p>
                While we only discuss linear adapters in the mathematical framework, our implementation also explores
                the use of simple 2-layer multi-layer perceptron (MLP) adapters.
            </p>

            <h3>Contrastive Learning Objective</h3>
            <p>
                The adapters are trained using a contrastive loss, which ensures that representations from the same data
                point across modalities are close in the shared latent space, while unrelated representations are pushed
                farther apart. The loss is defined as:
            </p>
            <p>
                \[
                \mathcal{L} = -\frac{1}{N} \sum_{i=1}^N \log \frac{\exp\left(\langle W_x X_\text{enc}(x^{(i)}), W_y
                Y_\text{enc}(y^{(i)}) \rangle / \tau \right)}{\sum_{j=1}^N \exp\left(\langle W_x X_\text{enc}(x^{(i)}),
                W_y Y_\text{enc}(y^{(j)}) \rangle / \tau \right)},
                \]
            </p>
            <p>
                where \( \tau \) is a temperature parameter that scales the similarity scores.
            </p>
            Intuitively, the contrastive loss function encourages the model to align matching pairs $(x(i), y(i))$ while
            pushing apart non-matching pairs.
            <img src="figures/conceptual_contrastive.svg" , alt="Aligning Image Text Dataset Flickr30k">
            <figcaption>
                <strong>Figure 3.</strong> Conceptual illustration of contrastive learning: aligning image and text
                representations from the Flickr30k dataset.
            </figcaption>

            <h3>Kernel Alignment Metric</h4>
                <p>
                    We characterize representations in terms of their <strong>kernels</strong>. Kernels capture how
                    models measure the distance/\( \operatorname{sim} \)ilarity between inputs.
                    Two representations are considered the same if their kernels are the same for corresponding inputs.
                    We then say the representations are <strong>aligned</strong>.
                    For example, if a text encoder
                    \( f_{\text{text}} \) is aligned with an image encoder \( f_{\text{img}} \), we expect the
                    \( \operatorname{sim} \)ilarity of apples and oranges in text to correspond closely to the \(
                    \operatorname{sim} \)ilarity
                    of their image representations:
                </p>
                <figure>
                    <img src="figures/alignment_metric.jpg" , alt="Representation Similarity">
                </figure>

                <p>
                    To evaluate representational alignment, we use an <strong>kernel alignment metric</strong>, which
                    quantifies the similarity between kernels derived from different models.
                    In particular, we employ the <strong>mutual \( k \)-nearest neighbor (mutual-KNN)</strong> alignment
                    metric \( m\). This metric measures the overlap of \( k \)-nearest neighbor sets induced by the
                    kernels of two
                    models (see Appendix A of Huh et al. (2024) for details).
                </p>
                <p>
                    We define kernels for both the unimodal encoders and the shared representation space:
                </p>
                <ul>
                    <li><strong>Unimodal Kernels:</strong>
                        \[
                        K_X(i, j) = \langle X_\text{enc}(x^{(i)}), X_\text{enc}(x^{(j)}) \rangle, \quad
                        K_Y(i, j) = \langle Y_\text{enc}(y^{(i)}), Y_\text{enc}(y^{(j)}) \rangle.
                        \]
                    </li>
                    <li><strong>Aligned Multimodal Kernel:</strong>
                        \[
                        K_\text{repr}(i, j) = \langle r^{(i)}, r^{(j)} \rangle, \quad r^{(i)} = W_x
                        X_\text{enc}(x^{(i)}).
                        \]
                    </li>
                    <li><strong>Performant Model Kernel:</strong>
                        \[
                        K_\text{DinoV2}(i, j) = \langle \text{DinoV2}(x^{(i)}), \text{DinoV2}(x^{(j)}) \rangle.
                        \]
                    </li>
                </ul>

                <h4>Before Training</h4>
                <p>
                    Compute alignment metrics for unimodal kernels relative to the reference performant model kernel \(
                    K_{\text{DinoV2}} \):
                </p>
                <p>
                    \[
                    m(K_X, K_{\text{DinoV2}}), \quad m(K_Y, K_{\text{DinoV2}}).
                    \]
                </p>

                <h4>After Training</h4>
                <p>
                    Evaluate the mutual-KNN alignment for the aligned multimodal kernel \( K_{\text{repr}} \) against \(
                    K_{\text{DinoV2}} \):
                </p>
                <p>
                    \[
                    m(K_{\text{repr}}, K_{\text{DinoV2}}).
                    \]
                </p>


                <h4>Key Hypothesis</h4>
                <p>
                    The aligned multimodal kernel achieve <strong>higher similarity</strong> with \( K_{\text{DinoV2}}
                    \) than the unimodal kernels:
                </p>
                <p>
                    \[
                    m(K_{\text{repr}}, K_{\text{DinoV2}}) > \text{avg}\big(m(K_X, K_{\text{DinoV2}}), m(K_Y,
                    K_{\text{DinoV2}})\big).
                    \]
                </p>
                <p>
                    Huh et al. (2023) indentify that representations are converging across modalities i.e. models are
                    learning an increasingly modality agnostics representation of the world.
                    Our hypothesis goes beyond that an suggests an actual algorithm to drive this convergence i.e.
                    multimodal alignment via contrastive learning as an algorithm to drive representational convergence
                    towards the platonic repesentation.
                </p>
                <p>
                    A stronger version of our hypothesis is if the aligned multimodal kernel surpasses even the better
                    unimodal kernel:
                </p>
                <p>
                    \[
                    m(K_{\text{repr}}, K_{\text{DinoV2}}) > \max\big(m(K_X, K_{\text{DinoV2}}), m(K_Y,
                    K_{\text{DinoV2}})\big).
                    \]
                </p>
                <p>
                    This would suggest an emergent-like
                    "whole is greater than the sum of its parts" property where the multimodal kernel
                    is more similar to the hypothesized platonic representation than either unimodal kernel alone.
                </p>
        </section>

        <section id="methodology">
            <h2>Methodology</h2>

            <h3>Data Pipeline</h3>
            <p>
                The data pipeline processes paired image and text data from the Flickr30k dataset, which contains paired
                image-caption samples, making it ideal for our two-modality prototype. Key stages include:
            </p>
            <ul>
                <li><strong>Image Processing:</strong> Images are resized to \( 224 \times 224 \) pixels, normalized to
                    match ResNet-18 preprocessing, and converted to tensors.</li>
                <li><strong>Text Processing:</strong> Captions are tokenized using the BERT tokenizer, padded to uniform
                    length, and converted to tensors.</li>
                <li><strong>Data Loading:</strong> Configurations include a batch size of 64, shuffling, and
                    optimizations for GPU utilization.</li>
            </ul>
            <figure>
                <img src="figures/data_pipeline.png" alt="Data Pipeline">
                <figcaption>Data pipeline: preprocessing images and captions for input to the model.</figcaption>
            </figure>

            <h3>Model Architecture</h3>
            <p>
                The architecture combines frozen pre-trained encoders with trainable adapters to project representations
                into a shared multimodal space:
            </p>
            <ul>
                <li><strong>Encoders:</strong> ResNet-18 for images (\( d_\text{enc} = \mathbb{R}^{512} \)) and
                    DistilBERT for text (\(
                    d_\text{enc} = \mathbb{R}^{768} \)).</li>
                <li><strong>Adapters:</strong>
                    <ul>
                        <li>Linear adapters (\( \mathbb{R}^{d_\text{enc}} \to \mathbb{R}^{384} \)).</li>
                        <li>MLP adapters (\( \mathbb{R}^{d_\text{enc}} \to \mathbb{R}^{2048} \to \mathbb{R}^{1024} \to
                            \mathbb{R}^{384} \)).</li>
                    </ul>
                </li>
            </ul>
            <figure>
                <img src="figures/model_architecture.png" alt="Model Architecture">
                <figcaption>Model architecture: frozen encoders, adapters, and shared multimodal latent space.
                </figcaption>
            </figure>

            <h3>Training Procedure</h3>
            <p>
                We use contrastive learning to align representations. The training loop involves:
            </p>
            <ol>
                <li>Processing paired batches through the encoders and adapters.</li>
                <li>Computing contrastive loss to align matching pairs and separate mismatched pairs:
                    <br>
                    \[
                    \mathcal{L} = -\frac{1}{N} \sum_{i=1}^N \log \frac{\exp\left(\langle W_x X_\text{enc}(x^{(i)}), W_y
                    Y_\text{enc}(y^{(i)}) \rangle / \tau \right)}{\sum_{j=1}^N \exp\left(\langle W_x
                    X_\text{enc}(x^{(i)}), W_y Y_\text{enc}(y^{(j)}) \rangle / \tau \right)}.
                    \]
                </li>
                <li>Updating adapter weights using gradient descent.</li>
            </ol>
            <figure>
                <img src="figures/training_loss_metrics.png" alt="Training Metrics">
                <figcaption>Training loss and kernel alignment metrics over epochs.</figcaption>
            </figure>

            <h3>Evaluation Metrics</h3>
            <p>
                We evaluate the learned representations using the mutual-KNN kernel alignment metric (\( m \)),
                comparing kernels before and after training:
            </p>
            <ul>
                <li><strong>Unimodal Kernel Similarity:</strong> Baseline similarity of ResNet-18 and DistilBERT
                    representations to DINOv2 embeddings.</li>
                <li><strong>Aligned Kernel Similarity:</strong> Final similarity of the multimodal representations to
                    DINOv2 embeddings.</li>
            </ul>
            <figure>
                <img src="figures/kernel_heatmaps.png" alt="Kernel Similarity Heatmaps">
                <figcaption>Heatmaps showing kernel similarities for unimodal and multimodal embeddings with DINOv2.
                </figcaption>
            </figure>

            <h3>Implementation Details</h3>
            <p>
                All code we used for this project is implemented in this <a
                    href="https://colab.research.google.com/drive/1tguG-THn52pPGcU9KIkmVBzYbhiPfw1w?usp=sharing">Google
                    Colab notebook</a>.
            </p>
            <ul>
                <li><strong>Frameworks:</strong> PyTorch for modeling, Hugging Face Transformers for text encoding, and
                    Weights & Biases for experiment tracking.</li>
                <li><strong>Hyperparameters:</strong>
                    <ul>
                        <li>Batch size: 64</li>
                        <li>Learning rate: \( 1 \times 10^{-4} \)</li>
                        <li>Temperature (\( \tau \)): 0.07</li>
                        <li>Epochs: 50</li>
                    </ul>
                </li>
            </ul>
        </section>


        <section id="initial-results">
            <h2>Initial Results</h2>
            <p>
                Our experiments with image and text modalities have yielded promising results. By aligning
                representations from ResNet-18 and DistilBERT, we observed:
            </p>
            <ul>
                <li><strong>Alignment Success:</strong> Aligned representations demonstrated improved similarity to
                    DinoV2 embeddings.</li>
                <li><strong>Cross-Modality Insights:</strong> Text embeddings, which DinoV2 was not trained to process,
                    gained representational properties closer to its image embeddings.</li>
                <li><strong>Caveat:</strong> While the aligned representation exceeded the average similarity of
                    unimodal kernels to DinoV2, it fell short of the maximum similarity (i.e., the image modality
                    kernel).</li>
            </ul>

            <h3>Metrics</h3>
            <p>
                Key kernel alignment metrics include:
            </p>
            <ul>
                <li>Text-to-DinoV2 similarity: Low, due to modality mismatch.</li>
                <li>Image-to-DinoV2 similarity: High, reflecting shared training data.</li>
                <li>Aligned multimodal similarity: Intermediate, surpassing the average of unimodal similarities.</li>
            </ul>
            <div class="figures-container">
                <figure>
                    <img src="figures/mlp/2d/m2d.gif" alt="2D MLP Adapter">
                    <figcaption>MLP Adapter</figcaption>
                </figure>
                <figure>
                    <img src="figures/mlp/3d/m3d.gif" alt="3D MLP Adapter">
                    <figcaption>MLP Adapter</figcaption>
                </figure>
            </div>
            <div class="figures-container">
                <figure>
                    <img src="figures/linear/2d/l2d.gif" alt="2D Linear Adapter">
                    <figcaption>Linear Adapter</figcaption>
                </figure>
                <figure>
                    <img src="figures/linear/3d/l3d.gif" alt="3D Linear Adapter">
                    <figcaption>Linear Adapter</figcaption>
                </figure>
            </div>
            <figure>
                <!-- TODO: Create a barplot comparing the kernel alignement metric before and after training.
                The top subplot is for the linear adapter. The bottom suplot is for the MLP adapter.
                Each suplplot will have two x-ticks (before training and after training) and each xtick has three bars: 
                    one for the image modality, one for the text modality, and one for the aligned multimodal representation.
                The y-axis is the kernel alignment metric value (i.e. \(m\) which for us is the mutual-KNN). 
                This bar plot will be allow us to immediately visually test our key hypotheses from the mathematical framework section. -->
                <img src="figures/bar_plot_sketch.jpg" alt="Kernel Alignment Metric Before and After Trainign">
                <figcaption>Placeholder: Bar plot showing kernel alignment for text, image, and multimodal embeddings
                    before and after training adapters to align unimodal representations into a latent multimodal
                    represntation
                    using contrastive loss.</figcaption>
            </figure>
        </section>

        <section id="proposed-extensions">
            <h2>Proposed Extensions</h2>
            <h3>1. Downstream Task Evaluation</h3>
            <p>
                Evaluate aligned embeddings on downstream tasks, such as classification, using pre-trained heads.
                Hypotheses include:
            </p>
            <ul>
                <li>Slight performance degradation compared to DinoV2.</li>
                <li>Aligned multimodal embeddings outperform unimodal embeddings.</li>
            </ul>

            <h3>2. Adding a Third Modality</h3>
            <p>
                Expand the framework to include audio representations from models like Wav2Vec2. Test whether
                integrating a third modality enhances alignment and demonstrates additive representation benefits.
            </p>

            <figure>
                <img src="figures/multimodal-alignment.svg" alt="Multimodal Alignment Process">
                <figcaption>Placeholder: Diagram illustrating multimodal alignment across three modalities (image, text,
                    and audio).</figcaption>
            </figure>
        </section>

        <section id="future-directions">
            <h2>Future Directions</h2>
            <ul>
                <li>Scale experiments to richer multimodal datasets and benchmarks.</li>
                <li>Investigate cross-architecture generalization of aligned embeddings.</li>
                <li>Analyze mechanistic insights into alignment-driven representation convergence.</li>
            </ul>
        </section>

        <section id="conclusion">
            <h2>Conclusion</h2>
            <p>
                This project illustrates a scalable and efficient approach to aligning unimodal encoders into a shared
                multimodal space. By leveraging pre-trained models and lightweight adapters, we unlock potential
                applications in resource-efficient AI and multimodal representation learning. Our findings highlight the
                promise of this methodology and pave the way for further exploration into the convergence of
                representations across modalities.
            </p>
        </section>
    </main>

    <footer>
        <p>Multimodal Alignment Analysis | Blog | November 2024</p>
    </footer>
</body>

<style>
    figure {
        display: flex;
        /* Arrange images side by side */
        flex-direction: column;
        /* Ensure caption appears below images */
        align-items: center;
        /* Center the content */
        gap: 10px;
        /* Add space between images and caption */
    }

    .figures-container {
        display: flex;
        /* Arrange figures side by side */
        justify-content: center;
        /* Center the figures on the page */
        gap: 20px;
        /* Add spacing between the figures */
    }

    figure img {
        width: 400px;
        /* Adjust image width */
        height: auto;
        /* Maintain aspect ratio */
    }

    figcaption {
        text-align: center;
        /* Center-align the caption text */
        font-size: 14px;
        /* Adjust caption font size */
        color: #555;
        /* Add a softer color for the caption */
    }
</style>

</html>

<!-- # TODO: Make an Appendix section and put this there. -->
<!-- <h4>Mutual \( k \)-Nearest Neighbor Metric</h4>
<ol>
    <li>
        <strong>Feature Extraction:</strong> From a batch of paired samples \( \{ (x_i, y_i) \}_{i=1}^N \), extract their features:
        \[
        \phi_i = f(x_i), \quad \psi_i = g(y_i).
        \]
        Here, \( \phi_i \) and \( \psi_i \) are feature vectors for modalities \( x \) and \( y \), respectively.
    </li>
    <li>
        <strong>Compute \( k \)-Nearest Neighbors:</strong>
        For each feature \( \phi_i \) in batch \( \Phi \) and each \( \psi_i \) in batch \( \Psi \), compute the indices of their \( k \)-nearest neighbors:
        \[
        d_{\text{knn}}(\phi_i, \Phi) = S(\phi_i), \quad d_{\text{knn}}(\psi_i, \Psi) = S(\psi_i),
        \]
        where \( S(\phi_i) \) and \( S(\psi_i) \) represent the sets of indices of their \( k \)-nearest neighbors.
    </li>
    <li>
        <strong>Mutual-KNN Alignment:</strong> Measure the average intersection of nearest neighbor sets between the two modalities:
        \[
        m_{\text{NN}}(\phi_i, \psi_i) = \frac{1}{k} \left| S(\phi_i) \cap S(\psi_i) \right|.
        \]
        The final metric is the mean of \( m_{\text{NN}} \) values across all sample pairs.
    </li>
</ol> -->