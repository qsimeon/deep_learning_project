<!DOCTYPE html>
<html lang="en">

<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Aligning Modalities: Efficient Multimodal Representation Learning</title>
    <link rel="stylesheet" href="styles/style.css">
</head>

<body>
    <header>
        <h1>Aligning Modalities: Efficient Multimodal Representation Learning</h1>
    </header>

    <main>
        <section id="introduction">
            <h2>Introduction</h2>
            <p>
                In recent years, deep learning has revolutionized the way we process and understand multimodal data,
                which combines information from diverse sources like images, text, and audio. However, the creation of
                such systems often demands immense computational resources and end-to-end training of massive neural
                networks. Our project aims to tackle these challenges by introducing a modular and efficient approach to
                multimodal representation learning.
            </p>
            <p>
                Inspired by the success of CLIP in aligning representations across modalities and the theoretical
                insights of the Platonic Representation Hypothesis, we propose a framework that aligns pre-trained
                unimodal encoders into a shared multimodal latent space using simple linear adapters. By leveraging
                frozen encoders such as ResNet-18 for images and GPT-2 for text, we aim to achieve alignment without
                retraining these large models, enabling scalability to new modalities with minimal overhead.
            </p>
            <h3>Motivation</h3>
            <p>
                Our approach is motivated by three key insights:
            </p>
            <ol>
                <li><strong>Learning from CLIP:</strong> The CLIP framework demonstrated that cross-modal
                    representations could be aligned through paired data and contrastive learning. However, it requires
                    joint training of encoders, limiting extensibility to additional modalities. Our method decouples
                    the encoders, focusing instead on aligning their outputs via lightweight adapters.</li>
                <li><strong>Testing the Platonic Representation Hypothesis:</strong> This hypothesis posits that
                    performant models converge toward a shared statistical model of reality in their representation
                    spaces. By aligning diverse unimodal encoders, we provide a testbed for exploring whether this
                    convergence can be achieved explicitly.</li>
                <li><strong>Scalability and Modularity:</strong> Traditional multimodal models often require joint
                    training on extensive datasets. By aligning frozen encoders post hoc, our framework supports modular
                    integration of new modalities, enabling efficient experimentation.</li>
            </ol>
        </section>

        <section id="hypothesis">
            <h2>Hypothesis</h2>
            <p>
                Our work is grounded in the following hypotheses:
            </p>
            <ul>
                <li>A shared latent space exists where unimodal representations from different encoders can be aligned
                    through linear transformations.</li>
                <li>Aligning these representations produces embeddings that closely approximate those of performant
                    models, such as DinoV2, as measured by kernel alignment metrics.</li>
                <li>Multimodal alignment captures mechanisms of representation convergence, offering empirical evidence
                    for the Platonic Representation Hypothesis.</li>
            </ul>
        </section>

        <!-- <section id="methodology">
            <h2>Methodology</h2>
            <h3>Dataset and Models</h3>
            <ul>
                <li><strong>Image Encoder:</strong> ResNet-18, pre-trained on ImageNet, provides feature vectors for
                    images.</li>
                <li><strong>Text Encoder:</strong> GPT-2, pre-trained for language tasks, processes captions into
                    textual embeddings.</li>
                <li><strong>Dataset:</strong> Flickr30k, a collection of image-caption pairs, serves as our primary
                    dataset for aligning modalities.</li>
                <li><strong>Performance Benchmark:</strong> DinoV2, a state-of-the-art vision model, serves as the
                    target for kernel alignment comparisons.</li>
            </ul>

            <h3>Alignment Procedure</h3>
            <p>
                Representations from each modality are extracted using their respective encoders. Linear adapters map
                these representations into a shared latent space. The alignment process is driven by contrastive
                learning, which minimizes the distance between aligned embeddings of matching image-caption pairs while
                maximizing the distance for unrelated pairs.
            </p>
            <p>
                The training objective is formulated as a contrastive loss, incorporating a temperature parameter to
                control the sharpness of the alignment. This loss encourages the model to focus on meaningful
                cross-modal relationships while maintaining scalability.
            </p>
        </section> -->
        <section id="methodology">
            <h2>Methodology</h2>

            <h3>Data Pipeline</h3>
            <p>
                The data pipeline processes paired image and text data from the Flickr30k dataset, preparing it for
                input into our model.
                This dataset is ideal for aligning visual and textual modalities due to its paired image-caption
                structure.
                Key stages of the pipeline include:
            </p>
            <ul>
                <li> <strong>Image Processing:</strong> Images are resized to 224x224 pixels, normalized to match the
                    preprocessing of ResNet-18,
                    and converted to tensors for batch input. </li>
                <li> <strong>Text Processing:</strong> Captions are tokenized using a BERT tokenizer, padded for uniform
                    length,
                    and converted to tensors. </li>
                <li> <strong>Data Loading:</strong> Configured with a batch size of 64, the data loader shuffles,
                    batches, and optimizes data for
                    efficient GPU utilization. </li>
            </ul>
            <figure>
                <img src="figures/data_pipeline.jpg" alt="Data Pipeline">
                <figcaption>
                    Illustration of the data pipeline: image and text processing leading to batched inputs for encoders.
                </figcaption>
            </figure>

            <h3>Model Architecture</h3>
            <p>
                Our model leverages frozen pre-trained encoders for each modality, integrated with lightweight trainable
                adapters to map features into a shared multimodal space.
            </p>
            <ul>
                <li>
                    <strong>Pre-trained Encoders:</strong> ResNet-18 extracts 512-dimensional image features, while
                    DistilBERT generates 768-dimensional text embeddings.
                </li>
                <li>
                    <strong>Linear Adapters:</strong> Trainable linear projections align features into a shared latent
                    space of 768 dimensions. Adapter variations include:
                    <ul>
                        <li>Linear adapters (e.g., 512 → 768).</li>
                        <li>MLP adapters (e.g., 512 → 1024 → 768).</li>
                        <li>Bottleneck adapters (e.g., 512 → 256 → 768).</li>
                    </ul>
                </li>
            </ul>
            <figure>
                <img src="figures/model_architecture.png" alt="Model Architecture">
                <figcaption>Diagram of the model architecture: frozen encoders, adapters, and shared multimodal latent
                    space.</figcaption>
            </figure>

            <h3>Training Procedure</h3>
            <p>
                The alignment process uses contrastive learning to minimize the distance between embeddings of matching
                image-caption pairs while maximizing it for mismatched pairs.
            </p>
            <ol>
                <li>
                    <strong>Contrastive Loss:</strong> A temperature-scaled similarity metric determines the loss,
                    ensuring meaningful alignment across modalities:
                    <br>
                    <code>
                    L = - (1/N) Σ log(exp(similarity(x, y) / τ) / Σ exp(similarity(x, y')))
                    </code>
                </li>
                <li>
                    <strong>Training Loop:</strong> The pipeline processes batches through the encoders and adapters,
                    computes gradients, and updates adapter parameters.
                </li>
                <li>
                    <strong>Evaluation:</strong> Kernel alignment metrics measure the similarity between the learned
                    multimodal representations and DinoV2 embeddings.
                </li>
            </ol>
            <figure>
                <img src="figures/training_loss_metrics.png" alt="Training Metrics">
                <figcaption>Placeholder: Training loss and alignment metrics over epochs.</figcaption>
            </figure>

            <h3>Evaluation Metrics</h3>
            <ul>
                <li>
                    <strong>Unimodal Kernel Similarity:</strong> Measures the initial similarity of image and text
                    kernels to DinoV2.
                </li>
                <li>
                    <strong>Aligned Kernel Similarity:</strong> Evaluates the final alignment of multimodal kernels with
                    DinoV2 after training.
                </li>
            </ul>
            <figure>
                <img src="figures/kernel_heatmaps.png" alt="Kernel Similarity Heatmaps">
                <figcaption>Heatmaps comparing the similarity of multimodal kernels with DinoV2 embeddings.</figcaption>
            </figure>

            <h3>Implementation Details</h3>
            <ul>
                <li><strong>Frameworks:</strong> PyTorch for modeling, Hugging Face Transformers for text encoding, and
                    Weights & Biases for tracking experiments.</li>
                <li><strong>Hyperparameters:</strong>
                    <ul>
                        <li>Batch size: 64</li>
                        <li>Learning rate: 1e-4</li>
                        <li>Temperature parameter: τ = 0.07</li>
                        <li>Epochs: 50</li>
                    </ul>
                </li>
            </ul>
        </section>


        <section id="initial-results">
            <h2>Initial Results</h2>
            <p>
                Our experiments with image and text modalities have yielded promising results. By aligning
                representations from ResNet-18 and GPT-2, we observed:
            </p>
            <ul>
                <li><strong>Alignment Success:</strong> Aligned representations demonstrated improved similarity to
                    DinoV2 embeddings.</li>
                <li><strong>Cross-Modality Insights:</strong> Text embeddings, which DinoV2 was not trained to process,
                    gained representational properties closer to its image embeddings.</li>
                <li><strong>Efficiency:</strong> Models trained on smaller datasets converged faster and achieved higher
                    alignment scores.</li>
            </ul>

            <h3>Metrics</h3>
            <p>
                Key kernel alignment metrics include:
            </p>
            <ul>
                <li>Text-to-DinoV2 similarity: Low, due to modality mismatch.</li>
                <li>Image-to-DinoV2 similarity: High, reflecting shared training data.</li>
                <li>Aligned multimodal similarity: Intermediate, surpassing the average of unimodal similarities.</li>
            </ul>

            <figure>
                <img src="figures/kernel_alignment_epochs.png" alt="Kernel Alignment Over Epochs">
                <figcaption>Placeholder: Line plot showing kernel alignment for text, image, and multimodal embeddings
                    over training epochs.</figcaption>
            </figure>
        </section>

        <section id="proposed-extensions">
            <h2>Proposed Extensions</h2>
            <h3>1. Downstream Task Evaluation</h3>
            <p>
                Evaluate aligned embeddings on downstream tasks, such as classification, using pre-trained heads.
                Hypotheses include:
            </p>
            <ul>
                <li>Slight performance degradation compared to DinoV2.</li>
                <li>Aligned multimodal embeddings outperform unimodal embeddings.</li>
            </ul>

            <h3>2. Adding a Third Modality</h3>
            <p>
                Expand the framework to include audio representations from models like Wav2Vec2. Test whether
                integrating a third modality enhances alignment and demonstrates additive representation benefits.
            </p>

            <figure>
                <img src="figures/conceptual_diagram.png" alt="Multimodal Alignment Process">
                <figcaption>Placeholder: Diagram illustrating multimodal alignment across three modalities (image, text,
                    and audio).</figcaption>
            </figure>
        </section>

        <section id="future-directions">
            <h2>Future Directions</h2>
            <ul>
                <li>Scale experiments to richer multimodal datasets and benchmarks.</li>
                <li>Investigate cross-architecture generalization of aligned embeddings.</li>
                <li>Analyze mechanistic insights into alignment-driven representation convergence.</li>
            </ul>
        </section>

        <section id="conclusion">
            <h2>Conclusion</h2>
            <p>
                This project illustrates a scalable and efficient approach to aligning unimodal encoders into a shared
                multimodal space. By leveraging pre-trained models and lightweight adapters, we unlock potential
                applications in resource-efficient AI and multimodal representation learning. Our findings highlight the
                promise of this methodology and pave the way for further exploration into the convergence of
                representations across modalities.
            </p>
        </section>
    </main>

    <footer>
        <p>Multimodal Alignment Analysis | Blog | November 2024</p>
    </footer>
</body>

</html>