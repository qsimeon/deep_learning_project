# -*- coding: utf-8 -*-
"""rep_aligment_project.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1BkyPko0x-8CL41Z-VSmyxI41B90hB2Fc

# Imports
"""

!pip install torch torchvision transformers pillow tqdm numpy scikit-learn matplotlib seaborn datasets wandb

import torch
import torchvision
from torchvision import transforms
from transformers import (
    DistilBertModel,
    DistilBertTokenizer,
    Dinov2Model
    )
from torch.utils.data import Dataset, DataLoader
import torch.nn as nn
import torch.nn.functional as F

from datasets import load_dataset
from pathlib import Path
from tqdm import tqdm
import logging
import warnings
import time
import sys
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.decomposition import PCA
import numpy as np
import wandb
import math
from matplotlib.colors import LinearSegmentedColormap
import datetime
import traceback
import random
from PIL import Image
import json


# Basic logging setup
logging.basicConfig(
    level=logging.INFO,
    format='%(message)s',
    handlers=[
        logging.StreamHandler(sys.stdout)
    ]
)
logger = logging.getLogger(__name__)

# Ignore specific warnings
# warnings.filterwarnings("ignore", message="xFormers is not available")

"""# Config"""

class Config:
    def __init__(self, config_dict):
        # Model dimensions (fixed)
        self.img_dim = 512  # ResNet-18 output dimension
        self.text_dim = 768  # DistilBERT output dimension
        self.embed_dim = 768  # Match DinoV2's dimension

        # Required parameters
        self.adapter_type = config_dict['adapter_type']
        self.name = config_dict['name']

        # Get parameters dictionary
        params = config_dict['params']

        # Training parameters
        self.batch_size = params.get('batch_size', 128)
        self.num_samples = params.get('num_samples', 12800)
        self.lr = params.get('lr', 5e-5)
        self.min_lr = params.get('min_lr', 1e-6)
        self.temperature = params.get('temperature', 0.07)
        self.epochs = params.get('epochs', 30)
        self.warmup_epochs = params.get('warmup_epochs', 2)

        # Distribution matching parameters
        self.variance_weight = params.get('variance_weight', 0.1)
        self.distribution_matching = params.get('distribution_matching', True)

        # Regularization parameters
        self.weight_decay = params.get('weight_decay', 0.01)
        self.dropout = params.get('dropout', 0.1)
        self.label_smoothing = params.get('label_smoothing', 0.1)
        self.clip_grad_norm = params.get('clip_grad_norm', 1.0)
        self.grad_accumulation_steps = params.get('gradient_accumulation_steps', 2)

        # MLP specific parameters
        self.mlp_hidden_dims = params.get('mlp_hidden_dims', [2048, 1024])
        self.mlp_activation = params.get('activation', 'gelu')
        self.layer_norm = params.get('layer_norm', True)

        # Evaluation parameters
        self.downstream_eval_frequency = params.get('downstream_eval_frequency', 5)
        self.downstream_batch_size = params.get('downstream_batch_size', 128)
        self.classifier_hidden_dim = params.get('classifier_hidden_dim', 512)
        self.num_classes = params.get('num_classes', 10)

        # Project configuration
        self.project_name = params.get('project_name', 'multimodal-alignment')
        self.wandb_entity = params.get('wandb_entity', None)
        self.save_dir = Path(params.get('save_dir', './experiments')) / self.name
        self.save_dir.mkdir(parents=True, exist_ok=True)

        # Visualization frequency
        self.viz_frequency = params.get('viz_frequency', 1)
        self.log_frequency = params.get('log_frequency', 10)

        # Compute steps
        self.steps_per_epoch = self.num_samples // (self.batch_size * self.grad_accumulation_steps)
        self.total_steps = self.steps_per_epoch * self.epochs
        self.warmup_steps = self.steps_per_epoch * self.warmup_epochs

        # Device configuration
        self.device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')

        # Model configurations
        self.model_configs = {
            'dino': 'facebook/dinov2-base',
            'distilbert': 'distilbert-base-uncased',
            'resnet': '18'
        }

        # Print configuration
        self.print_config()

    def to_dict(self):
        """Convert config to dictionary for wandb."""
        config_dict = {
            'img_dim': self.img_dim,
            'text_dim': self.text_dim,
            'embed_dim': self.embed_dim,
            'adapter_type': self.adapter_type,
            'batch_size': self.batch_size,
            'num_samples': self.num_samples,
            'lr': self.lr,
            'min_lr': self.min_lr,
            'temperature': self.temperature,
            'epochs': self.epochs,
            'warmup_epochs': self.warmup_epochs,
            'weight_decay': self.weight_decay,
            'dropout': self.dropout,
            'label_smoothing': self.label_smoothing,
            'clip_grad_norm': self.clip_grad_norm,
            'grad_accumulation_steps': self.grad_accumulation_steps,
            'model_configs': self.model_configs,
            'device': str(self.device)
        }

        if self.adapter_type == 'mlp':
            config_dict.update({
                'mlp_hidden_dims': self.mlp_hidden_dims,
                'mlp_activation': self.mlp_activation,
                'layer_norm': self.layer_norm
            })

        return config_dict

    def print_config(self):
        """Print configuration in organized format."""
        print("\n" + "="*50)
        print(f"Configuration for: {self.name}")
        print("="*50)

        sections = [
            ("Model Dimensions", {
                "Image dimension": self.img_dim,
                "Text dimension": self.text_dim,
                "Embedding dimension": self.embed_dim,
                "MLP hidden dimensions": self.mlp_hidden_dims if self.adapter_type == 'mlp' else None
            }),
            ("Training Parameters", {
                "Adapter type": self.adapter_type,
                "Batch size": self.batch_size,
                "Number of samples": self.num_samples,
                "Learning rate": self.lr,
                "Temperature": self.temperature,
                "Epochs": self.epochs,
                "Warmup epochs": self.warmup_epochs
            }),
            ("Regularization", {
                "Weight decay": self.weight_decay,
                "Dropout": self.dropout,
                "Label smoothing": self.label_smoothing,
                "Gradient clipping": self.clip_grad_norm,
                "Gradient accumulation steps": self.grad_accumulation_steps
            })
        ]

        for section_name, section_dict in sections:
            print(f"\n{section_name}:")
            for key, value in section_dict.items():
                if value is not None:
                    print(f"  • {key}: {value}")

        print("="*50 + "\n")

"""# Data

## Flickr30k
"""

class Flickr30kDataset(Dataset):
    def __init__(self, transform=None, tokenizer=None, max_samples=None):
        print()
        logger.info("Loading Flickr30k dataset...")
        print()

        # Initialize dataset with retries
        max_retries = 5
        retry_delay = 1  # Initial delay in seconds

        for attempt in range(max_retries):
            try:
                self.dataset = load_dataset(
                    "nlphuji/flickr30k",
                    split='test',
                    streaming=False  # Change to non-streaming to better handle rate limits
                )
                break
            except Exception as e:
                if attempt == max_retries - 1:  # Last attempt
                    raise Exception(f"Failed to load dataset after {max_retries} attempts: {str(e)}")
                wait_time = retry_delay * (2 ** attempt)  # Exponential backoff
                logger.info(f"Rate limit hit. Waiting {wait_time}s before retry {attempt + 1}/{max_retries}")
                time.sleep(wait_time)

        self.transform = transform
        self.tokenizer = tokenizer

        # Store image-caption pairs with progress tracking
        self.pairs = []
        max_samples = max_samples if max_samples else len(self.dataset)

        logger.info(f"Processing {max_samples} samples...")
        for idx, item in enumerate(tqdm(self.dataset, total=max_samples, desc="Loading Flickr30k")):
            if idx >= max_samples:
                break
            try:
                self.pairs.append((item['image'], item['caption'][0]))
            except Exception as e:
                logger.warning(f"Failed to process item {idx}: {str(e)}")
                continue

    def __getitem__(self, idx):
        image, caption = self.pairs[idx]

        # Convert image to RGB and apply transforms
        if self.transform:
            image = self.transform(image.convert('RGB'))

        # Tokenize caption
        if self.tokenizer:
            encoded = self.tokenizer(
                caption,
                padding='max_length',
                truncation=True,
                max_length=64,
                return_tensors='pt'
            )
            return (
                image,
                encoded['input_ids'].squeeze(),
                encoded['attention_mask'].squeeze()
            )

        return image, caption

    def __len__(self):
        return len(self.pairs)

"""# Adapter Models

## Adapters
"""

class LinearAdapter(nn.Module):
    """Simple linear adapter with normalization."""
    def __init__(self, input_dim: int, embed_dim: int):
        super().__init__()
        self.linear = nn.Linear(input_dim, embed_dim)  # input_dim -> embed_dim
        self.norm = nn.LayerNorm(embed_dim)

    def forward(self, x):
        out = self.linear(x)
        return self.norm(out)

class MLPAdapter(nn.Module):
    """Enhanced MLP adapter with multiple hidden layers and configurable activation."""
    def __init__(self, input_dim: int, embed_dim: int, config):
        super().__init__()

        # Build layers dynamically
        layers = []
        current_dim = input_dim

        # Add hidden layers
        for hidden_dim in config.mlp_hidden_dims:
            layers.extend([
                nn.Linear(current_dim, hidden_dim),
                nn.GELU() if config.mlp_activation == 'gelu' else nn.ReLU(),
                nn.LayerNorm(hidden_dim) if config.layer_norm else nn.Identity(),
                nn.Dropout(config.dropout)
            ])
            current_dim = hidden_dim

        # Add final layer
        layers.extend([
            nn.Linear(current_dim, embed_dim),
            nn.LayerNorm(embed_dim) if config.layer_norm else nn.Identity()
        ])

        self.mlp = nn.Sequential(*layers)

    def forward(self, x):
        return self.mlp(x)

"""# Training Components

## KernelAlignmentMetric
"""

class KernelAlignmentMetric:
    def __init__(self, device):
        self.device = device

    def compute_kernel(self, features):
        """
        Compute centered kernel matrix with proper normalization.

        Args:
            features: torch.Tensor of shape [N, D]
        Returns:
            K_centered: Centered and normalized kernel matrix [N, N]
        """
        # Ensure features are on correct device and normalized
        features = features.to(self.device)
        if not torch.allclose(torch.norm(features, dim=1), torch.ones(features.shape[0], device=self.device)):
            features = F.normalize(features, dim=1)

        # Compute base kernel matrix
        N = features.shape[0]
        K = torch.matmul(features, features.T)

        # Center the kernel: K_c = HKH where H = I - 1/n 11^T
        ones = torch.ones(N, device=self.device) / N
        H = torch.eye(N, device=self.device) - ones.unsqueeze(0)
        K_centered = H @ K @ H

        # Normalize by Frobenius norm
        K_norm = torch.norm(K_centered, p='fro')
        if K_norm > 0:
            K_centered = K_centered / K_norm

        return K_centered

    def kernel_alignment(self, K1, K2):
        """Compute alignment between two kernel matrices."""
        # Ensure matrices are on the right device
        K1 = K1.to(self.device)
        K2 = K2.to(self.device)

        # Frobenius inner product
        align = torch.sum(K1 * K2)
        # Normalization
        norm1 = torch.sqrt(torch.sum(K1 * K1))
        norm2 = torch.sqrt(torch.sum(K2 * K2))

        return align / (norm1 * norm2)

    def evaluate_alignment(self, unimodal_features, aligned_features, dino_features):
        """
        Evaluate kernel alignment with DinoV2 and test hypotheses.

        Args:
            unimodal_features: dict with 'image' and 'text' features
            aligned_features: aligned multimodal features
            dino_features: DINO features as reference

        Returns:
            Dict containing alignment scores and hypothesis results
        """
        # Compute kernel matrices
        K_dino = self.compute_kernel(dino_features)
        K_image = self.compute_kernel(unimodal_features['image'])
        K_text = self.compute_kernel(unimodal_features['text'])
        K_aligned = self.compute_kernel(aligned_features)

        # Compute alignments directly between pairs
        alignments = {
            'image_text_alignment': self.kernel_alignment(K_image, K_text),
            'image_aligned_alignment': self.kernel_alignment(K_image, K_aligned),
            'text_aligned_alignment': self.kernel_alignment(K_text, K_aligned),
            'image_dino_alignment': self.kernel_alignment(K_image, K_dino),
            'text_dino_alignment': self.kernel_alignment(K_text, K_dino),
            'aligned_dino_alignment': self.kernel_alignment(K_aligned, K_dino),
        }

        # Make alignments symmetric
        for key in list(alignments.keys()):
            mod1, mod2, _ = key.split('_')
            rev_key = f'{mod2}_{mod1}_alignment'
            alignments[rev_key] = alignments[key]

        # Add _sim keys for visualization
        for key in list(alignments.keys()):
            sim_key = key.replace('_alignment', '_sim')
            alignments[sim_key] = alignments[key]

        # Compute metrics for hypotheses
        m_image_dino = alignments['image_dino_alignment']
        m_text_dino = alignments['text_dino_alignment']
        m_aligned_dino = alignments['aligned_dino_alignment']

        # Average and best unimodal alignments
        avg_unimodal = (m_image_dino + m_text_dino) / 2
        best_unimodal = max(m_image_dino, m_text_dino)

        # Add hypothesis metrics
        alignments.update({
            'image_dino_alignment': m_image_dino,
            'text_dino_alignment': m_text_dino,
            'repr_dino_alignment': m_aligned_dino,
            'avg_unimodal_alignment': avg_unimodal,
            'best_unimodal_alignment': best_unimodal,
            'h1_verified': float(m_aligned_dino > avg_unimodal),
            'h1_margin': float(m_aligned_dino - avg_unimodal),
            'h2_verified': float(m_aligned_dino > best_unimodal),
            'h2_margin': float(m_aligned_dino - best_unimodal),
        })

        # Ensure all values are Python floats
        alignments = {k: float(v) if torch.is_tensor(v) else v
                    for k, v in alignments.items()}

        return alignments

"""## Contrastive Loss"""

class ContrastiveLoss(nn.Module):
    """Enhanced contrastive loss with temperature scaling and label smoothing."""
    def __init__(self, temperature=0.05, label_smoothing=0.0):
        super().__init__()
        self.temperature = temperature
        self.label_smoothing = label_smoothing

    def forward(self, img_features, text_features):
        """
        Compute bidirectional contrastive loss with label smoothing

        Args:
            img_features: Tensor of shape [batch_size, embed_dim]
            text_features: Tensor of shape [batch_size, embed_dim]
        """
        # Normalize features
        img_features = F.normalize(img_features, dim=1)
        text_features = F.normalize(text_features, dim=1)

        # Compute similarity matrix
        logits = torch.matmul(img_features, text_features.T) / self.temperature
        batch_size = len(img_features)

        # Create smoothed labels
        if self.label_smoothing > 0:
            # Create one-hot labels
            labels = torch.eye(batch_size, device=img_features.device)
            # Apply label smoothing
            smooth_labels = labels * (1 - self.label_smoothing) + (1 - labels) * self.label_smoothing / (batch_size - 1)
            # Compute bidirectional loss with smoothing
            loss_i2t = -torch.sum(smooth_labels * F.log_softmax(logits, dim=1), dim=1).mean()
            loss_t2i = -torch.sum(smooth_labels * F.log_softmax(logits.T, dim=1), dim=1).mean()
        else:
            # Standard cross-entropy without smoothing
            labels = torch.arange(batch_size, device=img_features.device)
            loss_i2t = F.cross_entropy(logits, labels)
            loss_t2i = F.cross_entropy(logits.T, labels)

        # Average both directions
        loss = (loss_i2t + loss_t2i) / 2

        # Compute additional metrics
        with torch.no_grad():
            metrics = {
                'i2t_loss': loss_i2t.item(),
                't2i_loss': loss_t2i.item(),
                'mean_similarity': logits.mean().item(),
                'std_similarity': logits.std().item()
            }

        return loss, metrics

"""## Alignment Trainer"""

class AlignmentTrainer:
    def __init__(self, config, image_encoder, text_encoder, adapters):
        self.config = config
        self.device = config.device

        # Encoders setup
        self.image_encoder = image_encoder.to(self.device)
        self.text_encoder = text_encoder.to(self.device)

        # Adapters setup
        self.img_adapter, self.text_adapter = adapters
        self.img_adapter = self.img_adapter.to(self.device)
        self.text_adapter = self.text_adapter.to(self.device)

        # Loss and optimization
        self.criterion = ContrastiveLoss(
            temperature=config.temperature,
            label_smoothing=config.label_smoothing
        )

        # Add variance matching weight
        self.variance_weight = 0.1  # Adjust this weight as needed

        # Optimizer with weight decay
        self.optimizer = torch.optim.AdamW([
            {'params': self.img_adapter.parameters(), 'weight_decay': config.weight_decay},
            {'params': self.text_adapter.parameters(), 'weight_decay': config.weight_decay}
        ], lr=config.lr)

        # Learning rate scheduler
        self.scheduler = get_cosine_schedule_with_min_lr(
            self.optimizer,
            num_warmup_steps=config.warmup_steps,
            num_training_steps=config.total_steps,
            num_cycles=0.5,
            min_lr=config.min_lr
        )

    def variance_matching_loss(self, aligned_features, reference_features):
        """Compute variance matching loss between aligned and reference features."""
        aligned_var = torch.var(aligned_features, dim=0).mean()
        reference_var = torch.var(reference_features, dim=0).mean()
        return F.mse_loss(aligned_var, reference_var)

    def get_image_features(self, images):
        """Extract and adapt image features."""
        images = images.to(self.device)
        with torch.set_grad_enabled(self.img_adapter.training):
            img_feats = self.image_encoder(images)
            adapted = self.img_adapter(img_feats)
            return adapted

    def get_text_features(self, input_ids, attention_mask):
        """Extract and adapt text features."""
        input_ids = input_ids.to(self.device)
        attention_mask = attention_mask.to(self.device)
        with torch.set_grad_enabled(self.text_adapter.training):
            outputs = self.text_encoder(
                input_ids,
                attention_mask=attention_mask
            )
            text_feats = outputs.last_hidden_state[:, 0]  # CLS token
            adapted = self.text_adapter(text_feats)
            return adapted

    def get_aligned_features(self, images, input_ids=None, attention_mask=None, reference_features=None):
        """Get aligned features with distribution matching."""
        # Extract features from each modality when available
        img_feats = self.get_image_features(images)
        img_feats = F.normalize(img_feats, dim=1)

        if input_ids is not None and attention_mask is not None:
            text_feats = self.get_text_features(input_ids, attention_mask)
            text_feats = F.normalize(text_feats, dim=1)

            # Compute alignment weights with temperature scaling
            similarity = torch.sum(img_feats * text_feats, dim=1, keepdim=True)
            alpha = torch.sigmoid(similarity / self.config.temperature)

            # Weighted combination
            aligned_feats = alpha * img_feats + (1 - alpha) * text_feats

            # Normalize aligned features
            aligned_feats = F.normalize(aligned_feats, dim=1)

            # If reference features provided, match their distribution scale
            if reference_features is not None:
                reference_std = torch.std(reference_features, dim=0).mean()
                aligned_feats = aligned_feats * reference_std

            return aligned_feats

        return img_feats

    def train_epoch(self, train_loader, epoch, total_epochs, dino_model=None):
        """Training epoch with distribution matching."""
        self.img_adapter.train()
        self.text_adapter.train()

        total_loss = 0
        running_loss = 0
        self.optimizer.zero_grad()

        progress_bar = tqdm(train_loader, desc=f'Epoch [{epoch}/{total_epochs}]')

        for i, (images, input_ids, attention_mask) in enumerate(progress_bar):
            # Get DINO features as reference if model provided
            dino_features = None
            if dino_model is not None:
                with torch.no_grad():
                    dino_output = dino_model(images.to(self.device))
                    dino_features = F.normalize(dino_output.last_hidden_state[:, 0], dim=1)

            # Get features and compute alignment
            img_adapted = self.get_image_features(images)
            text_adapted = self.get_text_features(input_ids, attention_mask)
            aligned_features = self.get_aligned_features(
                images, input_ids, attention_mask,
                reference_features=dino_features
            )

            # Compute contrastive loss
            contrastive_loss, metrics = self.criterion(img_adapted, text_adapted)

            # Add variance matching loss if DINO features available
            if dino_features is not None:
                var_loss = self.variance_matching_loss(aligned_features, dino_features)
                total_batch_loss = contrastive_loss + self.variance_weight * var_loss
            else:
                total_batch_loss = contrastive_loss

            # Scale loss for accumulation
            loss = total_batch_loss / self.config.grad_accumulation_steps
            loss.backward()

            running_loss += loss.item() * self.config.grad_accumulation_steps

            # Step every grad_accumulation_steps or at the end
            if (i + 1) % self.config.grad_accumulation_steps == 0 or (i + 1) == len(train_loader):
                # Gradient clipping
                torch.nn.utils.clip_grad_norm_(
                    self.img_adapter.parameters(),
                    self.config.clip_grad_norm
                )
                torch.nn.utils.clip_grad_norm_(
                    self.text_adapter.parameters(),
                    self.config.clip_grad_norm
                )

                # Optimize
                self.optimizer.step()
                self.scheduler.step()
                self.optimizer.zero_grad()

                # Update metrics
                total_loss += running_loss
                running_loss = 0

                # Log progress
                current_lr = self.scheduler.get_last_lr()[0]
                progress_bar.set_postfix({
                    'loss': f"{loss.item():.4f}",
                    'avg_loss': f"{total_loss/(i+1):.4f}",
                    'lr': f"{current_lr:.2e}"
                })

                # Log to wandb
                step = epoch * len(train_loader) + i
                wandb.log({
                    'train/step_loss': loss.item(),
                    'train/learning_rate': current_lr,
                    'train/step': step,
                    'train/variance_loss': var_loss.item() if dino_features is not None else 0
                })

        avg_loss = total_loss / len(train_loader)
        return {'train_loss': avg_loss}

    @torch.no_grad()
    def evaluate(self, val_loader, dino_model=None):
        """Evaluate using kernel alignment and distribution metrics."""
        self.img_adapter.eval()
        self.text_adapter.eval()
        total_loss = 0
        total_alignment = 0
        total_var_loss = 0
        num_batches = 0

        progress_bar = tqdm(val_loader, desc="Validating", leave=False)

        for images, input_ids, attention_mask in progress_bar:
            # Get DINO features if model provided
            dino_features = None
            if dino_model is not None:
                dino_output = dino_model(images.to(self.device))
                dino_features = F.normalize(dino_output.last_hidden_state[:, 0], dim=1)

            # Get features
            img_adapted = self.get_image_features(images)
            text_adapted = self.get_text_features(input_ids, attention_mask)
            aligned_features = self.get_aligned_features(
                images, input_ids, attention_mask,
                reference_features=dino_features
            )

            # Compute contrastive loss
            loss, _ = self.criterion(img_adapted, text_adapted)

            # Compute variance matching loss if DINO features available
            var_loss = 0
            if dino_features is not None:
                var_loss = self.variance_matching_loss(aligned_features, dino_features)
                total_var_loss += var_loss.item()

            # Compute kernel alignment
            kernel_metric = KernelAlignmentMetric(self.device)
            alignment_score = kernel_metric.kernel_alignment(
                kernel_metric.compute_kernel(img_adapted),
                kernel_metric.compute_kernel(text_adapted)
            )

            total_loss += loss.item()
            total_alignment += alignment_score.item()
            num_batches += 1

            progress_bar.set_postfix(
                loss=f"{loss.item():.4f}",
                alignment=f"{alignment_score.item():.4f}",
                var_loss=f"{var_loss:.4f}" if dino_features is not None else "N/A"
            )

        metrics = {
            'val_loss': total_loss / num_batches,
            'alignment_score': total_alignment / num_batches,
        }

        if dino_features is not None:
            metrics['variance_loss'] = total_var_loss / num_batches

        return metrics

"""# Evaluation & Visualization

## Representation Comparator
"""

class RepresentationComparator:
    def __init__(self, config):
        self.config = config
        self.device = config.device

        # Load DinoV2 as reference model
        self.dino = Dinov2Model.from_pretrained(
            config.model_configs['dino'],
            output_hidden_states=True
        ).to(self.device).eval()

        # Initialize kernel metric
        self.kernel_metric = KernelAlignmentMetric(device=config.device)

    @torch.no_grad()
    def extract_dino_features(self, images):
        """Extract DinoV2 features."""
        outputs = self.dino(images)
        # Get last hidden state CLS token
        dino_feats = outputs.last_hidden_state[:, 0]
        # Apply normalization
        dino_feats = F.normalize(dino_feats, dim=1)
        return dino_feats

    @torch.no_grad()
    def get_all_features(self, trainer, loader):
        """
        Extract all feature types with proper alignment and normalization.

        Returns:
            features: Dict containing normalized features for each modality
            similarity_metrics: Dict containing pairwise similarity scores
        """
        features = {
            'image': [],
            'text': [],
            'aligned': [],
            'dino': []
        }
        print()
        # Extract features batch by batch
        for images, input_ids, attention_mask in tqdm(loader, desc="Extracting features"):
            # Move data to device
            images = images.to(self.device)
            input_ids = input_ids.to(self.device)
            attention_mask = attention_mask.to(self.device)

            # Extract features for each modality
            with torch.no_grad():
                # Get image features
                img_feats = trainer.get_image_features(images)

                # Get text features
                text_feats = trainer.get_text_features(input_ids, attention_mask)

                # Get aligned features using both modalities
                aligned_feats = trainer.get_aligned_features(
                    images,
                    input_ids,
                    attention_mask
                )

                # Get DINO features
                dino_feats = self.extract_dino_features(images)

            # Append features to lists (keeping on device for memory efficiency)
            features['image'].append(img_feats)
            features['text'].append(text_feats)
            features['aligned'].append(aligned_feats)
            features['dino'].append(dino_feats)

        # Concatenate features
        features = {k: torch.cat(v) for k, v in features.items()}

        # Normalize all features at once
        features = {k: F.normalize(v, dim=1) for k, v in features.items()}

        # Compute pairwise similarities
        similarity_metrics = {}
        modalities = ['image', 'text', 'aligned', 'dino']

        for i, mod1 in enumerate(modalities):
            for j, mod2 in enumerate(modalities):
                if i < j:  # Compute only upper triangle
                    sim = torch.mean(torch.matmul(features[mod1], features[mod2].T)).item()
                    similarity_metrics[f'{mod1}_{mod2}_sim'] = sim

        return features, similarity_metrics

    def test_hypotheses(self, features):
        """Test alignment hypotheses using kernel alignment."""
        # Unpack features if tuple
        if isinstance(features, tuple):
            features, similarities = features
        else:
            similarities = {}

        # Compute kernel alignment scores
        kernel_results = self.kernel_metric.evaluate_alignment(
            {'image': features['image'], 'text': features['text']},
            features['aligned'],
            features['dino']
        )

        # Add similarities to kernel results
        kernel_results.update(similarities)

        # Log to wandb
        wandb.log({
            'kernel/image_dino_alignment': kernel_results['image_dino_alignment'],
            'kernel/text_dino_alignment': kernel_results['text_dino_alignment'],
            'kernel/repr_dino_alignment': kernel_results['repr_dino_alignment'],
            'kernel/avg_unimodal_alignment': kernel_results['avg_unimodal_alignment'],
            'kernel/best_unimodal_alignment': kernel_results['best_unimodal_alignment'],
            'hypotheses/h1_verified': kernel_results['h1_verified'],
            'hypotheses/h2_verified': kernel_results['h2_verified'],
            'hypotheses/h1_margin': kernel_results['h1_margin'],
            'hypotheses/h2_margin': kernel_results['h2_margin']
        })

        return kernel_results

"""## Downstream Evaluator"""

class DownstreamEvaluator:
    def __init__(self, config):
        self.config = config
        self.device = config.device

        # Initialize DINO
        self.dino = Dinov2Model.from_pretrained(
            "facebook/dinov2-base"
        ).to(self.device)
        self.dino.eval()

        # Create classifiers for all modalities
        self.dino_classifier = self._create_classifier()
        self.aligned_classifier = self._create_classifier()
        self.image_classifier = self._create_classifier()
        self.text_classifier = self._create_classifier()

        # Training components
        self.criterion = nn.CrossEntropyLoss()
        self.dino_optimizer = torch.optim.Adam(self.dino_classifier.parameters(), lr=1e-3)
        self.aligned_optimizer = torch.optim.Adam(self.aligned_classifier.parameters(), lr=1e-3)
        self.image_optimizer = torch.optim.Adam(self.image_classifier.parameters(), lr=1e-3)
        self.text_optimizer = torch.optim.Adam(self.text_classifier.parameters(), lr=1e-3)

        # Load CIFAR
        self.train_loader, self.test_loader = self.get_cifar10_loaders()

    def _create_classifier(self):
        """Helper to create a classifier with consistent architecture."""
        return nn.Sequential(
            nn.Linear(self.config.embed_dim, self.config.classifier_hidden_dim),
            nn.ReLU(),
            nn.Dropout(self.config.dropout),
            nn.Linear(self.config.classifier_hidden_dim, self.config.num_classes)
        ).to(self.device)

    def get_cifar10_loaders(self):
        """Create data loaders for CIFAR-10 using config batch size."""
        transform = transforms.Compose([
            transforms.Resize(256),
            transforms.CenterCrop(224),
            transforms.ToTensor(),
            transforms.Normalize(mean=[0.485, 0.456, 0.406],
                              std=[0.229, 0.224, 0.225])
        ])

        trainset = torchvision.datasets.CIFAR10(
            root='./data', train=True, download=True, transform=transform)
        testset = torchvision.datasets.CIFAR10(
            root='./data', train=False, download=True, transform=transform)

        train_loader = DataLoader(
            trainset,
            batch_size=self.config.downstream_batch_size,
            shuffle=True,
            num_workers=2,
            pin_memory=True
        )
        test_loader = DataLoader(
            testset,
            batch_size=self.config.downstream_batch_size,
            shuffle=False,
            num_workers=2,
            pin_memory=True
        )

        return train_loader, test_loader

    def load_best_model(self, trainer, checkpoint_path):
        """Load the best model checkpoint."""
        if not checkpoint_path.exists():
            logger.warning(f"No checkpoint found at {checkpoint_path}")
            return trainer

        try:
            checkpoint = torch.load(checkpoint_path, weights_only=True)
            if 'model_state' in checkpoint:
                trainer.img_adapter.load_state_dict(checkpoint['model_state']['img_adapter'])
                trainer.text_adapter.load_state_dict(checkpoint['model_state']['text_adapter'])
                logger.info("Loaded best model checkpoint for evaluation")
        except Exception as e:
            logger.error(f"Error loading checkpoint: {str(e)}")

        return trainer

    def train_classifier(self, name, classifier, optimizer, get_features_fn, epochs=1):
        """Generic classifier training function."""
        print(f"\nTraining {name} classifier...")
        classifier.train()

        for epoch in range(epochs):
            correct = 0
            total = 0
            running_loss = 0

            for images, labels in tqdm(self.train_loader, desc=f"Epoch {epoch+1}/{epochs}"):
                images = images.to(self.device)
                labels = labels.to(self.device)

                # Get features using provided function
                with torch.no_grad():
                    features = get_features_fn(images)

                # Train classifier
                optimizer.zero_grad()
                logits = classifier(features)
                loss = self.criterion(logits, labels)
                loss.backward()
                optimizer.step()

                # Track metrics
                running_loss += loss.item()
                preds = logits.argmax(dim=1)
                correct += (preds == labels).sum().item()
                total += labels.size(0)

            epoch_loss = running_loss / len(self.train_loader)
            epoch_acc = 100 * correct / total
            print(f"Epoch {epoch+1}: Loss = {epoch_loss:.4f}, Acc = {epoch_acc:.2f}%")

    @torch.no_grad()
    def evaluate_classifier(self, name, classifier, get_features_fn, loader):
        """Generic classifier evaluation function."""
        classifier.eval()
        correct = 0
        total = 0

        for images, labels in tqdm(loader, desc=f"Evaluating {name}"):
            images = images.to(self.device)
            labels = labels.to(self.device)

            # Get features and predictions
            features = get_features_fn(images)
            logits = classifier(features)
            preds = logits.argmax(dim=1)

            correct += (preds == labels).sum().item()
            total += labels.size(0)

        return correct / total

    def evaluate(self, trainer, best_model_path=None):
        """Train classifiers and evaluate on test set using best model."""
        print("\n" + "="*50)
        print("🔄 Running Downstream Evaluation on CIFAR-10")
        print("="*50)

        # Load best model if available
        if best_model_path:
            trainer = self.load_best_model(trainer, best_model_path)

        # Define feature extraction functions
        feature_extractors = {
            'DINO': lambda x: self.dino(x).last_hidden_state[:, 0],
            'Aligned': trainer.get_aligned_features,
            'Image': trainer.get_image_features,
            'Text': trainer.get_text_features
        }

        # Train all classifiers
        classifiers = {
            'DINO': (self.dino_classifier, self.dino_optimizer),
            'Aligned': (self.aligned_classifier, self.aligned_optimizer),
            'Image': (self.image_classifier, self.image_optimizer),
            'Text': (self.text_classifier, self.text_optimizer)
        }

        results = {}

        # Train and evaluate DINO first as baseline
        self.train_classifier('DINO', *classifiers['DINO'], feature_extractors['DINO'])
        results['dino'] = self.evaluate_classifier('DINO', self.dino_classifier,
                                                 feature_extractors['DINO'], self.test_loader)

        # Train and evaluate other modalities
        for name in ['Aligned', 'Image']:
            classifier, optimizer = classifiers[name]
            self.train_classifier(name, classifier, optimizer, feature_extractors[name])
            results[name.lower()] = self.evaluate_classifier(
                name, classifier, feature_extractors[name], self.test_loader
            )

        # Calculate relative performances
        base_acc = results['dino']
        relative_perfs = {
            f"{k}_relative": (v / base_acc) * 100 if base_acc > 0 else 0
            for k, v in results.items() if k != 'dino'
        }
        results.update(relative_perfs)

        # Print results
        print("\n" + "="*50)
        print("CIFAR-10 Classification Results:")
        print("="*50)
        print(f"DINO Accuracy:     {results['dino']*100:>8.2f}%")
        print(f"Image Accuracy:    {results['image']*100:>8.2f}% ({results['image_relative']:.1f}%)")
        print(f"Aligned Accuracy:  {results['aligned']*100:>8.2f}% ({results['aligned_relative']:.1f}%)")
        print("="*50)

        # Log to wandb
        wandb.log({
            'downstream/cifar10_dino_acc': results['dino'],
            'downstream/cifar10_image_acc': results['image'],
            'downstream/cifar10_aligned_acc': results['aligned'],
            'downstream/cifar10_image_relative': results['image_relative'],
            'downstream/cifar10_aligned_relative': results['aligned_relative']
        })

        return results

"""## Visualization Manager"""

class VisualizationManager:
    def __init__(self, config):
        self.config = config
        self.save_dir = config.save_dir / 'visualizations'
        self.save_dir.mkdir(exist_ok=True)

        # Define clearly distinct colors for each modality
        self.colors = {
            'image': '#FF6B6B',     # Red
            'text': 'purple',
            'aligned': 'yellow',
            'dino': '#96CEB4'       # Green
        }

        # Define color scheme for heatmap
        self.heatmap_cmap = LinearSegmentedColormap.from_list(
            'custom',
            ['#053061', '#2166AC', '#4393C3', '#92C5DE', '#F7F7F7',
             '#F4A582', '#D6604D', '#B2182B', '#67001F']
        )

    def plot_and_log_features(self, features_tuple, epoch):
        """Plot feature space visualizations."""
        # Unpack features from tuple if necessary
        features_dict = features_tuple[0] if isinstance(features_tuple, tuple) else features_tuple

        # Normalize features and move to CPU
        features_dict = {
            k: F.normalize(v.cpu(), dim=1).numpy()
            for k, v in features_dict.items()
        }

        # Create visualizations
        self._plot_feature_space(features_dict, epoch, mode='all', dims=2)
        self._plot_feature_space(features_dict, epoch, mode='aligned_dino', dims=2)
        self._plot_feature_space(features_dict, epoch, mode='all', dims=3)

    def _plot_feature_space(self, features_dict, epoch, mode='all', dims=2):
        """Plot feature space with distinct colors for each modality."""
        # Select features based on mode
        if mode == 'aligned_dino':
            selected_features = {k: features_dict[k] for k in ['aligned', 'dino']}
            title_prefix = 'Aligned-DINO'
            filename_prefix = 'aligned_dino'
        else:
            selected_features = features_dict
            title_prefix = 'All Modalities'
            filename_prefix = 'all'

        # Prepare data
        all_features = []
        labels = []
        for name, feats in selected_features.items():
            all_features.append(feats)
            labels.extend([name] * len(feats))

        all_features = np.concatenate(all_features)
        labels = np.array(labels)

        # PCA reduction
        pca = PCA(n_components=dims)
        reduced = pca.fit_transform(all_features)

        # Create figure with proper size
        plt.figure(figsize=(15, 10))

        if dims == 2:
            # 2D scatter plot with larger points and higher alpha
            for name in selected_features.keys():
                mask = labels == name
                plt.scatter(
                    reduced[mask, 0],
                    reduced[mask, 1],
                    c=[self.colors[name]],
                    label=name,
                    alpha=0.7,
                    s=70,  # Larger point size
                    edgecolors='white',
                    linewidth=0.5
                )

            plt.xlabel('First Principal Component')
            plt.ylabel('Second Principal Component')

        else:
            # 3D scatter plot
            ax = plt.axes(projection='3d')
            for name in selected_features.keys():
                mask = labels == name
                ax.scatter(
                    reduced[mask, 0],
                    reduced[mask, 1],
                    reduced[mask, 2],
                    c=[self.colors[name]],
                    label=name,
                    alpha=0.7,
                    s=70,
                    edgecolors='white',
                    linewidth=0.5
                )

            ax.set_xlabel('First PC')
            ax.set_ylabel('Second PC')
            ax.set_zlabel('Third PC')

        plt.title(f'{title_prefix} Feature Space (Epoch {epoch})')
        plt.legend(bbox_to_anchor=(1.05, 1), loc='upper left')
        plt.grid(True, alpha=0.3)
        plt.tight_layout()

        # Save and log
        temp_path = self.save_dir / f'features_{dims}d_{filename_prefix}_epoch_{epoch}.png'
        plt.savefig(temp_path, dpi=300, bbox_inches='tight')
        wandb.log({
            f"visualizations/feature_space_{dims}d_{filename_prefix}_epoch_{epoch}":
            wandb.Image(str(temp_path))
        })
        plt.show()
        plt.close()

    def plot_and_log_alignment_heatmap(self, scores_dict, epoch):
        """Plot alignment scores heatmap with improved color scheme."""
        modalities = ['image', 'text', 'aligned', 'dino']
        n = len(modalities)
        matrix = np.zeros((n, n))

        # Fill matrix - look for both possible key formats
        for i, mod1 in enumerate(modalities):
            for j, mod2 in enumerate(modalities):
                if i == j:
                    matrix[i, j] = 1.0
                else:
                    # Try different possible key formats
                    key1 = f'{mod1}_{mod2}_sim'
                    key2 = f'{mod2}_{mod1}_sim'
                    key3 = f'{mod1}_{mod2}_alignment'
                    key4 = f'{mod2}_{mod1}_alignment'

                    # Look for the value in possible keys
                    value = None
                    for key in [key1, key2, key3, key4]:
                        if key in scores_dict:
                            value = scores_dict[key]
                            break

                    if value is not None:
                        matrix[i, j] = value
                        matrix[j, i] = value  # Ensure symmetry

        # Create heatmap with proper scaling
        plt.figure(figsize=(10, 8))
        sns.heatmap(
            matrix,
            annot=True,
            fmt='.3f',
            xticklabels=modalities,
            yticklabels=modalities,
            cmap=self.heatmap_cmap,
            vmin=0,  # Changed from -1 to 0 since these are similarity scores
            vmax=1,
            square=True,
            cbar_kws={'label': 'Alignment Score'}
        )

        plt.title(f'Modality Alignment Scores (Epoch {epoch})')
        plt.tight_layout()

        # Save and log
        temp_path = self.save_dir / f'alignment_heatmap_epoch_{epoch}.png'
        plt.savefig(temp_path, dpi=300, bbox_inches='tight')
        wandb.log({
            f"visualizations/alignment_heatmap_epoch_{epoch}": wandb.Image(str(temp_path)),
            "alignment_matrix": matrix.tolist()  # Also log raw values
        })
        plt.show()
        plt.close()

    def plot_and_log_metrics(self, history):
        """Plot training metrics."""
        plt.figure(figsize=(15, 5))

        # Loss plot
        plt.subplot(1, 2, 1)
        plt.plot(history['train_loss'], label='Train', color='#FF6B6B')
        plt.plot(history['val_loss'], label='Val', color='#4ECDC4')
        plt.title('Loss Curves')
        plt.xlabel('Epoch')
        plt.ylabel('Loss')
        plt.legend()
        plt.grid(True, alpha=0.3)

        # Alignment plot
        plt.subplot(1, 2, 2)
        plt.plot(
            history['alignment_scores'],
            label='Alignment Score',
            color='#45B7D1'
        )
        plt.title('Alignment Progress')
        plt.xlabel('Epoch')
        plt.ylabel('Score')
        plt.legend()
        plt.grid(True, alpha=0.3)

        plt.tight_layout()

        # Save and log plot
        temp_path = self.save_dir / 'training_metrics.png'
        plt.savefig(temp_path, dpi=300, bbox_inches='tight')
        wandb.log({
            "visualizations/training_metrics": wandb.Image(str(temp_path))
        })
        plt.show()
        plt.close()

    def plot_hypothesis_results(self, results, epoch):
        """Plot hypothesis verification results."""
        fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(15, 5))

        # Hypothesis 1: Better than average
        scores1 = [
            results['image_dino_alignment'],
            results['text_dino_alignment'],
            results['repr_dino_alignment']
        ]
        ax1.bar(['Image', 'Text', 'Aligned'], scores1, color=[
            self.colors['image'],
            self.colors['text'],
            self.colors['aligned']
        ])
        ax1.axhline(
            results['avg_unimodal_alignment'],
            color='red',
            linestyle='--',
            label='Unimodal Average'
        )
        ax1.set_title('H1: Better than Average')
        ax1.set_ylabel('Alignment with DINO')
        ax1.set_ylim(0,.53)
        ax1.legend()
        ax1.grid(True, alpha=0.3)

        # Hypothesis 2: Better than best
        scores2 = [
            results['best_unimodal_alignment'],
            results['repr_dino_alignment']
        ]
        ax2.bar(['Best Unimodal', 'Aligned'], scores2, color=[
            '#A8E6CE',
            self.colors['aligned']
        ])
        ax2.set_title('H2: Better than Best')
        ax2.set_ylabel('Alignment with DINO')
        ax2.grid(True, alpha=0.3)
        ax2.set_ylim(0,.53)

        plt.tight_layout()

        # Save and log plot
        temp_path = self.save_dir / f'hypothesis_results_epoch_{epoch}.png'
        plt.savefig(temp_path, dpi=300, bbox_inches='tight')
        wandb.log({
            f"visualizations/hypothesis_results_epoch_{epoch}":
            wandb.Image(str(temp_path))
        })
        plt.show()
        plt.close()

    def plot_alignments_comparison(self, initial_alignments, final_alignments, epoch):
        """Plot comparison of alignments before and after training."""
        plt.figure(figsize=(12, 6))

        # Data for plotting
        labels = ['Image-DINO', 'Text-DINO', 'Aligned-DINO']
        initial_values = [
            initial_alignments['image_dino_alignment'],
            initial_alignments['text_dino_alignment'],
            initial_alignments['aligned_dino_alignment']
        ]
        final_values = [
            final_alignments['image_dino_alignment'],
            final_alignments['text_dino_alignment'],
            final_alignments['aligned_dino_alignment']
        ]

        # Define consistent colors
        before_color = '#1f77b4'  # matplotlib default blue
        after_color = '#ff7f0e'   # matplotlib default orange

        # Plot bars
        x = np.arange(len(labels))
        width = 0.35

        ax = plt.gca()
        rects1 = ax.bar(x - width/2, initial_values, width, label='Before Training',
                    color=before_color, alpha=0.9)
        rects2 = ax.bar(x + width/2, final_values, width, label='After Training',
                    color=after_color, alpha=0.9)

        # Customize plot
        ax.set_ylabel('Alignment with DINO')
        ax.set_title('Alignment Comparison Before and After Training')
        ax.set_xticks(x)
        ax.set_xticklabels(labels)
        ax.legend()
        ax.grid(True, alpha=0.3)

        # Add value labels on bars
        def autolabel(rects):
            for rect in rects:
                height = rect.get_height()
                ax.annotate(f'{height:.3f}',
                        xy=(rect.get_x() + rect.get_width()/2, height),
                        xytext=(0, 3),  # 3 points vertical offset
                        textcoords="offset points",
                        ha='center', va='bottom')

        autolabel(rects1)
        autolabel(rects2)

        plt.tight_layout()

        # Save and log
        temp_path = self.save_dir / f'alignment_comparison_epoch_{epoch}.png'
        plt.savefig(temp_path, dpi=300, bbox_inches='tight')
        wandb.log({
            f"visualizations/alignment_comparison_epoch_{epoch}": wandb.Image(str(temp_path))
        })
        plt.show()
        plt.close()

"""# Validation"""

class ValidationLogger:
    """Log validation metrics and debug info during training."""
    def __init__(self, config):
        self.config = config
        self.log_dir = config.save_dir / 'validation_logs'
        self.log_dir.mkdir(exist_ok=True)

    def log_feature_shapes(self, features_dict):
        shapes = {k: v.shape for k, v in features_dict.items()}
        return shapes

    def validate_feature_pipeline(self, trainer, val_loader):
        batch = next(iter(val_loader))
        images = batch[0].to(self.config.device)

        validation = {
            'aligned_features': trainer.get_aligned_features(images).shape,
        }

        # Log shapes
        wandb.log({f"shapes/{k}": str(v) for k, v in validation.items()})
        return validation

    def validate_metrics(self, metrics_dict):
        """Validate metrics are within expected ranges."""
        validations = {}

        # Check accuracy ranges
        for k, v in metrics_dict.items():
            if 'accuracy' in k or 'score' in k:
                validations[k] = {
                    'value': v,
                    'valid': 0 <= v <= 1,
                }

        return validations

"""# Utility Functions

## Data & Model Loading
"""

def get_dataloaders(config):
    """Create data loaders for training and validation."""
    # Setup image transforms
    transform = transforms.Compose([
        transforms.Resize((224, 224)),
        transforms.ToTensor(),
        transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])
    ])

    # Setup DistilBERT tokenizer
    tokenizer = DistilBertTokenizer.from_pretrained(config.model_configs['distilbert'])

    # Create dataset
    dataset = Flickr30kDataset(
        transform=transform,
        tokenizer=tokenizer,
        max_samples=config.num_samples
    )

    # Split dataset
    train_size = int(0.75 * len(dataset))
    val_size = len(dataset) - train_size
    train_dataset, val_dataset = torch.utils.data.random_split(
        dataset,
        [train_size, val_size]
    )

    # Create dataloaders
    train_loader = DataLoader(
        train_dataset,
        batch_size=config.batch_size,
        shuffle=True,
        num_workers=2,
        pin_memory=True
    )

    val_loader = DataLoader(
        val_dataset,
        batch_size=config.batch_size,
        num_workers=2,
        pin_memory=True
    )

    return train_loader, val_loader

def get_base_models():
    """Initialize and return pre-trained vision and language models."""
    # ResNet18 part remains the same
    image_encoder = torchvision.models.resnet18(weights=torchvision.models.ResNet18_Weights.IMAGENET1K_V1)
    image_encoder = nn.Sequential(
        *list(image_encoder.children())[:-1],
        nn.Flatten()
    )

    # Initialize DistilBERT
    text_encoder = DistilBertModel.from_pretrained('distilbert-base-uncased')
    logger.info(f"Text encoder output dimension: {text_encoder.config.hidden_size}")

    # Freeze encoders
    for param in image_encoder.parameters():
        param.requires_grad = False
    for param in text_encoder.parameters():
        param.requires_grad = False

    return image_encoder, text_encoder

def get_adapters(config):
    """Initialize appropriate adapters based on config."""
    if config.adapter_type == 'linear':
        img_adapter = LinearAdapter(config.img_dim, config.embed_dim)
        text_adapter = LinearAdapter(config.text_dim, config.embed_dim)
    else:  # mlp
        img_adapter = MLPAdapter(config.img_dim, config.embed_dim, config)
        text_adapter = MLPAdapter(config.text_dim, config.embed_dim, config)

    # Move to device
    img_adapter = img_adapter.to(config.device)
    text_adapter = text_adapter.to(config.device)

    return (img_adapter, text_adapter)

def load_best_adapter(adapter_type, adapter, checkpoint_path):
    """Load adapter state with type checking."""
    if not checkpoint_path.exists():
        return

    try:
        checkpoint = torch.load(checkpoint_path, weights_only=True)

        # Get adapter's state dict structure
        adapter_state = adapter.state_dict()
        checkpoint_state = checkpoint['img_adapter_state']

        # Check if state dict structures match
        if set(adapter_state.keys()) == set(checkpoint_state.keys()):
            adapter.load_state_dict(checkpoint_state)
            logger.info(f"Successfully loaded {adapter_type} adapter checkpoint")
        else:
            logger.info(f"Skipping checkpoint loading - adapter architecture mismatch")
            logger.info(f"Current adapter keys: {set(adapter_state.keys())}")
            logger.info(f"Checkpoint keys: {set(checkpoint_state.keys())}")

    except Exception as e:
        logger.error(f"Error loading checkpoint: {str(e)}")
        logger.info("Starting with fresh adapter weights")

"""## Training Utilities"""

def get_cosine_schedule_with_min_lr(optimizer, num_warmup_steps, num_training_steps, num_cycles=0.5, min_lr=0.0):
    """
    Create a schedule with a learning rate that decreases following the values of the cosine function between the
    initial lr set in the optimizer to min_lr, after a warmup period during which it increases linearly between 0 and the
    initial lr set in the optimizer.
    """
    def lr_lambda(current_step):
        if current_step < num_warmup_steps:
            # Warmup
            return float(current_step) / float(max(1, num_warmup_steps))

        # Progress after warmup
        progress = float(current_step - num_warmup_steps) / float(max(1, num_training_steps - num_warmup_steps))
        scale = 0.5 * (1.0 + math.cos(math.pi * float(num_cycles) * 2.0 * progress))

        # Scale between initial lr and min_lr
        range_size = optimizer.param_groups[0]['lr'] - min_lr
        return scale * range_size / optimizer.param_groups[0]['lr'] + min_lr / optimizer.param_groups[0]['lr']

    return torch.optim.lr_scheduler.LambdaLR(optimizer, lr_lambda)

"""## Training & Experimentation"""

def train(config, trainer, comparator, viz, val_logger, train_loader, val_loader):
    """Training function with comprehensive evaluation and visualization."""
    # Initialize wandb
    wandb.init(
        project=config.project_name,
        name=f"{config.name}_{datetime.datetime.now().strftime('%Y%m%d_%H%M%S')}",
        config=config.to_dict(),
        job_type="training"
    )

    # Initialize downstream evaluator and get DINO model
    downstream_eval = DownstreamEvaluator(config)
    dino_model = comparator.dino  # Use DINO model from comparator

    # Get initial alignment scores and save them
    initial_features = comparator.get_all_features(trainer, val_loader)
    initial_results = comparator.test_hypotheses(initial_features)

    # Store initial alignments for comparison
    initial_alignments = {
        'image_dino_alignment': initial_results['image_dino_alignment'],
        'text_dino_alignment': initial_results['text_dino_alignment'],
        'aligned_dino_alignment': initial_results['aligned_dino_alignment']
    }

    # Track best model and metrics
    best_metrics = {
        'score': 0,
        'epoch': 0,
        'model_state': None,
        'hypothesis_results': None,
        'kernel_results': None,
        'downstream_results': None
    }

    # Training history
    history = {
        'train_loss': [],
        'val_loss': [],
        'alignment_scores': [],
        'downstream_results': [],
        'variance_differences': []
    }

    logger.info("\n🚀 Starting Training")
    training_start_time = time.time()

    for epoch in range(1, config.epochs + 1):
        epoch_start_time = time.time()

        # Train epoch with DINO model
        train_metrics = trainer.train_epoch(train_loader, epoch, config.epochs, dino_model=dino_model)
        history['train_loss'].append(train_metrics['train_loss'])

        # Validate
        val_metrics = trainer.evaluate(val_loader, dino_model=dino_model)
        history['val_loss'].append(val_metrics['val_loss'])
        history['alignment_scores'].append(val_metrics['alignment_score'])

        # Log basic metrics
        wandb.log({
            'train/loss': train_metrics['train_loss'],
            'val/loss': val_metrics['val_loss'],
            'val/alignment_score': val_metrics['alignment_score'],
            'epoch': epoch
        })

        # Periodic detailed evaluation
        if epoch % config.viz_frequency == 0:
            # Get features and compute alignments
            features = comparator.get_all_features(trainer, val_loader)
            kernel_results = comparator.test_hypotheses(features)
            validation_info = val_logger.validate_feature_pipeline(trainer, val_loader)

            # Plot alignment comparison
            viz.plot_alignments_comparison(initial_alignments, kernel_results, epoch)

            # Calculate and log variance differences
            aligned_features = features[0]['aligned']
            dino_features = features[0]['dino']
            aligned_var = torch.var(aligned_features, dim=0).mean()
            dino_var = torch.var(dino_features, dim=0).mean()
            var_diff = abs(aligned_var - dino_var)
            history['variance_differences'].append(var_diff.item())

            wandb.log({
                'distribution/variance_difference': var_diff.item(),
                'distribution/aligned_variance': aligned_var.item(),
                'distribution/dino_variance': dino_var.item()
            })

            # Regular visualizations
            viz.plot_and_log_features(features, epoch)
            viz.plot_and_log_metrics(history)
            viz.plot_and_log_alignment_heatmap(kernel_results, epoch)
            viz.plot_hypothesis_results(kernel_results, epoch)

        # Downstream evaluation
        if epoch % config.downstream_eval_frequency == 0:
            print("\nRunning downstream evaluation...")
            downstream_results = downstream_eval.evaluate(
                trainer,
                best_model_path=config.save_dir / 'best_model.pt'
            )
            history['downstream_results'].append(downstream_results)

        # Update best metrics
        current_score = val_metrics['alignment_score']
        if current_score > best_metrics['score']:
            best_metrics = {
                'score': current_score,
                'epoch': epoch,
                'model_state': {
                    'img_adapter': trainer.img_adapter.state_dict(),
                    'text_adapter': trainer.text_adapter.state_dict()
                },
                'hypothesis_results': kernel_results if 'kernel_results' in locals() else None,
                'validation_info': validation_info if 'validation_info' in locals() else None,
                'kernel_results': kernel_results if 'kernel_results' in locals() else None,
                'downstream_results': downstream_results if 'downstream_results' in locals() else None
            }

            # Save best model
            torch.save(best_metrics, config.save_dir / 'best_model.pt')

        # Print epoch summary
        print(f"\n📊 Epoch {epoch} Summary:")
        print(f"Train Loss: {train_metrics['train_loss']:.4f}")
        print(f"Val Loss: {val_metrics['val_loss']:.4f}")
        print(f"Alignment Score: {val_metrics['alignment_score']:.4f}")

        if 'variance_loss' in val_metrics:
            print(f"Variance Loss: {val_metrics['variance_loss']:.4f}")

        if epoch % config.downstream_eval_frequency == 0 and 'downstream_results' in locals():
            print(f"DINO Accuracy: {downstream_results['dino']*100:.2f}%")
            print(f"Image Accuracy: {downstream_results['image']*100:.2f}%")
            print(f"Aligned Accuracy: {downstream_results['aligned']*100:.2f}%")
            print(f"Image Relative: {downstream_results['image_relative']:.1f}%")
            print(f"Aligned Relative: {downstream_results['aligned_relative']:.1f}%")

        print(f"Time: {time.time() - epoch_start_time:.2f}s")

        # Print hypothesis verification results
        if epoch % config.viz_frequency == 0 and 'kernel_results' in locals():
            print("\nPlatonic Representation Hypothesis Tests:")
            print("\nHypothesis 1 (Better than Average):")
            print(f"✓ Verified: {kernel_results['h1_verified']}")
            print(f"  - Aligned-DINO Alignment: {kernel_results['repr_dino_alignment']:.4f}")
            print(f"  - Avg Unimodal-DINO Alignment: {kernel_results['avg_unimodal_alignment']:.4f}")
            print(f"  - Margin: {kernel_results['h1_margin']:.4f}")

            print("\nHypothesis 2 (Better than Best):")
            print(f"✓ Verified: {kernel_results['h2_verified']}")
            print(f"  - Aligned-DINO Alignment: {kernel_results['repr_dino_alignment']:.4f}")
            print(f"  - Best Unimodal-DINO Alignment: {kernel_results['best_unimodal_alignment']:.4f}")
            print(f"  - Margin: {kernel_results['h2_margin']:.4f}\n\n")

    # Training complete
    total_time = time.time() - training_start_time
    print(f"\n✨ Training completed in {total_time/60:.2f} minutes")

    # Final evaluation using best model
    print("\nRunning final evaluation with best model...")
    final_downstream_results = downstream_eval.evaluate(
        trainer,
        best_model_path=config.save_dir / 'best_model.pt'
    )

    # Get final alignment scores
    final_features = comparator.get_all_features(trainer, val_loader)
    final_results = comparator.test_hypotheses(final_features)

    # Log final metrics
    wandb.log({
        'time/total_minutes': total_time / 60,
        'time/total_seconds': total_time,
        'best/score': best_metrics['score'],
        'best/epoch': best_metrics['epoch'],
        'final/dino_accuracy': final_downstream_results['dino'],
        'final/image_accuracy': final_downstream_results['image'],
        'final/aligned_accuracy': final_downstream_results['aligned'],
        'final/image_relative': final_downstream_results['image_relative'],
        'final/aligned_relative': final_downstream_results['aligned_relative']
    })

    print("\n" + "="*50)
    print("Final Platonic Representation Hypothesis Results:")
    print("="*50)
    print(f"Image-DINO Alignment: {final_results['image_dino_alignment']:.4f}")
    print(f"Text-DINO Alignment: {final_results['text_dino_alignment']:.4f}")
    print(f"Aligned-DINO Alignment: {final_results['repr_dino_alignment']:.4f}")
    print(f"Average Unimodal Alignment: {final_results['avg_unimodal_alignment']:.4f}")
    print("\nHypothesis 1 (Better than Average):")
    print(f"✓ Verified: {final_results['h1_verified']}")
    print(f"  - Margin: {final_results['h1_margin']:.4f}")
    print("\nHypothesis 2 (Better than Best):")
    print(f"✓ Verified: {final_results['h2_verified']}")
    print(f"  - Margin: {final_results['h2_margin']:.4f}\n")
    print("="*50)

    wandb.finish()
    return best_metrics

def run_experiments():
    """Run all experiments defined in EXPERIMENTS."""
    results = {}

    # Create dataloaders once for all experiments
    print("\nLoading dataset once for all experiments...")
    config = Config(EXPERIMENTS[0])
    train_loader, val_loader = get_dataloaders(config)

    # Create results directory
    results_dir = Path('./results')
    results_dir.mkdir(exist_ok=True)

    for exp_config in EXPERIMENTS:
        print(f"\nStarting experiment: {exp_config['name']}")
        exp_start_time = time.time()

        try:
            # Create config and components
            config = Config(exp_config)
            image_encoder, text_encoder = get_base_models()
            img_adapter, text_adapter = get_adapters(config)

            # Initialize trainer
            trainer = AlignmentTrainer(
                config,
                image_encoder,
                text_encoder,
                (img_adapter, text_adapter)
            )

            # Initialize evaluation components
            comparator = RepresentationComparator(config)
            viz = VisualizationManager(config)
            val_logger = ValidationLogger(config)

            # Run training
            result = train(
                config=config,
                trainer=trainer,
                comparator=comparator,
                viz=viz,
                val_logger=val_logger,
                train_loader=train_loader,
                val_loader=val_loader
            )

            # Save experiment results
            results[config.name] = result

            # Log experiment timing
            exp_time = time.time() - exp_start_time
            print(f"\n⏱️ Experiment completed in {exp_time/60:.2f} minutes")

        except Exception as e:
            logger.error(f"Error in experiment {exp_config['name']}: {str(e)}")
            results[config.name] = {'error': str(e)}
            traceback.print_exc()
        finally:
            if wandb.run is not None:
                wandb.finish()

    # Save results summary
    summary_path = results_dir / f'summary_{datetime.datetime.now().strftime("%Y%m%d_%H%M%S")}.json'

    def tensor_to_serializable(obj):
        if isinstance(obj, torch.Tensor):
            return obj.cpu().detach().numpy().tolist()
        elif isinstance(obj, (np.ndarray, np.generic)):
            return obj.tolist()
        elif isinstance(obj, dict):
            return {k: tensor_to_serializable(v) for k, v in obj.items()}
        elif isinstance(obj, list):
            return [tensor_to_serializable(item) for item in obj]
        elif isinstance(obj, (int, float, str, bool, type(None))):
            return obj
        else:
            try:
                return obj.item() if hasattr(obj, 'item') else str(obj)
            except:
                return str(obj)

    # Convert results to serializable format
    serializable_results = tensor_to_serializable(results)

    # Save to JSON
    with open(summary_path, 'w') as f:
        json.dump(serializable_results, f, indent=2)

    return results

"""# Main"""

EXPERIMENTS = [
    {
        'adapter_type': 'linear',
        'name': 'linear_adapter_enhanced',
        'params': {
            # Core training parameters
            'batch_size': 256,
            'num_samples': 12800,
            'gradient_accumulation_steps': 2,
            'lr': 2e-4,  # Increased for faster convergence
            'min_lr': 1e-6,
            'temperature': 0.15,  # Increased to reduce repulsion
            'epochs': 20,  # Reduced epochs but with better scheduling
            'warmup_epochs': 2,
            'num_cycles': 0.5,

            # Distribution matching parameters
            'variance_weight': 0.3,  # Increased for better distribution matching
            'distribution_matching': True,

            # Regularization
            'weight_decay': 0.005,  # Increased to control feature spread
            'clip_grad_norm': 0.5,  # Reduced for more stable training
            'label_smoothing': 0.1,  # Increased to soften contrastive effects
            'dropout': 0.15,

            # Evaluation
            'downstream_eval_frequency': 20,
            'downstream_batch_size': 256,

            # Visualization and logging
            'viz_frequency': 1,
            'log_frequency': 10,
            'project_name': 'multimodal-alignment',
            'save_dir': './experiments'
        }
    },
    {
        'adapter_type': 'mlp',
        'name': 'mlp_adapter_compact_distribution',
        'params': {
            # Core training parameters
            'batch_size': 256,
            'num_samples': 12800,
            'gradient_accumulation_steps': 2,
            'lr': 1e-4,  # Increased slightly
            'min_lr': 1e-6,
            'temperature': 0.2,  # Significantly increased for softer contrasts
            'epochs': 20,
            'warmup_epochs': 3,  # More warmup epochs

            # Distribution matching parameters
            'variance_weight': 0.4,  # Significantly increased
            'distribution_matching': True,

            # Regularization
            'weight_decay': 0.02,  # Doubled for stronger regularization
            'clip_grad_norm': 0.5,  # Reduced for stability
            'label_smoothing': 0.15,  # Increased to reduce feature spreading
            'dropout': 0.2,  # Increased dropout

            # MLP specific parameters
            'mlp_hidden_dims': [1024, 1024],  # More balanced architecture
            'activation': 'gelu',
            'layer_norm': True,

            # Evaluation
            'downstream_eval_frequency': 20,
            'downstream_batch_size': 128,
            'classifier_hidden_dim': 512,

            # Visualization and logging
            'viz_frequency': 1,
            'log_frequency': 10,
            'project_name': 'multimodal-alignment',
            'save_dir': './experiments'
        }
    }
]

"""Main execution function."""
start_time = time.time()

# Setup logging
log_dir = Path('./logs')
log_dir.mkdir(exist_ok=True)
log_file = log_dir / f'training_{datetime.datetime.now().strftime("%Y%m%d_%H%M%S")}.log'

logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(levelname)s - %(message)s',
    handlers=[
        logging.StreamHandler(sys.stdout),
        logging.FileHandler(log_file)
    ]
)

# Create necessary directories
required_dirs = {
    'experiments': './experiments',
    'logs': './logs',
    'results': './results',
    'data': './data'
}

for dir_name, dir_path in required_dirs.items():
    Path(dir_path).mkdir(exist_ok=True)
    logger.info(f"✓ Directory {dir_name} ready at {dir_path}")

# Set random seeds
seed = 42
torch.manual_seed(seed)
np.random.seed(seed)
random.seed(seed)
if torch.cuda.is_available():
    torch.cuda.manual_seed_all(seed)
    torch.backends.cudnn.deterministic = True
    torch.backends.cudnn.benchmark = False

# Initialize wandb
try:
    wandb.login()
    logger.info("✓ Successfully logged into Weights & Biases")
except Exception as e:
    logger.warning(f"Could not log in to wandb: {str(e)}")
    logger.warning("Training will continue without wandb logging")

# Run experiments
try:
    logger.info("\n🚀 Starting experiments...")
    results = run_experiments()
    logger.info("\n✨ All experiments completed successfully!")

    # Save final results
    results_file = Path('./results') / f'final_results_{datetime.datetime.now().strftime("%Y%m%d_%H%M%S")}.pt'
    torch.save(results, results_file)

except Exception as e:
    logger.error(f"\n❌ Error during execution: {str(e)}")
    traceback.print_exc()
    raise
finally:
    if wandb.run is not None:
        wandb.finish()

# Print execution time
total_time = time.time() - start_time
hours = int(total_time // 3600)
minutes = int((total_time % 3600) // 60)
seconds = int(total_time % 60)
logger.info(f"\nTotal execution time: {hours}h {minutes}m {seconds}s")

!zip -r /content/results.zip /content



from google.colab import drive
drive.mount('/content/drive', force_remount=True)

from google.colab import files
files.download('/content/results.zip')

